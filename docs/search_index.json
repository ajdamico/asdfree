[["index.html", "Analyze Survey Data for Free Forty Public Microdatasets To Analyze Before You Die From An Easy To Type Website", " Analyze Survey Data for Free Forty Public Microdatasets To Analyze Before You Die From An Easy To Type Website Please ask questions about this book at stackoverflow.com with the R and survey tags. This book replaces my archived blog, prior code, and the no longer maintained lodown package. A work of R is never finished, merely abandoned. - Anthony Damico "],["trend-analysis-of-complex-survey-data.html", "Trend Analysis of Complex Survey Data Download, Import, Preparation Append Polynomials to Each Year Unadjusted Analysis Examples Calculate Joinpoints Needed Intermish Calculate Predicted Marginals Identify Joinpoint(s) or Breakpoint(s) Interpret and Conclude", " Trend Analysis of Complex Survey Data The purpose of this analysis is to make statistically valid statements such as, “there was a significant linear decrease in the prevalence of high school aged americans who have ever smoked a cigarette across the period 1999-2011” with complex sample survey data. This step-by-step walkthrough exactly reproduces the statistics presented in the Center for Disease Control &amp; Prevention’s (CDC) linear trend analysis. This analysis may complement qualitative evaluation on prevalence changes observed from surveillance data by providing quantitative evidence, such as when a joinpoint (also called breakpoint or changepoint) occurred; however, this analysis does not explain why or how changes in trends occur. Download, Import, Preparation Download and import the multi-year stacked file: library(SAScii) library(readr) sas_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/sadc_2019/2019-SADC-SAS-Input-Program.sas&quot; dat_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/sadc_2019/sadc_2019_national.dat&quot; sas_positions &lt;- parse.SAScii( sas_url ) sas_positions[ , &#39;varname&#39; ] &lt;- tolower( sas_positions[ , &#39;varname&#39; ] ) variables_to_keep &lt;- c( &quot;sex&quot; , &quot;grade&quot; , &quot;race4&quot; , &quot;q30&quot; , &quot;year&quot; , &quot;psu&quot; , &quot;stratum&quot; , &quot;weight&quot; ) sas_positions[ , &#39;column_types&#39; ] &lt;- ifelse( !( sas_positions[ , &#39;varname&#39; ] %in% variables_to_keep ) , &quot;_&quot; , ifelse( sas_positions[ , &#39;char&#39; ] , &quot;c&quot; , &quot;d&quot; ) ) yrbss_tbl &lt;- read_fwf( dat_url , fwf_widths( abs( sas_positions[ , &#39;width&#39; ] ) , col_names = sas_positions[ , &#39;varname&#39; ] ) , col_types = paste0( sas_positions[ , &#39;column_types&#39; ] , collapse = &quot;&quot; ) , na = c( &quot;&quot; , &quot;.&quot; ) ) yrbss_df &lt;- data.frame( yrbss_tbl ) Restrict the dataset to only years shown in the original analysis and re-name the main variable: yrbss_df &lt;- subset( yrbss_df , year %in% seq( 1991 , 2011 , 2 ) ) yrbss_df[ , &#39;ever_smoked&#39; ] &lt;- as.numeric( yrbss_df[ , &#39;q30&#39; ] == 1 ) yrbss_df[ , &#39;q30&#39; ] &lt;- NULL Recode each categorical variable to factor class: yrbss_df[ , &#39;sex&#39; ] &lt;- relevel( factor( yrbss_df[ , &#39;sex&#39; ] ) , ref = &quot;2&quot; ) for ( i in c( &#39;race4&#39; , &#39;grade&#39; ) ){ yrbss_df[ , i ] &lt;- relevel( factor( yrbss_df[ , i ] ) , ref = &quot;1&quot; ) } Append Polynomials to Each Year “The polynomials we have used as predictors to this point are natural polynomials, generated from the linear predictor by centering and then powering the linear predictor.” For more detail on this subject, see page 216 of Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences By Jacob Cohen, Patricia Cohen, Stephen G. West, Leona S. Aiken distinct_years_available &lt;- length( seq( 1991 , 2011 , 2 ) ) # store the linear polynomials c11l &lt;- contr.poly( distinct_years_available )[ , &quot;.L&quot; ] # store the quadratic polynomials c11q &lt;- contr.poly( distinct_years_available )[ , &quot;.Q&quot; ] # store the cubic polynomials c11c &lt;- contr.poly( distinct_years_available )[ , &quot;.C&quot; ] For each record in the data set, tack on the linear, quadratic, and cubic contrast value, these contrast values will serve as replacement for the linear year variable in any regression: # year^1 term (linear) yrbss_df[ , &quot;t11l&quot; ] &lt;- c11l[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] # year^2 term (quadratic) yrbss_df[ , &quot;t11q&quot; ] &lt;- c11q[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] # year^3 term (cubic) yrbss_df[ , &quot;t11c&quot; ] &lt;- c11c[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] Unadjusted Analysis Examples Construct a complex sample survey design and match the published unadjusted prevalence rates: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) des &lt;- svydesign( id = ~psu , strata = ~interaction( stratum , year ) , data = yrbss_df , weights = ~weight , nest = TRUE ) prevalence_over_time &lt;- svyby( ~ ever_smoked , ~ year , des , svymean , na.rm = TRUE ) # confirm prevalence rates match published estimates # of high school students that ever smoked stopifnot( all.equal( round( coef( prevalence_over_time ) , 3 ) , c( .701 , .695 , .713 , .702 , .704 , .639 , .584 , .543 , .503 , .463 , .447 ) , check.attributes = FALSE ) ) Calculate Joinpoints Needed Using the orthogonal coefficients (linear, quadratic, cubic terms) that we previously added to our yrbss_df object before constructing the multi-year stacked survey design, determine how many joinpoints will be needed for a trend analysis. Epidemiological models typically control for possible confounding variables such as age, sex, and race/ethnicity, so those have been included alongside the linear, cubic, and quadratic year terms. Calculate the “ever smoked” regression, adjusted by sex, grade, race/ethnicity, and linear year contrast: linyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l , design = des , family = quasibinomial ) summary( linyear ) The linear year-contrast variable t11l is significant. Therefore, there is probably going to be some sort of trend. A linear trend by itself does not need joinpoints. Not one, just zero joinpoints. If the linear term were the only significant term (out of linear, quadratic, cubic), then we would not need to calculate a joinpoint. In other words, we would not need to figure out where to best break our time trend into two, three, or even four segments. Since the linear trend is significant, we know there is at least one change across the entire 1991 to 2011 period. Interpretation note about segments of time: The linear term t11l was significant, so we probably have a significant linear trend somewhere to report. Now we need to figure out when that significant linear trend started and when it ended. It might be semantically true that there was a significant linear decrease in high school aged smoking over the entire period of our data 1991-2011; however, it’s inexact to end this analysis after only detecting a linear trend. The purpose of the following few steps is to cordon off different time points from one another. As you’ll see later, there actually was not any detectable decrease from 1991-1999. The entirety of the decline in smoking occurred over the period from 1999-2011. So these next (methodologically tricky) steps serve to provide you and your audience with a more careful statement of statistical significance. It’s not technically wrong to conclude that smoking declined over the period of 1991-2011, it’s just verbose. Think of it as the difference between “humans first walked on the moon in the sixties” and “humans first walked on the moon in 1969” - both statements are correct, but the latter exhibits greater scientific precision. Calculate the “ever smoked” binomial regression, adjusted by sex, grade, race/ethnicity, and both linear and quadratic year contrasts. Notice the addition of t11q: quadyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l + t11q , design = des , family = quasibinomial ) summary( quadyear ) The linear year-contrast variable is significant but the quadratic year-contrast variable is also significant. Therefore, we should use joinpoint software (the segmented package) for this analysis. A significant quadratic trend needs one joinpoint. Since both linear and quadratic terms are significant, we can also move ahead and test whether the cubic term is also significant. Calculate the “ever smoked” binomial regression, adjusted by sex, grade, race/ethnicity, and linear, quadratic, and cubic year contrasts. Notice the addition of t11c: cubyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l + t11q + t11c , design = des , family = quasibinomial ) summary( cubyear ) The cubic year-contrast term is also significant in this model. Therefore, we might potentially evaluate this trend using two joinpoints. In other words, a significant result for all linear, quadratic, and cubic year contrasts at this point means we might be able to evaluate three distinct trends (separated by our two joinpoints) across the broader 1991 - 2011 time period of analysis. Although we might now have the statistical ability to analyze three distinct time periods (separated by two joinpoints) across our data, the utility of this depends on the circumstances. Cubic and higher polynomials account for not only the direction of change but also the pace of that change, allowing statistical statements that might not be of interest to an audience: While it might be an exercise in precision to conclude that smoking rates dropped quickest across 1999-2003 and less quickly across 2003-2011, that scientific pair of findings may not be as compelling as the simpler (quadratic but not cubic) statement that smoking rates have dropped across the period of 1999-2011. Intermish the author programming in sas Calculate Predicted Marginals Calculate the survey-year-independent predictor effects and store these results: marginals &lt;- svyglm( formula = ever_smoked ~ sex + race4 + grade , design = des , family = quasibinomial ) Run these marginals through the svypredmeans function. For archaeology fans out there, this function emulates the PREDMARG statement in the ancient language of SUDAAN: ( means_for_joinpoint &lt;- svypredmeans( marginals , ~factor( year ) ) ) Clean up these results a bit in preparation for a joinpoint analysis: # coerce the results to a data.frame object means_for_joinpoint &lt;- as.data.frame( means_for_joinpoint ) # extract the row names as the survey year means_for_joinpoint[ , &quot;year&quot; ] &lt;- as.numeric( rownames( means_for_joinpoint ) ) # must be sorted, just in case it&#39;s not already means_for_joinpoint &lt;- means_for_joinpoint[ order( means_for_joinpoint[ , &quot;year&quot; ] ) , ] Identify Joinpoint(s) or Breakpoint(s) Let’s take a look at how confident we are in the value at each adjusted timepoint. Carrying out a trend analysis requires creating new weights to fit a piecewise linear regression. First, create that weight variable: means_for_joinpoint[ , &quot;wgt&quot; ] &lt;- with( means_for_joinpoint, ( mean / SE ) ^ 2 ) Second, fit a piecewise linear regression, estimating the ‘starting’ linear model with the usual lm function using the log values and the weights: o &lt;- lm( log( mean ) ~ year , weights = wgt , data = means_for_joinpoint ) Now that the regression has been structured correctly, estimate the year that our complex survey trend should be broken into two or more segments: library(segmented) # find only one joinpoint os &lt;- segmented( o , ~year ) summary( os ) Look for the Estimated Break-Point(s) in that result - that’s the critical number from this joinpoint analysis. The segmented package uses an iterative procedure (described in the article below); between-year solutions are returned and should be rounded to the nearest time point in the analysis. The joinpoint software implements two estimating algorithms: the grid-search and the Hudson algorithm. For more detail about these methods, see Muggeo V. (2003) Estimating regression models with unknown break-points. Statistics in Medicine, 22: 3055-3071.. Obtain the annual percent change estimates for each time point: slope( os , APC = TRUE ) The confidence intervals for the annual percent change (APC) may be different from the ones returned by NCI’s Joinpoint Software; for further details, check out Muggeo V. (2010) A Comment on `Estimating average annual per cent change in trend analysis’ by Clegg et al., Statistics in Medicine; 28, 3670-3682. Statistics in Medicine, 29, 1958-1960. This analysis returned similar results to the NCI’s Joinpoint Regression Program by estimating a joinpoint at year=1999 - and, more precisely, that the start of that decreasing trend in smoking prevalence happened at an APC of -3.92 percent. That is, slope2 from the output above. Remember that the cubic-year model above had significant terms as well. Therefore, it would be statistically defensible to calculate two joinpoints rather than only one. However, for this analyses, breaking the 1999-2011 trend into two separate downward trends might not be of interest to the audience. Looking at the slope2 and slope3 estimates and confidence intervals, we might be able to conclude that “ever smoking” decreased across 1999-2003 and also decreased (albeit less rapidly) across 2003-2011. However, communicating two consecutive downward trends might not be of much interest to a lay audience. Forgoing a second possible joinpoint makes sense when the direction of change is more compelling than the pace of change: # find two joinpoints rather than only one os2 &lt;- segmented( o , ~year , npsi = 2 ) summary( os2 ) slope( os2 , APC = TRUE ) Interpret and Conclude After identifying the joinpoint for smoking prevalence, we can create two regression models (one for each time segment - if we had two joinpoints, we would need three regression models). The first model covers the years leading up to (and including) the joinpoint (i.e., 1991 to 1999). The second model includes the years from the joinpoint forward (i.e., 1999 to 2011). So start with 1991, 1993, 1995, 1997, 1999, the five year-points before (and including) 1999: # calculate a five-timepoint linear contrast vector c5l &lt;- contr.poly( 5 )[ , 1 ] # tack the five-timepoint linear contrast vectors onto the current survey design object des &lt;- update( des , t5l = c5l[ match( year , seq( 1991 , 1999 , 2 ) ) ] ) pre_91_99 &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t5l , design = subset( des , year &lt;= 1999 ) , family = quasibinomial ) summary( pre_91_99 ) # confirm 1991-1999 trend coefficient matches published estimates stopifnot( round( pre_91_99$coefficients[&#39;t5l&#39;] , 5 ) == .03704 ) This reproduces the calculations behind the sentence on pdf page 6 of the original document: In this example, T5L_L had a p-value=0.52261 and beta=0.03704. Therefore, there was “no significant change in the prevalence of ever smoking a cigarette during 1991-1999.” Then move on to 1999, 2001, 2003, 2005, 2007, 2009, and 2011, the seven year-points after (and including) 1999: # calculate a seven-timepoint linear contrast vector c7l &lt;- contr.poly( 7 )[ , 1 ] # tack the seven-timepoint linear contrast vectors onto the current survey design object des &lt;- update( des , t7l = c7l[ match( year , seq( 1999 , 2011 , 2 ) ) ] ) post_99_11 &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t7l , design = subset( des , year &gt;= 1999 ) , family = quasibinomial ) summary( post_99_11 ) # confirm 1999-2011 trend coefficient matches published estimates stopifnot( round( post_99_11$coefficients[&#39;t7l&#39;] , 5 ) == -0.99165 ) This reproduces the calculations behind the sentence on pdf page 6 of the original document: In this example, T7L_R had a p-value&lt;0.0001 and beta=-0.99165. Therefore, there was a “significant linear decrease in the prevalence of ever smoking a cigarette during 1999-2011.” "],["american-community-survey-acs.html", "American Community Survey (ACS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Poverty and Inequality Estimation with convey   Analysis Examples with srvyr  ", " American Community Survey (ACS) The US Census Bureau’s annual replacement for the long-form decennial census. Two tables per state, the first with one row per household and the second with one row per individual. The civilian population of the United States. Released annually since 2005. Administered and financed by the US Census Bureau. Please skim before you begin: Guidance for Data Users Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # one percent sample # the decennial census # in miniature Download, Import, Preparation Download and import the Alabama household file: (switch sas_hal to sas_hak for Alaska or sas_hus for the entire country) library(haven) tf_household &lt;- tempfile() this_url_household &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_hal.zip&quot; download.file( this_url_household , tf_household , mode = &#39;wb&#39; ) unzipped_files_household &lt;- unzip( tf_household , exdir = tempdir() ) acs_sas_household &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_household , value = TRUE ) acs_df_household &lt;- read_sas( acs_sas_household ) names( acs_df_household ) &lt;- tolower( names( acs_df_household ) ) Download and import the Alabama person file: (switch sas_pal to sas_pak for Alaska or sas_pus for the entire country) tf_person &lt;- tempfile() this_url_person &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_pal.zip&quot; download.file( this_url_person , tf_person , mode = &#39;wb&#39; ) unzipped_files_person &lt;- unzip( tf_person , exdir = tempdir() ) acs_sas_person &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_person , value = TRUE ) acs_df_person &lt;- read_sas( acs_sas_person ) names( acs_df_person ) &lt;- tolower( names( acs_df_person ) ) Remove overlapping column and merge household + person files: acs_df_household[ , &#39;rt&#39; ] &lt;- NULL acs_df_person[ , &#39;rt&#39; ] &lt;- NULL acs_df &lt;- merge( acs_df_household , acs_df_person ) stopifnot( nrow( acs_df ) == nrow( acs_df_person ) ) acs_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # acs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ACS&quot; , &quot;this_file.rds&quot; ) # saveRDS( acs_df , file = acs_fn , compress = FALSE ) Load the same object: # acs_df &lt;- readRDS( acs_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) acs_design &lt;- svrepdesign( weight = ~pwgtp , repweights = &#39;pwgtp[0-9]+&#39; , scale = 4 / 80 , rscales = rep( 1 , 80 ) , mse = TRUE , type = &#39;JK1&#39; , data = acs_df ) Variable Recoding Add new columns to the data set: acs_design &lt;- update( acs_design , state_name = factor( as.numeric( st ) , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L, 72L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;Puerto Rico&quot;) ) , cit = factor( cit , levels = 1:5 , labels = c( &#39;born in the u.s.&#39; , &#39;born in the territories&#39; , &#39;born abroad to american parents&#39; , &#39;naturalized citizen&#39; , &#39;non-citizen&#39; ) ) , poverty_level = as.numeric( povpip ) , married = as.numeric( mar %in% 1 ) , sex = factor( sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( acs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cit , acs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , acs_design ) svyby( ~ one , ~ cit , acs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ poverty_level , acs_design , 0.5 , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ssip , denominator = ~ pincp , acs_design , na.rm = TRUE ) Subsetting Restrict the survey design to senior citizens: sub_acs_design &lt;- subset( acs_design , agep &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ poverty_level , sub_acs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ poverty_level , acs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( acs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ poverty_level , acs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ married , acs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( poverty_level ~ married , acs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ married + sex , acs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( poverty_level ~ married + sex , acs_design ) summary( glm_result ) Intermish the author as c++ increment operator Replication Example This example matches statistics, standard errors, and margin of errors from the 2021 PUMS tallies: Match the sum of the weights: stopifnot( round( coef( svytotal( ~ one , acs_design ) ) , 0 ) == 5039877 ) Compute the population by age: pums_estimate &lt;- c(288139L, 299245L, 336727L, 334606L, 327102L, 635004L, 641405L, 615709L, 335431L, 341926L, 538367L, 265742L, 80474L) pums_standard_error &lt;- c(2727L, 5368L, 6067L, 4082L, 4485L, 5716L, 4420L, 3706L, 4836L, 5100L, 2158L, 3363L, 3186L) pums_margin_of_error &lt;- c(4486L, 8830L, 9981L, 6715L, 7378L, 9402L, 7271L, 6096L, 7956L, 8389L, 3550L, 5532L, 5240L) results &lt;- svytotal( ~ as.numeric( agep %in% 0:4 ) + as.numeric( agep %in% 5:9 ) + as.numeric( agep %in% 10:14 ) + as.numeric( agep %in% 15:19 ) + as.numeric( agep %in% 20:24 ) + as.numeric( agep %in% 25:34 ) + as.numeric( agep %in% 35:44 ) + as.numeric( agep %in% 45:54 ) + as.numeric( agep %in% 55:59 ) + as.numeric( agep %in% 60:64 ) + as.numeric( agep %in% 65:74 ) + as.numeric( agep %in% 75:84 ) + as.numeric( agep %in% 85:100 ) , acs_design ) stopifnot( all( round( coef( results ) , 0 ) == pums_estimate ) ) stopifnot( all( round( SE( results ) , 0 ) == pums_standard_error ) ) stopifnot( all( round( SE( results ) * 1.645 , 0 ) == pums_margin_of_error ) ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for ACS users, this code calculates the gini coefficient on complex sample survey data: library(convey) acs_design &lt;- convey_prep( acs_design ) svygini( ~ hincp , acs_design , na.rm = TRUE ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ACS users, this code replicates previously-presented examples: library(srvyr) acs_srvyr_design &lt;- as_survey( acs_design ) Calculate the mean (average) of a linear variable, overall and by groups: acs_srvyr_design %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) acs_srvyr_design %&gt;% group_by( cit ) %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) "],["area-health-resources-files-ahrf.html", "Area Health Resources Files (AHRF) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " Area Health Resources Files (AHRF) National, state, and county-level data on health care professions, health facilities, population characteristics, health workforce training, hospital utilization and expenditure, and the environment. One table with one row per county and a second table with one row per state. Replaced annually with the latest available county- and state-level statistics. Compiled by the Bureau of Health Workforce at the Health Services and Resources Administration. Please skim before you begin: User Documentation for the County Area Health Resources File (AHRF) 2021-2022 Release Frequently Asked Questions This human-composed haiku or a bouquet of artificial intelligence-generated limericks # local aggregates # to spread merge join spline regress # like fresh buttered bread Download, Import, Preparation Download and import the most current county-level file: library(haven) tf &lt;- tempfile() ahrf_url &lt;- &quot;https://data.hrsa.gov//DataDownload/AHRF/AHRF_2021-2022_SAS.zip&quot; download.file( ahrf_url , tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) sas_fn &lt;- grep( &quot;\\\\.sas7bdat$&quot; , unzipped_files , value = TRUE ) ahrf_tbl &lt;- read_sas( sas_fn ) ahrf_df &lt;- data.frame( ahrf_tbl ) names( ahrf_df ) &lt;- tolower( names( ahrf_df ) ) Save locally   Save the object at any point: # ahrf_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;AHRF&quot; , &quot;this_file.rds&quot; ) # saveRDS( ahrf_df , file = ahrf_fn , compress = FALSE ) Load the same object: # ahrf_df &lt;- readRDS( ahrf_fn ) Variable Recoding Add new columns to the data set: ahrf_df &lt;- transform( ahrf_df , cbsa_indicator_code = factor( as.numeric( f1406720 ) , levels = 0:2 , labels = c( &quot;not metro&quot; , &quot;metro&quot; , &quot;micro&quot; ) ) , mhi_2020 = f1322620 , whole_county_hpsa_2022 = as.numeric( f0978722 ) == 1 , census_region = factor( as.numeric( f04439 ) , levels = 1:4 , labels = c( &quot;northeast&quot; , &quot;midwest&quot; , &quot;south&quot; , &quot;west&quot; ) ) ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( ahrf_df ) table( ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( ahrf_df[ , &quot;mhi_2020&quot; ] , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2020&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( ahrf_df[ , &quot;census_region&quot; ] ) ) prop.table( table( ahrf_df[ , c( &quot;census_region&quot; , &quot;cbsa_indicator_code&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( ahrf_df[ , &quot;mhi_2020&quot; ] , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2020&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( ahrf_df[ , &quot;mhi_2020&quot; ] , 0.5 , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2020&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to California: sub_ahrf_df &lt;- subset( ahrf_df , f12424 == &quot;CA&quot; ) Calculate the mean (average) of this subset: mean( sub_ahrf_df[ , &quot;mhi_2020&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( ahrf_df[ , &quot;mhi_2020&quot; ] , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2020&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( mhi_2020 ~ whole_county_hpsa_2022 , ahrf_df ) Perform a chi-squared test of association: this_table &lt;- table( ahrf_df[ , c( &quot;whole_county_hpsa_2022&quot; , &quot;census_region&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( mhi_2020 ~ whole_county_hpsa_2022 + census_region , data = ahrf_df ) summary( glm_result ) Intermish indus river cease to be empire synecdoche thanks to radcliffe’s jesting hand now it flows through pakistan portrayed here as rembrandt’s faust cyril would not harm a mouse yet one sweep of hammy fist: hist’ry’s worst exodusist Replication Example Match the record count in row number 8,543 of AHRF 2021-2022 Technical Documentation.xlsx: stopifnot( nrow( ahrf_df ) == 3232 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for AHRF users, this code replicates previously-presented examples: library(dplyr) ahrf_tbl &lt;- as_tibble( ahrf_df ) Calculate the mean (average) of a linear variable, overall and by groups: ahrf_tbl %&gt;% summarize( mean = mean( mhi_2020 , na.rm = TRUE ) ) ahrf_tbl %&gt;% group_by( cbsa_indicator_code ) %&gt;% summarize( mean = mean( mhi_2020 , na.rm = TRUE ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for AHRF users, this code replicates previously-presented examples: library(data.table) ahrf_dt &lt;- data.table( ahrf_df ) Calculate the mean (average) of a linear variable, overall and by groups: ahrf_dt[ , mean( mhi_2020 , na.rm = TRUE ) ] ahrf_dt[ , mean( mhi_2020 , na.rm = TRUE ) , by = cbsa_indicator_code ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for AHRF users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;ahrf&#39; , ahrf_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( mhi_2020 ) FROM ahrf&#39; ) dbGetQuery( con , &#39;SELECT cbsa_indicator_code , AVG( mhi_2020 ) FROM ahrf GROUP BY cbsa_indicator_code&#39; ) "],["american-housing-survey-ahs.html", "American Housing Survey (AHS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " American Housing Survey (AHS) The nationwide assessment of housing stock, with information on physical condition and neighborhood, costs of financing and maintenance, owner and renter characteristics, and changes over time. Nationally-representative and metropolitan flat files with one row per household, plus relational files. A complex sample survey of occupied and vacant housing units designed to generalize to all structures in the United States, both nationally and also for about thirty-five metropolitan areas. Released more or less biennially since 1973, with longitudinal samples redrawn in 1985 and 2015. Sponsored by the Department of Housing and Urban Development, run by the Census Bureau. Please skim before you begin: Getting Started with the Public Use File: 2015 and Beyond Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # real estate supply # half bath addition, raised roof # vent, rent too damn high Download, Import, Preparation Download and import the national 2021 flat file: library(haven) library(httr) tf &lt;- tempfile() this_url &lt;- paste0( &quot;https://www2.census.gov/programs-surveys/ahs/&quot; , &quot;2021/AHS%202021%20National%20PUF%20v1.0%20Flat%20SAS.zip&quot; ) GET( this_url , write_disk( tf ) , progress() ) ahs_tbl &lt;- read_sas( tf ) ahs_df &lt;- data.frame( ahs_tbl ) names( ahs_df ) &lt;- tolower( names( ahs_df ) ) Save locally   Save the object at any point: # ahs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;AHS&quot; , &quot;this_file.rds&quot; ) # saveRDS( ahs_df , file = ahs_fn , compress = FALSE ) Load the same object: # ahs_df &lt;- readRDS( ahs_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) ahs_design &lt;- svrepdesign( weights = ~ weight , repweights = &quot;repweight[1-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , mse = TRUE , data = ahs_df ) Variable Recoding Add new columns to the data set: ahs_design &lt;- update( ahs_design , one = 1 , tenure = factor( ifelse( tenure %in% c( -6 , &#39;N&#39; ) , 4 , tenure ) , levels = 1:4 , labels = c( &#39;Owned or being bought&#39; , &#39;Rented for cash rent&#39; , &#39;Occupied without payment of cash rent&#39; , &#39;Not occupied&#39; ) ) , lotsize = factor( lotsize , levels = 1:7 , labels = c( &quot;Less then 1/8 acre&quot; , &quot;1/8 up to 1/4 acre&quot; , &quot;1/4 up to 1/2 acre&quot; , &quot;1/2 up to 1 acre&quot; , &quot;1 up to 5 acres&quot; , &quot;5 up to 10 acres&quot; , &quot;10 acres or more&quot; ) ) , below_poverty = as.numeric( perpovlvl &lt; 100 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( ahs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ tenure , ahs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , ahs_design ) svyby( ~ one , ~ tenure , ahs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ totrooms , ahs_design , na.rm = TRUE ) svyby( ~ totrooms , ~ tenure , ahs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ lotsize , ahs_design , na.rm = TRUE ) svyby( ~ lotsize , ~ tenure , ahs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ totrooms , ahs_design , na.rm = TRUE ) svyby( ~ totrooms , ~ tenure , ahs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ lotsize , ahs_design , na.rm = TRUE ) svyby( ~ lotsize , ~ tenure , ahs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ totrooms , ahs_design , 0.5 , na.rm = TRUE ) svyby( ~ totrooms , ~ tenure , ahs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ totrooms , denominator = ~ rent , ahs_design , na.rm = TRUE ) Subsetting Restrict the survey design to homes with a garage or carport: sub_ahs_design &lt;- subset( ahs_design , garage == 1 ) Calculate the mean (average) of this subset: svymean( ~ totrooms , sub_ahs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ totrooms , ahs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ totrooms , ~ tenure , ahs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ahs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ totrooms , ahs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ totrooms , ahs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ totrooms , ahs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ below_poverty , ahs_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( totrooms ~ below_poverty , ahs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ below_poverty + lotsize , ahs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( totrooms ~ below_poverty + lotsize , ahs_design ) summary( glm_result ) Intermish the author as isaiah zagar mosaic Replication Example This example matches the estimate and margin of error of the Total row of the General Housing tab from the AHS 2021 Table Specifications and PUF Estimates for User Verification: result &lt;- svytotal( ~ as.numeric( intstatus == 1 ) , ahs_design ) stopifnot( round( coef( result ) / 1000 , 0 ) == 128504 ) ci_results &lt;- confint( result , level = 0.9 ) stopifnot( round( ( ci_results[ 2 ] - coef( result ) ) / 1000 , 0 ) == 388 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for AHS users, this code replicates previously-presented examples: library(srvyr) ahs_srvyr_design &lt;- as_survey( ahs_design ) Calculate the mean (average) of a linear variable, overall and by groups: ahs_srvyr_design %&gt;% summarize( mean = survey_mean( totrooms , na.rm = TRUE ) ) ahs_srvyr_design %&gt;% group_by( tenure ) %&gt;% summarize( mean = survey_mean( totrooms , na.rm = TRUE ) ) "],["american-time-use-survey-atus.html", "American Time Use Survey (ATUS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " American Time Use Survey (ATUS) Sampled individuals write down everything they do for a single twenty-four hour period, in ten minute intervals. Time use data allows for the study of uncompensated work like cooking, chores, childcare. Many tables with structures described in the user guide, linkable to the Current Population Survey. A complex survey generalizing to person-hours among civilian non-institutional americans aged 15+. Released annually since 2003. Administered by the Bureau of Labor Statistics. Please skim before you begin: American Time Use Survey User’s Guide Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # don&#39;t judge me bruno # eat one hour, sleep the rest # it&#39;s my lazy day Download, Import, Preparation Define a function to download, unzip, and import each comma-separated value dat file: library(RCurl) atus_csv_import &lt;- function( this_url ){ this_tf &lt;- tempfile() writeBin( getBinaryURL( this_url ) , this_tf ) unzipped_files &lt;- unzip( this_tf , exdir = tempdir() ) this_dat &lt;- grep( &#39;\\\\.dat$&#39; , unzipped_files , value = TRUE ) this_df &lt;- read.csv( this_dat ) file.remove( c( this_tf , unzipped_files ) ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } Download and import the activity, respondent, roster, and weights tables: act_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusact-2021.zip&quot; ) resp_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusresp-2021.zip&quot; ) rost_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusrost-2021.zip&quot; ) wgts_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atuswgts-2021.zip&quot; ) Specify which variables to keep in each of the data.frame objects: act_df &lt;- act_df[ c( &#39;tucaseid&#39; , &#39;tutier1code&#39; , &#39;tutier2code&#39; , &#39;tuactdur24&#39; ) ] resp_df &lt;- resp_df[ c( &#39;tucaseid&#39; , &#39;tufinlwgt&#39; , &#39;tulineno&#39; ) ] rost_df &lt;- rost_df[ , c( &#39;tucaseid&#39; , &#39;tulineno&#39; , &#39;teage&#39; , &#39;tesex&#39; ) ] Distribute travel-related activities (tutier1code == 18 from the lexicon) based on their second tier code: act_df[ act_df[ , &#39;tutier1code&#39; ] == 18 &amp; act_df[ , &#39;tutier2code&#39; ] == 99 , &#39;tutier1code&#39; ] &lt;- 50 act_df[ act_df[ , &#39;tutier1code&#39; ] == 18 , &#39;tutier1code&#39; ] &lt;- act_df[ act_df[ , &#39;tutier1code&#39; ] == 18 , &#39;tutier2code&#39; ] Sum up all durations at the (respondent x major activity category)-level: act_long_df &lt;- aggregate( tuactdur24 ~ tucaseid + tutier1code , data = act_df , sum ) act_wide_df &lt;- reshape( act_long_df , idvar = &#39;tucaseid&#39; , timevar = &#39;tutier1code&#39; , direction = &#39;wide&#39; ) # for individuals not engaging in an activity category, replace missings with zero minutes act_wide_df[ is.na( act_wide_df ) ] &lt;- 0 # for all columns except the respondent identifier, convert minutes to hours act_wide_df[ , -1 ] &lt;- act_wide_df[ , -1 ] / 60 Merge the respondent and summed activity tables, then the roster table, and finally the replicate weights: resp_act_df &lt;- merge( resp_df , act_wide_df ) stopifnot( nrow( resp_act_df ) == nrow( resp_df ) ) resp_act_rost_df &lt;- merge( resp_act_df , rost_df ) stopifnot( nrow( resp_act_rost_df ) == nrow( resp_df ) ) atus_df &lt;- merge( resp_act_rost_df , wgts_df ) stopifnot( nrow( atus_df ) == nrow( resp_df ) ) # remove dots from column names names( atus_df ) &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , names( atus_df ) ) atus_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # atus_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; , &quot;this_file.rds&quot; ) # saveRDS( atus_df , file = atus_fn , compress = FALSE ) Load the same object: # atus_df &lt;- readRDS( atus_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) atus_design &lt;- svrepdesign( weights = ~ tufinlwgt , repweights = &quot;finlwgt[0-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , mse = TRUE , data = atus_df ) Variable Recoding Add new columns to the data set: # caring for and helping household members is top level 03 from the lexicon # https://www.bls.gov/tus/lexicons/lexiconnoex2021.pdf atus_design &lt;- update( atus_design , any_care = as.numeric( tuactdur24_3 &gt; 0 ) , tesex = factor( tesex , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , age_category = factor( 1 + findInterval( teage , c( 18 , 35 , 65 ) ) , labels = c( &quot;under 18&quot; , &quot;18 - 34&quot; , &quot;35 - 64&quot; , &quot;65 or older&quot; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( atus_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_category , atus_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , atus_design ) svyby( ~ one , ~ age_category , atus_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ tuactdur24_1 , atus_design ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ tesex , atus_design ) svyby( ~ tesex , ~ age_category , atus_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ tuactdur24_1 , atus_design ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ tesex , atus_design ) svyby( ~ tesex , ~ age_category , atus_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ tuactdur24_1 , atus_design , 0.5 ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ tuactdur24_5 , denominator = ~ tuactdur24_12 , atus_design ) Subsetting Restrict the survey design to any time volunteering: sub_atus_design &lt;- subset( atus_design , tuactdur24_15 &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ tuactdur24_1 , sub_atus_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ tuactdur24_1 , atus_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( atus_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ tuactdur24_1 , atus_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ tuactdur24_1 , atus_design , deff = TRUE ) # SRS with replacement svymean( ~ tuactdur24_1 , atus_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ any_care , atus_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( tuactdur24_1 ~ any_care , atus_design ) Perform a chi-squared test of association for survey data: svychisq( ~ any_care + tesex , atus_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( tuactdur24_1 ~ any_care + tesex , atus_design ) summary( glm_result ) Intermish the author at second helping of quotidian coincidental truth-telling Replication Example This example matches the “Caring for and helping household members” row of Table A-1: hours_per_day_civilian_population &lt;- svymean( ~ tuactdur24_3 , atus_design ) stopifnot( round( coef( hours_per_day_civilian_population ) , 2 ) == 0.47 ) percent_engaged_per_day &lt;- svymean( ~ any_care , atus_design ) stopifnot( round( coef( percent_engaged_per_day ) , 3 ) == 0.217 ) hours_per_day_among_engaged &lt;- svymean( ~ tuactdur24_3 , subset( atus_design , any_care ) ) stopifnot( round( coef( hours_per_day_among_engaged ) , 2 ) == 2.17 ) This example matches the average hours and SE from Section 7.5 of the User’s Guide: Download and import the activity, activity summary, respondent, and weights tables: actsum07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atussum_2007.zip&quot; ) resp07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusresp_2007.zip&quot; ) act07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusact_2007.zip&quot; ) wgts07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atuswgts_2007.zip&quot; ) Option 1. Sum the two television fields from the activity summary file, removing zeroes: television_per_person &lt;- data.frame( tucaseid = actsum07_df[ , &#39;tucaseid&#39; ] , tuactdur24 = rowSums( actsum07_df[ , c( &#39;t120303&#39; , &#39;t120304&#39; ) ] ) ) television_per_person &lt;- television_per_person[ television_per_person[ , &#39;tuactdur24&#39; ] &gt; 0 , ] Option 2. Limit the activity file to television watching records according to the 2007 Lexicon: television_activity &lt;- subset( act07_df , tutier1code == 12 &amp; tutier2code == 3 &amp; tutier3code %in% 3:4 ) television_activity_summed &lt;- aggregate( tuactdur24 ~ tucaseid , data = television_activity , sum ) Confirm both aggregation options yield the same results: stopifnot( all( television_per_person[ , &#39;tucaseid&#39; ] == television_activity_summed[ , &#39;tucaseid&#39; ] ) ) stopifnot( all( television_per_person[ , &#39;tuactdur24&#39; ] == television_activity_summed[ , &#39;tuactdur24&#39; ] ) ) Merge the respondent and summed activity tables, then the replicate weights: resp07_tpp_df &lt;- merge( resp07_df[ , c( &#39;tucaseid&#39; , &#39;tufinlwgt&#39; ) ] , television_per_person , all.x = TRUE ) stopifnot( nrow( resp07_tpp_df ) == nrow( resp07_df ) ) # for individuals without television time, replace missings with zero minutes resp07_tpp_df[ is.na( resp07_tpp_df[ , &#39;tuactdur24&#39; ] ) , &#39;tuactdur24&#39; ] &lt;- 0 # convert minutes to hours resp07_tpp_df[ , &#39;tuactdur24_hour&#39; ] &lt;- resp07_tpp_df[ , &#39;tuactdur24&#39; ] / 60 atus07_df &lt;- merge( resp07_tpp_df , wgts07_df ) stopifnot( nrow( atus07_df ) == nrow( resp07_df ) ) Construct a complex sample survey design: atus07_design &lt;- svrepdesign( weights = ~ tufinlwgt , repweights = &quot;finlwgt[0-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , data = atus07_df ) Match the statistic and SE of the number of hours daily that americans older than 14 watch tv: result &lt;- svymean( ~ tuactdur24_hour , atus07_design ) stopifnot( round( coef( result ) , 2 ) == 2.62 ) stopifnot( round( SE( result ) , 4 ) == 0.0293 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ATUS users, this code replicates previously-presented examples: library(srvyr) atus_srvyr_design &lt;- as_survey( atus_design ) Calculate the mean (average) of a linear variable, overall and by groups: atus_srvyr_design %&gt;% summarize( mean = survey_mean( tuactdur24_1 ) ) atus_srvyr_design %&gt;% group_by( age_category ) %&gt;% summarize( mean = survey_mean( tuactdur24_1 ) ) "],["behavioral-risk-factor-surveillance-system-brfss.html", "Behavioral Risk Factor Surveillance System (BRFSS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " Behavioral Risk Factor Surveillance System (BRFSS) A health behavior telephone interview survey with enough sample size to examine all fifty states. One table with one row per telephone respondent. A complex survey designed to generalize to the civilian non-institutional adult population of the U.S. Released annually since 1984 but all states did not participate until 1994. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: BRFSS Data User Guide Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # a cellphone vibrates # it&#39;s the cdc! asking # if you ate veggies Download, Import, Preparation Download and import the national file: library(haven) zip_tf &lt;- tempfile() zip_url &lt;- &quot;https://www.cdc.gov/brfss/annual_data/2021/files/LLCP2021XPT.zip&quot; download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) brfss_tbl &lt;- read_xpt( zip_tf ) brfss_df &lt;- data.frame( brfss_tbl ) names( brfss_df ) &lt;- tolower( names( brfss_df ) ) brfss_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # brfss_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;BRFSS&quot; , &quot;this_file.rds&quot; ) # saveRDS( brfss_df , file = brfss_fn , compress = FALSE ) Load the same object: # brfss_df &lt;- readRDS( brfss_fn ) Survey Design Definition Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) variables_to_keep &lt;- c( &#39;one&#39; , &#39;x_psu&#39; , &#39;x_ststr&#39; , &#39;x_llcpwt&#39; , &#39;genhlth&#39; , &#39;medcost1&#39; , &#39;x_state&#39; , &#39;x_age80&#39; , &#39;nummen&#39; , &#39;numadult&#39; , &#39;x_hlthpln&#39; ) brfss_df &lt;- brfss_df[ variables_to_keep ] brfss_national_design &lt;- svydesign( id = ~ x_psu , strata = ~ x_ststr , data = brfss_df , weight = ~ x_llcpwt , nest = TRUE ) Since large linearized survey designs execute slowly, a replication design might be preferrable for exploratory analysis. Coefficients (such as means and medians) do not change, standard errors and confidence intervals differ slightly. The initial conversion with as.svrepdesign requires an extended period of processing time (perhaps run once overnight), subsequent analyses will finish much faster: # brfss_replication_design &lt;- # as.svrepdesign( # brfss_national_design , # type = &#39;bootstrap&#39; # ) # system.time( print( svymean( ~ x_age80 , brfss_national_design ) ) ) # system.time( print( svymean( ~ x_age80 , brfss_replication_design ) ) ) In this example, limit the national design to only Alaska for quicker processing: brfss_design &lt;- subset( brfss_national_design , x_state %in% 2 ) Variable Recoding Add new columns to the data set: brfss_design &lt;- update( brfss_design , fair_or_poor_health = ifelse( genhlth %in% 1:5 , as.numeric( genhlth &gt; 3 ) , NA ) , no_doc_visit_due_to_cost = factor( medcost1 , levels = c( 1 , 2 , 7 , 9 ) , labels = c( &quot;yes&quot; , &quot;no&quot; , &quot;dk&quot; , &quot;rf&quot; ) ) , state_name = factor( x_state , levels = c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 66, 72, 78) , labels = c(&quot;ALABAMA&quot;, &quot;ALASKA&quot;, &quot;ARIZONA&quot;, &quot;ARKANSAS&quot;, &quot;CALIFORNIA&quot;, &quot;COLORADO&quot;, &quot;CONNECTICUT&quot;, &quot;DELAWARE&quot;, &quot;DISTRICT OF COLUMBIA&quot;, &quot;FLORIDA&quot;, &quot;GEORGIA&quot;, &quot;HAWAII&quot;, &quot;IDAHO&quot;, &quot;ILLINOIS&quot;, &quot;INDIANA&quot;, &quot;IOWA&quot;, &quot;KANSAS&quot;, &quot;KENTUCKY&quot;, &quot;LOUISIANA&quot;, &quot;MAINE&quot;, &quot;MARYLAND&quot;, &quot;MASSACHUSETTS&quot;, &quot;MICHIGAN&quot;, &quot;MINNESOTA&quot;, &quot;MISSISSIPPI&quot;, &quot;MISSOURI&quot;, &quot;MONTANA&quot;, &quot;NEBRASKA&quot;, &quot;NEVADA&quot;, &quot;NEW HAMPSHIRE&quot;, &quot;NEW JERSEY&quot;, &quot;NEW MEXICO&quot;, &quot;NEW YORK&quot;, &quot;NORTH CAROLINA&quot;, &quot;NORTH DAKOTA&quot;, &quot;OHIO&quot;, &quot;OKLAHOMA&quot;, &quot;OREGON&quot;, &quot;PENNSYLVANIA&quot;, &quot;RHODE ISLAND&quot;, &quot;SOUTH CAROLINA&quot;, &quot;SOUTH DAKOTA&quot;, &quot;TENNESSEE&quot;, &quot;TEXAS&quot;, &quot;UTAH&quot;, &quot;VERMONT&quot;, &quot;VIRGINIA&quot;, &quot;WASHINGTON&quot;, &quot;WEST VIRGINIA&quot;, &quot;WISCONSIN&quot;, &quot;WYOMING&quot;, &quot;GUAM&quot;, &quot;PUERTO RICO&quot;, &quot;U.S. VIRGIN ISLANDS&quot;) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( brfss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , brfss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , brfss_design ) svyby( ~ one , ~ state_name , brfss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ x_age80 , brfss_design ) svyby( ~ x_age80 , ~ state_name , brfss_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ no_doc_visit_due_to_cost , brfss_design , na.rm = TRUE ) svyby( ~ no_doc_visit_due_to_cost , ~ state_name , brfss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ x_age80 , brfss_design ) svyby( ~ x_age80 , ~ state_name , brfss_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ no_doc_visit_due_to_cost , brfss_design , na.rm = TRUE ) svyby( ~ no_doc_visit_due_to_cost , ~ state_name , brfss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ x_age80 , brfss_design , 0.5 ) svyby( ~ x_age80 , ~ state_name , brfss_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ nummen , denominator = ~ numadult , brfss_design , na.rm = TRUE ) Subsetting Restrict the survey design to persons without health insurance: sub_brfss_design &lt;- subset( brfss_design , x_hlthpln == 2 ) Calculate the mean (average) of this subset: svymean( ~ x_age80 , sub_brfss_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ x_age80 , brfss_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ x_age80 , ~ state_name , brfss_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( brfss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ x_age80 , brfss_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ x_age80 , brfss_design , deff = TRUE ) # SRS with replacement svymean( ~ x_age80 , brfss_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ fair_or_poor_health , brfss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( x_age80 ~ fair_or_poor_health , brfss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ fair_or_poor_health + no_doc_visit_due_to_cost , brfss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( x_age80 ~ fair_or_poor_health + no_doc_visit_due_to_cost , brfss_design ) summary( glm_result ) Intermish the author as yoda with a soda Replication Example This example matches Alaska’s confidence intervals from the BRFSS Prevalence &amp; Trends Data: result &lt;- svymean( ~ no_doc_visit_due_to_cost , subset( brfss_design , no_doc_visit_due_to_cost %in% c( &#39;yes&#39; , &#39;no&#39; ) ) , na.rm = TRUE ) stopifnot( round( confint( result )[ 1 , 1 ] , 3 ) == 0.094 ) stopifnot( round( confint( result )[ 1 , 2 ] , 3 ) == 0.122 ) stopifnot( round( confint( result )[ 2 , 1 ] , 3 ) == 0.878 ) stopifnot( round( confint( result )[ 2 , 2 ] , 3 ) == 0.906 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for BRFSS users, this code replicates previously-presented examples: library(srvyr) brfss_srvyr_design &lt;- as_survey( brfss_design ) Calculate the mean (average) of a linear variable, overall and by groups: brfss_srvyr_design %&gt;% summarize( mean = survey_mean( x_age80 ) ) brfss_srvyr_design %&gt;% group_by( state_name ) %&gt;% summarize( mean = survey_mean( x_age80 ) ) "],["consumer-expenditure-survey-ces.html", "Consumer Expenditure Survey (CES) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " Consumer Expenditure Survey (CES) A household budget survey designed to guide major economic indicators like the Consumer Price Index. One table of survey responses per quarter with one row per sampled household (consumer unit). Additional tables containing one record per expenditure. A complex sample survey designed to generalize to the civilian non-institutional U.S. population. Released annually since 1996. Administered by the Bureau of Labor Statistics. Please skim before you begin: Consumer Expenditure Surveys Public Use Microdata Getting Started Guide Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # price indices and # you spent how much on beans, jack? # pocketbook issues Download, Import, Preparation Download both the prior and current year of interview microdata: library(RCurl) tf_prior_year &lt;- tempfile() this_url_prior_year &lt;- &quot;https://www.bls.gov/cex/pumd/data/stata/intrvw21.zip&quot; writeBin( getBinaryURL( this_url_prior_year ) , tf_prior_year ) unzipped_files_prior_year &lt;- unzip( tf_prior_year , exdir = tempdir() ) tf_current_year &lt;- tempfile() this_url_current_year &lt;- &quot;https://www.bls.gov/cex/pumd/data/stata/intrvw22.zip&quot; writeBin( getBinaryURL( this_url_current_year ) , tf_current_year ) unzipped_files_current_year &lt;- unzip( tf_current_year , exdir = tempdir() ) unzipped_files &lt;- c( unzipped_files_current_year , unzipped_files_prior_year ) Import and stack all 2022 quarterly files plus 2023’s first quarter: library(haven) fmli_files &lt;- grep( &quot;fmli2[2-3]&quot; , unzipped_files , value = TRUE ) fmli_tbls &lt;- lapply( fmli_files , read_dta ) fmli_dfs &lt;- lapply( fmli_tbls , data.frame ) fmli_dfs &lt;- lapply( fmli_dfs , function( w ){ names( w ) &lt;- tolower( names( w ) ) ; w } ) fmli_cols &lt;- lapply( fmli_dfs , names ) intersecting_cols &lt;- Reduce( intersect , fmli_cols ) fmli_dfs &lt;- lapply( fmli_dfs , function( w ) w[ intersecting_cols ] ) ces_df &lt;- do.call( rbind , fmli_dfs ) Scale the weight columns based on the number of months in 2022: ces_df[ , c( &#39;qintrvyr&#39; , &#39;qintrvmo&#39; ) ] &lt;- sapply( ces_df[ , c( &#39;qintrvyr&#39; , &#39;qintrvmo&#39; ) ] , as.numeric ) weight_columns &lt;- grep( &#39;wt&#39; , names( ces_df ) , value = TRUE ) ces_df &lt;- transform( ces_df , mo_scope = ifelse( qintrvyr %in% 2022 &amp; qintrvmo %in% 1:3 , qintrvmo - 1 , ifelse( qintrvyr %in% 2023 , 4 - qintrvmo , 3 ) ) ) for ( this_column in weight_columns ){ ces_df[ is.na( ces_df[ , this_column ] ) , this_column ] &lt;- 0 ces_df[ , paste0( &#39;popwt_&#39; , this_column ) ] &lt;- ( ces_df[ , this_column ] * ces_df[ , &#39;mo_scope&#39; ] / 12 ) } Combine previous quarter and current quarter variables into a single variable: expenditure_variables &lt;- gsub( &quot;pq$&quot; , &quot;&quot; , grep( &quot;pq$&quot; , names( ces_df ) , value = TRUE ) ) # confirm that for every variable ending in pq, # there&#39;s the same variable ending in cq stopifnot( all( paste0( expenditure_variables , &#39;cq&#39; ) %in% names( ces_df ) ) ) # confirm none of the variables without the pq or cq suffix exist if( any( expenditure_variables %in% names( ces_df ) ) ) stop( &quot;variable conflict&quot; ) for( this_column in expenditure_variables ){ ces_df[ , this_column ] &lt;- rowSums( ces_df[ , paste0( this_column , c( &#39;pq&#39; , &#39;cq&#39; ) ) ] , na.rm = TRUE ) # annualize the quarterly spending ces_df[ , this_column ] &lt;- 4 * ces_df[ , this_column ] ces_df[ is.na( ces_df[ , this_column ] ) , this_column ] &lt;- 0 } Append any interview survey UCC found at https://www.bls.gov/cex/ce_source_integrate.xlsx: ucc_exp &lt;- c( &quot;450110&quot; , &quot;450210&quot; ) mtbi_files &lt;- grep( &quot;mtbi2[2-3]&quot; , unzipped_files , value = TRUE ) mtbi_tbls &lt;- lapply( mtbi_files , read_dta ) mtbi_dfs &lt;- lapply( mtbi_tbls , data.frame ) mtbi_dfs &lt;- lapply( mtbi_dfs , function( w ){ names( w ) &lt;- tolower( names( w ) ) ; w } ) mtbi_dfs &lt;- lapply( mtbi_dfs , function( w ) w[ c( &#39;newid&#39; , &#39;cost&#39; , &#39;ucc&#39; , &#39;ref_yr&#39; ) ] ) mtbi_df &lt;- do.call( rbind , mtbi_dfs ) mtbi_df &lt;- subset( mtbi_df , ( ref_yr %in% 2022 ) &amp; ( ucc %in% ucc_exp ) ) mtbi_agg &lt;- aggregate( cost ~ newid , data = mtbi_df , sum ) names( mtbi_agg ) &lt;- c( &#39;newid&#39; , &#39;new_car_truck_exp&#39; ) before_nrow &lt;- nrow( ces_df ) ces_df &lt;- merge( ces_df , mtbi_agg , all.x = TRUE ) stopifnot( nrow( ces_df ) == before_nrow ) ces_df[ is.na( ces_df[ , &#39;new_car_truck_exp&#39; ] ) , &#39;new_car_truck_exp&#39; ] &lt;- 0 Save locally   Save the object at any point: # ces_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; , &quot;this_file.rds&quot; ) # saveRDS( ces_df , file = ces_fn , compress = FALSE ) Load the same object: # ces_df &lt;- readRDS( ces_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: Separate the ces_df data.frame into five implicates, each differing from the others only in the multiply-imputed variables: library(survey) library(mitools) # create a vector containing all of the multiply-imputed variables # (leaving the numbers off the end) mi_vars &lt;- gsub( &quot;5$&quot; , &quot;&quot; , grep( &quot;[a-z]5$&quot; , names( ces_df ) , value = TRUE ) ) # loop through each of the five variables.. for ( i in 1:5 ){ # copy the &#39;ces_df&#39; table over to a new temporary data frame &#39;x&#39; x &lt;- ces_df # loop through each of the multiply-imputed variables.. for ( j in mi_vars ){ # copy the contents of the current column (for example &#39;welfare1&#39;) # over to a new column ending in &#39;mi&#39; (for example &#39;welfaremi&#39;) x[ , paste0( j , &#39;mi&#39; ) ] &lt;- x[ , paste0( j , i ) ] # delete the all five of the imputed variable columns x &lt;- x[ , !( names( x ) %in% paste0( j , 1:5 ) ) ] } assign( paste0( &#39;imp&#39; , i ) , x ) } ces_design &lt;- svrepdesign( weights = ~ finlwt21 , repweights = &quot;^wtrep[0-9][0-9]$&quot; , data = imputationList( list( imp1 , imp2 , imp3 , imp4 , imp5 ) ) , type = &quot;BRR&quot; , combined.weights = TRUE , mse = TRUE ) Variable Recoding Add new columns to the data set: ces_design &lt;- update( ces_design , one = 1 , any_food_stamp = as.numeric( jfs_amtmi &gt; 0 ) , bls_urbn = factor( bls_urbn , levels = 1:2 , labels = c( &#39;urban&#39; , &#39;rural&#39; ) ) , sex_ref = factor( sex_ref , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( ces_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( ces_design , svyby( ~ one , ~ bls_urbn , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( ces_design , svytotal( ~ one ) ) ) MIcombine( with( ces_design , svyby( ~ one , ~ bls_urbn , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( ces_design , svymean( ~ totexp ) ) ) MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( ces_design , svymean( ~ sex_ref ) ) ) MIcombine( with( ces_design , svyby( ~ sex_ref , ~ bls_urbn , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( ces_design , svytotal( ~ totexp ) ) ) MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( ces_design , svytotal( ~ sex_ref ) ) ) MIcombine( with( ces_design , svyby( ~ sex_ref , ~ bls_urbn , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( ces_design , svyquantile( ~ totexp , 0.5 , se = TRUE ) ) ) MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svyquantile , 0.5 , se = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( ces_design , svyratio( numerator = ~ totexp , denominator = ~ fincbtxmi ) ) ) Subsetting Restrict the survey design to california residents: sub_ces_design &lt;- subset( ces_design , state == &#39;06&#39; ) Calculate the mean (average) of this subset: MIcombine( with( sub_ces_design , svymean( ~ totexp ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( ces_design , svymean( ~ totexp ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ces_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( ces_design , svyvar( ~ totexp ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( ces_design , svymean( ~ totexp , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( ces_design , svymean( ~ totexp , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ any_food_stamp , ces_design , # method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( totexp ~ any_food_stamp , ces_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ any_food_stamp + sex_ref , ces_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( ces_design , svyglm( totexp ~ any_food_stamp + sex_ref ) ) ) summary( glm_result ) Intermish the author as idiom Replication Example This example matches the number of consumer units and the Cars and trucks, new rows of Table R-1: result &lt;- MIcombine( with( ces_design , svytotal( ~ as.numeric( popwt_finlwt21 / finlwt21 ) ) ) ) stopifnot( round( coef( result ) , -3 ) == 134090000 ) results &lt;- sapply( weight_columns , function( this_column ){ sum( ces_df[ , &#39;new_car_truck_exp&#39; ] * ces_df[ , this_column ] ) / sum( ces_df[ , paste0( &#39;popwt_&#39; , this_column ) ] ) } ) stopifnot( round( results[1] , 2 ) == 2195.30 ) standard_error &lt;- sqrt( ( 1 / 44 ) * sum( ( results[-1] - results[1] )^2 ) ) stopifnot( round( standard_error , 2 ) == 174.02 ) # note the minor differences MIcombine( with( ces_design , svymean( ~ cartkn ) ) ) "],["census-of-governments-cog.html", "Census of Governments (COG) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " Census of Governments (COG) Location, employment, and payroll for state and local (but not federal) government agencies in the U.S. One record per agency, one per agency function, plus the government units master address file. Complete enumeration of civilian employment in state and local governments in the 50 states + D.C. The Annual Survey of Public Employment &amp; Payroll becomes a census in years ending with 2 and 7. Administered and financed by the US Census Bureau. Please skim before you begin: 2022 Census of Governments, Survey of Public Employment &amp; Payroll Methodology Government Units Survey Methodology This human-composed haiku or a bouquet of artificial intelligence-generated limericks # courthouse steps wedding # schools police fire water # no fed mail invite Download, Import, Preparation Download, import, and stack the government units listing file: library(readxl) tf_gus &lt;- tempfile() gus_url &lt;- &quot;https://www2.census.gov/programs-surveys/gus/datasets/2022/govt_units_2022.ZIP&quot; download.file( gus_url , tf_gus , mode = &#39;wb&#39; ) unzipped_files_gus &lt;- unzip( tf_gus , exdir = tempdir() ) xlsx_gus_fn &lt;- grep( &quot;\\\\.xlsx$&quot; , unzipped_files_gus , value = TRUE ) xlsx_sheets &lt;- excel_sheets( xlsx_gus_fn ) # read all sheets into a list of tibbles gus_tbl_list &lt;- lapply( xlsx_sheets , function( w ) read_excel( xlsx_gus_fn , sheet = w ) ) # convert all tibbles to data.frame objects gus_df_list &lt;- lapply( gus_tbl_list , data.frame ) # lowercase all column names gus_df_list &lt;- lapply( gus_df_list , function( w ){ names( w ) &lt;- tolower( names( w ) ) ; w } ) # add the excel tab source to each data.frame for( i in seq( xlsx_sheets ) ) gus_df_list[[ i ]][ , &#39;source_tab&#39; ] &lt;- xlsx_sheets[ i ] # determine which columns are in all tables column_intersect &lt;- Reduce( intersect , lapply( gus_df_list , names ) ) # determine which columns are in some but not all tables column_union &lt;- unique( unlist( lapply( gus_df_list , names ) ) ) # these columns will be discarded by stacking: unique( unlist( lapply( lapply( gus_df_list , names ) , function( w ) column_union[ !column_union %in% w ] ) ) ) # stack all excel sheets, keeping only the columns that all tables have in common gus_df &lt;- Reduce( rbind , lapply( gus_df_list , function( w ) w[ column_intersect ] ) ) Download and import the survey of public employment &amp; payroll, one record per function (not per unit): tf_apes &lt;- tempfile() apes_url &lt;- paste0( &quot;https://www2.census.gov/programs-surveys/apes/datasets/&quot; , &quot;2022/2022%20COG-E%20Individual%20Unit%20Files.zip&quot; ) download.file( apes_url , tf_apes , mode = &#39;wb&#39; ) unzipped_files_apes &lt;- unzip( tf_apes , exdir = tempdir() ) xlsx_apes_fn &lt;- grep( &quot;\\\\.xlsx$&quot; , unzipped_files_apes , value = TRUE ) apes_tbl &lt;- read_excel( xlsx_apes_fn ) apes_df &lt;- data.frame( apes_tbl ) names( apes_df ) &lt;- tolower( names( apes_df ) ) Review the non-matching records between these two tables, then merge: # all DEP School Districts and a third of Special Districts are not in the `apes_df` table( gus_df[ , &#39;census_id_gidid&#39; ] %in% apes_df[ , &#39;individual.unit.id&#39; ] , gus_df[ , &#39;source_tab&#39; ] , useNA = &#39;always&#39; ) # state governments are not in the `gus_df` table( apes_df[ , &#39;individual.unit.id&#39; ] %in% gus_df[ , &#39;census_id_gidid&#39; ] , apes_df[ , &#39;type.of.government&#39; ] , useNA = &#39;always&#39; ) # check for overlapping field names: ( overlapping_names &lt;- intersect( names( apes_df ) , names( gus_df ) ) ) # rename the state column in `gus_df` to state abbreviation names( gus_df )[ names( gus_df ) == &#39;state&#39; ] &lt;- &#39;stateab&#39; double_df &lt;- merge( apes_df , gus_df , by.x = &#39;individual.unit.id&#39; , by.y = &#39;census_id_gidid&#39; , all.x = TRUE ) stopifnot( nrow( double_df ) == nrow( apes_df ) ) # replace dots with underscores names( double_df ) &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , names( double_df ) ) Keep either the one record per agency rows or the one record per function rows: # `Total - All Government Employment Functions` records sum to the same as all other records: with( double_df , tapply( full_time_employees , grepl( &quot;Total&quot; , government_function ) , sum ) ) with( double_df , tapply( part_time_payroll , grepl( &quot;Total&quot; , government_function ) , sum ) ) # keep one record per government function (multiple records per agency): cog_df &lt;- subset( double_df , !grepl( &quot;Total&quot; , government_function ) ) # keep one record per government agency: # cog_df &lt;- subset( double_df , grepl( &quot;Total&quot; , government_function ) ) Save locally   Save the object at any point: # cog_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;COG&quot; , &quot;this_file.rds&quot; ) # saveRDS( cog_df , file = cog_fn , compress = FALSE ) Load the same object: # cog_df &lt;- readRDS( cog_fn ) Variable Recoding Add new columns to the data set: cog_df &lt;- transform( cog_df , one = 1 , total_payroll = full_time_payroll + part_time_payroll , total_employees = full_time_employees + part_time_employees , any_full_time_employees = full_time_employees &gt; 0 ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( cog_df ) table( cog_df[ , &quot;type_of_government&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( cog_df[ , &quot;full_time_employees&quot; ] ) tapply( cog_df[ , &quot;full_time_employees&quot; ] , cog_df[ , &quot;type_of_government&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( cog_df[ , &quot;census_region&quot; ] ) ) prop.table( table( cog_df[ , c( &quot;census_region&quot; , &quot;type_of_government&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( cog_df[ , &quot;full_time_employees&quot; ] ) tapply( cog_df[ , &quot;full_time_employees&quot; ] , cog_df[ , &quot;type_of_government&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( cog_df[ , &quot;full_time_employees&quot; ] , 0.5 ) tapply( cog_df[ , &quot;full_time_employees&quot; ] , cog_df[ , &quot;type_of_government&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to Elementary, Secondary, Higher, and Other Educational Government Agencies: sub_cog_df &lt;- subset( cog_df , grepl( &#39;Education&#39; , government_function ) ) Calculate the mean (average) of this subset: mean( sub_cog_df[ , &quot;full_time_employees&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( cog_df[ , &quot;full_time_employees&quot; ] ) tapply( cog_df[ , &quot;full_time_employees&quot; ] , cog_df[ , &quot;type_of_government&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( full_time_employees ~ any_full_time_employees , cog_df ) Perform a chi-squared test of association: this_table &lt;- table( cog_df[ , c( &quot;any_full_time_employees&quot; , &quot;census_region&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( full_time_employees ~ any_full_time_employees + census_region , data = cog_df ) summary( glm_result ) Intermish the author as anachronism Replication Example This example matches excel cell “C17” of Employment &amp; Payroll Data by State and by Function: financial_admin_df &lt;- subset( cog_df , government_function == &#39;Financial Administration&#39; ) stopifnot( sum( financial_admin_df[ , &#39;full_time_employees&#39; ] ) == 404228 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for COG users, this code replicates previously-presented examples: library(dplyr) cog_tbl &lt;- as_tibble( cog_df ) Calculate the mean (average) of a linear variable, overall and by groups: cog_tbl %&gt;% summarize( mean = mean( full_time_employees ) ) cog_tbl %&gt;% group_by( type_of_government ) %&gt;% summarize( mean = mean( full_time_employees ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for COG users, this code replicates previously-presented examples: library(data.table) cog_dt &lt;- data.table( cog_df ) Calculate the mean (average) of a linear variable, overall and by groups: cog_dt[ , mean( full_time_employees ) ] cog_dt[ , mean( full_time_employees ) , by = type_of_government ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for COG users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;cog&#39; , cog_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( full_time_employees ) FROM cog&#39; ) dbGetQuery( con , &#39;SELECT type_of_government , AVG( full_time_employees ) FROM cog GROUP BY type_of_government&#39; ) "],["current-population-survey-cps.html", "Current Population Survey (CPS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Poverty and Inequality Estimation with convey   Analysis Examples with srvyr  ", " Current Population Survey (CPS) The principal labor force survey, providing income, poverty, and health insurance coverage estimates. One table with one row per sampled household, a second table with one row per family within each sampled household, and a third table with one row per individual within each of those families. A complex sample designed to generalize to the civilian non-institutional population of the US. Released annually since 1998, linkable to the Basic Monthly releases. Administered jointly by the US Census Bureau and the Bureau of Labor Statistics. Please skim before you begin: Current Population Survey 2023 Annual Social and Economic (ASEC) Supplement Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # jobs robbed by robot # luddite rebellion looming # blue, due to red pill Download, Import, Preparation Download and unzip the 2023 file: library(httr) tf &lt;- tempfile() this_url &lt;- &quot;https://www2.census.gov/programs-surveys/cps/datasets/2023/march/asecpub23sas.zip&quot; GET( this_url , write_disk( tf ) , progress() ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) Import all four files: library(haven) four_tbl &lt;- lapply( unzipped_files , read_sas ) four_df &lt;- lapply( four_tbl , data.frame ) four_df &lt;- lapply( four_df , function( w ){ names( w ) &lt;- tolower( names( w ) ) ; w } ) household_df &lt;- four_df[[ grep( &#39;hhpub&#39; , basename( unzipped_files ) ) ]] family_df &lt;- four_df[[ grep( &#39;ffpub&#39; , basename( unzipped_files ) ) ]] person_df &lt;- four_df[[ grep( &#39;pppub&#39; , basename( unzipped_files ) ) ]] repwgts_df &lt;- four_df[[ grep( &#39;repwgt&#39; , basename( unzipped_files ) ) ]] Divide weights: household_df[ , &#39;hsup_wgt&#39; ] &lt;- household_df[ , &#39;hsup_wgt&#39; ] / 100 family_df[ , &#39;fsup_wgt&#39; ] &lt;- family_df[ , &#39;fsup_wgt&#39; ] / 100 for ( j in c( &#39;marsupwt&#39; , &#39;a_ernlwt&#39; , &#39;a_fnlwgt&#39; ) ) person_df[ , j ] &lt;- person_df[ , j ] / 100 Merge these four files: names( family_df )[ names( family_df ) == &#39;fh_seq&#39; ] &lt;- &#39;h_seq&#39; names( person_df )[ names( person_df ) == &#39;ph_seq&#39; ] &lt;- &#39;h_seq&#39; names( person_df )[ names( person_df ) == &#39;phf_seq&#39; ] &lt;- &#39;ffpos&#39; hh_fm_df &lt;- merge( household_df , family_df ) hh_fm_pr_df &lt;- merge( hh_fm_df , person_df ) cps_df &lt;- merge( hh_fm_pr_df , repwgts_df ) stopifnot( nrow( cps_df ) == nrow( person_df ) ) Save locally   Save the object at any point: # cps_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;CPS&quot; , &quot;this_file.rds&quot; ) # saveRDS( cps_df , file = cps_fn , compress = FALSE ) Load the same object: # cps_df &lt;- readRDS( cps_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) cps_design &lt;- svrepdesign( weights = ~ marsupwt , repweights = &quot;pwwgt[1-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , data = cps_df , combined.weights = TRUE , mse = TRUE ) Variable Recoding Add new columns to the data set: cps_design &lt;- update( cps_design , one = 1 , a_maritl = factor( a_maritl , labels = c( &quot;married - civilian spouse present&quot; , &quot;married - AF spouse present&quot; , &quot;married - spouse absent&quot; , &quot;widowed&quot; , &quot;divorced&quot; , &quot;separated&quot; , &quot;never married&quot; ) ) , state_name = factor( gestfips , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;) ) , male = as.numeric( a_sex == 1 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( cps_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , cps_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , cps_design ) svyby( ~ one , ~ state_name , cps_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ ptotval , cps_design ) svyby( ~ ptotval , ~ state_name , cps_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ a_maritl , cps_design ) svyby( ~ a_maritl , ~ state_name , cps_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ ptotval , cps_design ) svyby( ~ ptotval , ~ state_name , cps_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ a_maritl , cps_design ) svyby( ~ a_maritl , ~ state_name , cps_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ ptotval , cps_design , 0.5 ) svyby( ~ ptotval , ~ state_name , cps_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ moop , denominator = ~ ptotval , cps_design ) Subsetting Restrict the survey design to persons aged 18-64: sub_cps_design &lt;- subset( cps_design , a_age %in% 18:64 ) Calculate the mean (average) of this subset: svymean( ~ ptotval , sub_cps_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ ptotval , cps_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ ptotval , ~ state_name , cps_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( cps_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ ptotval , cps_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ ptotval , cps_design , deff = TRUE ) # SRS with replacement svymean( ~ ptotval , cps_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , cps_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( ptotval ~ male , cps_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + a_maritl , cps_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( ptotval ~ male + a_maritl , cps_design ) summary( glm_result ) Intermish johannes vermeer’s girl with a pearl earring Replication Example This example matches the count and share of individuals with health insurance in Table H-01: count_covered &lt;- svytotal( ~ as.numeric( cov == 1 ) , cps_design ) stopifnot( round( coef( count_covered ) , -5 ) == 304000000 ) stopifnot( round( coef( count_covered ) - confint( count_covered , level = 0.9 )[1] , -3 ) == 746000 ) share_covered &lt;- svymean( ~ as.numeric( cov == 1 ) , subset( cps_design , cov &gt; 0 ) ) stopifnot( round( coef( share_covered ) , 3 ) == 0.921 ) stopifnot( round( coef( share_covered ) - confint( share_covered , level = 0.9 )[1] , 3 ) == 0.002 ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for CPS users, this code calculates the gini coefficient on complex sample survey data: library(convey) cps_design &lt;- convey_prep( cps_design ) cps_household_design &lt;- subset( cps_design , a_exprrp %in% 1:2 ) svygini( ~ htotval , cps_household_design ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for CPS users, this code replicates previously-presented examples: library(srvyr) cps_srvyr_design &lt;- as_survey( cps_design ) Calculate the mean (average) of a linear variable, overall and by groups: cps_srvyr_design %&gt;% summarize( mean = survey_mean( ptotval ) ) cps_srvyr_design %&gt;% group_by( state_name ) %&gt;% summarize( mean = survey_mean( ptotval ) ) "],["exame-nacional-de-desempenho-de-estudantes-enade.html", "Exame Nacional de Desempenho de Estudantes (ENADE) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " Exame Nacional de Desempenho de Estudantes (ENADE) The nationwide mandatory examination of college graduates. One table with one row per individual undergraduate student in Brazil. An enumeration of undergraduate students in Brazil. Released annually since 2004. Compiled by the Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP). Please skim before you begin: Cálculo da nota final do Exame Nacional de Desempenho dos Estudiantes Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # undergraduates # sit for standardized testing # exit interview Download, Import, Preparation Download, import, and merge two of the 2021 files: library(httr) library(archive) tf &lt;- tempfile() this_url &lt;- &quot;https://download.inep.gov.br/microdados/microdados_enade_2021.zip&quot; GET( this_url , write_disk( tf ) , progress() ) archive_extract( tf , dir = tempdir() ) read_enade_archive &lt;- function( this_regular_expression , this_directory ){ this_filename &lt;- grep( this_regular_expression , list.files( this_directory , recursive = TRUE , full.names = TRUE ) , value = TRUE ) this_df &lt;- read.table( this_filename , header = TRUE , sep = &quot;;&quot; , na.strings = &quot;&quot; ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } arq1_df &lt;- read_enade_archive( &#39;arq1\\\\.txt$&#39; , tempdir() ) arq1_df &lt;- unique( arq1_df[ c( &#39;co_curso&#39; , &#39;co_uf_curso&#39; , &#39;co_categad&#39; , &#39;co_grupo&#39; ) ] ) arq3_df &lt;- read_enade_archive( &#39;arq3\\\\.txt$&#39; , tempdir() ) enade_df &lt;- merge( arq3_df , arq1_df ) stopifnot( nrow( enade_df ) == nrow( arq3_df ) ) Save locally   Save the object at any point: # enade_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ENADE&quot; , &quot;this_file.rds&quot; ) # saveRDS( enade_df , file = enade_fn , compress = FALSE ) Load the same object: # enade_df &lt;- readRDS( enade_fn ) Variable Recoding Add new columns to the data set: enade_df &lt;- transform( enade_df , # qual foi o tempo gasto por voce para concluir a prova? less_than_two_hours = as.numeric( co_rs_i9 %in% c( &#39;A&#39; , &#39;B&#39; ) ) , administrative_category = factor( co_categad , levels = c( 1:5 , 7 ) , labels = c( &#39;1. Pública Federal&#39; , &#39;2. Pública Estadual&#39; , &#39;3. Pública Municipal&#39; , &#39;4. Privada com fins lucrativos&#39; , &#39;5. Privada sem fins lucrativos&#39; , &#39;7. Especial&#39; ) ) , state_name = factor( co_uf_curso , levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) , labels = c( &quot;Rondonia&quot; , &quot;Acre&quot; , &quot;Amazonas&quot; , &quot;Roraima&quot; , &quot;Para&quot; , &quot;Amapa&quot; , &quot;Tocantins&quot; , &quot;Maranhao&quot; , &quot;Piaui&quot; , &quot;Ceara&quot; , &quot;Rio Grande do Norte&quot; , &quot;Paraiba&quot; , &quot;Pernambuco&quot; , &quot;Alagoas&quot; , &quot;Sergipe&quot; , &quot;Bahia&quot; , &quot;Minas Gerais&quot; , &quot;Espirito Santo&quot; , &quot;Rio de Janeiro&quot; , &quot;Sao Paulo&quot; , &quot;Parana&quot; , &quot;Santa Catarina&quot; , &quot;Rio Grande do Sul&quot; , &quot;Mato Grosso do Sul&quot; , &quot;Mato Grosso&quot; , &quot;Goias&quot; , &quot;Distrito Federal&quot; ) ) ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( enade_df ) table( enade_df[ , &quot;administrative_category&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( enade_df[ , &quot;state_name&quot; ] ) ) prop.table( table( enade_df[ , c( &quot;state_name&quot; , &quot;administrative_category&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( enade_df[ , &quot;nt_obj_fg&quot; ] , 0.5 , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to students reporting that the general training section was easy or very easy: sub_enade_df &lt;- subset( enade_df , co_rs_i1 %in% c( &quot;A&quot; , &quot;B&quot; ) ) Calculate the mean (average) of this subset: mean( sub_enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( nt_obj_fg ~ less_than_two_hours , enade_df ) Perform a chi-squared test of association: this_table &lt;- table( enade_df[ , c( &quot;less_than_two_hours&quot; , &quot;state_name&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( nt_obj_fg ~ less_than_two_hours + state_name , data = enade_df ) summary( glm_result ) Intermish edvard munch’s the scream Replication Example This example matches the tecnologia em gestão da tecnologia da informação test scores on PDF page 48 of the 2021 final results document: it_students &lt;- subset( enade_df , co_grupo %in% 6409 ) results &lt;- sapply( it_students[ c( &#39;nt_fg&#39; , &#39;nt_ce&#39; , &#39;nt_ger&#39; ) ] , mean , na.rm = TRUE ) stopifnot( round( results[ &#39;nt_fg&#39; ] , 1 ) == 30.4 ) stopifnot( round( results[ &#39;nt_ce&#39; ] , 1 ) == 38.2 ) stopifnot( round( results[ &#39;nt_ger&#39; ] , 1 ) == 36.3 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for ENADE users, this code replicates previously-presented examples: library(dplyr) enade_tbl &lt;- as_tibble( enade_df ) Calculate the mean (average) of a linear variable, overall and by groups: enade_tbl %&gt;% summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) ) enade_tbl %&gt;% group_by( administrative_category ) %&gt;% summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for ENADE users, this code replicates previously-presented examples: library(data.table) enade_dt &lt;- data.table( enade_df ) Calculate the mean (average) of a linear variable, overall and by groups: enade_dt[ , mean( nt_obj_fg , na.rm = TRUE ) ] enade_dt[ , mean( nt_obj_fg , na.rm = TRUE ) , by = administrative_category ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for ENADE users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;enade&#39; , enade_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( nt_obj_fg ) FROM enade&#39; ) dbGetQuery( con , &#39;SELECT administrative_category , AVG( nt_obj_fg ) FROM enade GROUP BY administrative_category&#39; ) "],["exame-nacional-do-ensino-medio-enem.html", "Exame Nacional do Ensino Medio (ENEM) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table  ", " Exame Nacional do Ensino Medio (ENEM) The national student aptitude test, used to assess high school completion and university admission. One table with one row per test-taking student, a second of study habit questionnaire respondents. Updated annually since 1998. Maintained by Brazil’s Instituto Nacional de Estudos e Pesquisas Educacionais Anisio Teixeira Please skim before you begin: Leia_Me_Enem included in each annual zipped file Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # graduation stage # shake hands, toss cap, unroll scroll, # mais um exame? Download, Import, Preparation Download and unzip the 2022 file: library(httr) library(archive) tf &lt;- tempfile() this_url &lt;- &quot;https://download.inep.gov.br/microdados/microdados_enem_2022.zip&quot; GET( this_url , write_disk( tf ) , progress() ) archive_extract( tf , dir = tempdir() ) Import the 2022 file: library(readr) enem_fns &lt;- list.files( tempdir() , recursive = TRUE , full.names = TRUE ) enem_fn &lt;- grep( &quot;MICRODADOS_ENEM_([0-9][0-9][0-9][0-9])\\\\.csv$&quot; , enem_fns , value = TRUE ) enem_tbl &lt;- read_csv2( enem_fn ) enem_df &lt;- data.frame( enem_tbl ) names( enem_df ) &lt;- tolower( names( enem_df ) ) Save locally   Save the object at any point: # enem_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ENEM&quot; , &quot;this_file.rds&quot; ) # saveRDS( enem_df , file = enem_fn , compress = FALSE ) Load the same object: # enem_df &lt;- readRDS( enem_fn ) Variable Recoding Add new columns to the data set: enem_df &lt;- transform( enem_df , domestic_worker = as.numeric( q007 %in% c( &#39;B&#39; , &#39;C&#39; , &#39;D&#39; ) ) , administrative_category = factor( tp_dependencia_adm_esc , levels = 1:4 , labels = c( &#39;Federal&#39; , &#39;Estadual&#39; , &#39;Municipal&#39; , &#39;Privada&#39; ) ) , state_name = factor( co_uf_esc , levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) , labels = c( &quot;Rondonia&quot; , &quot;Acre&quot; , &quot;Amazonas&quot; , &quot;Roraima&quot; , &quot;Para&quot; , &quot;Amapa&quot; , &quot;Tocantins&quot; , &quot;Maranhao&quot; , &quot;Piaui&quot; , &quot;Ceara&quot; , &quot;Rio Grande do Norte&quot; , &quot;Paraiba&quot; , &quot;Pernambuco&quot; , &quot;Alagoas&quot; , &quot;Sergipe&quot; , &quot;Bahia&quot; , &quot;Minas Gerais&quot; , &quot;Espirito Santo&quot; , &quot;Rio de Janeiro&quot; , &quot;Sao Paulo&quot; , &quot;Parana&quot; , &quot;Santa Catarina&quot; , &quot;Rio Grande do Sul&quot; , &quot;Mato Grosso do Sul&quot; , &quot;Mato Grosso&quot; , &quot;Goias&quot; , &quot;Distrito Federal&quot; ) ) ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( enem_df ) table( enem_df[ , &quot;administrative_category&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( enem_df[ , &quot;nu_nota_mt&quot; ] , na.rm = TRUE ) tapply( enem_df[ , &quot;nu_nota_mt&quot; ] , enem_df[ , &quot;administrative_category&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( enem_df[ , &quot;state_name&quot; ] ) ) prop.table( table( enem_df[ , c( &quot;state_name&quot; , &quot;administrative_category&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( enem_df[ , &quot;nu_nota_mt&quot; ] , na.rm = TRUE ) tapply( enem_df[ , &quot;nu_nota_mt&quot; ] , enem_df[ , &quot;administrative_category&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( enem_df[ , &quot;nu_nota_mt&quot; ] , 0.5 , na.rm = TRUE ) tapply( enem_df[ , &quot;nu_nota_mt&quot; ] , enem_df[ , &quot;administrative_category&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to mother graduated from high school: sub_enem_df &lt;- subset( enem_df , q002 %in% c( &#39;E&#39; , &#39;F&#39; , &#39;G&#39; ) ) Calculate the mean (average) of this subset: mean( sub_enem_df[ , &quot;nu_nota_mt&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( enem_df[ , &quot;nu_nota_mt&quot; ] , na.rm = TRUE ) tapply( enem_df[ , &quot;nu_nota_mt&quot; ] , enem_df[ , &quot;administrative_category&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( nu_nota_mt ~ domestic_worker , enem_df ) Perform a chi-squared test of association: this_table &lt;- table( enem_df[ , c( &quot;domestic_worker&quot; , &quot;state_name&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( nu_nota_mt ~ domestic_worker + state_name , data = enem_df ) summary( glm_result ) Intermish van gogh’s self-portrait with bandaged ear Replication Example This example matches the registration counts in the Sinopse ENEM 2022 Excel table: stopifnot( nrow( enem_df ) == 3476105 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for ENEM users, this code replicates previously-presented examples: library(dplyr) enem_tbl &lt;- as_tibble( enem_df ) Calculate the mean (average) of a linear variable, overall and by groups: enem_tbl %&gt;% summarize( mean = mean( nu_nota_mt , na.rm = TRUE ) ) enem_tbl %&gt;% group_by( administrative_category ) %&gt;% summarize( mean = mean( nu_nota_mt , na.rm = TRUE ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for ENEM users, this code replicates previously-presented examples: library(data.table) enem_dt &lt;- data.table( enem_df ) Calculate the mean (average) of a linear variable, overall and by groups: enem_dt[ , mean( nu_nota_mt , na.rm = TRUE ) ] enem_dt[ , mean( nu_nota_mt , na.rm = TRUE ) , by = administrative_category ] "],["european-social-survey-ess.html", "European Social Survey (ESS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " European Social Survey (ESS) The barometer of political opinion and behavior across the continent. One table per country with one row per sampled respondent. A complex sample designed to generalize to residents aged 15 and older in participating nations. Released biennially since 2002. Headquartered at City, University of London and governed by a scientific team across Europe. Please skim before you begin: Findings from the European Social Survey Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # pent up belief gauge # open border monarchists # survey for your thoughts Download, Import, Preparation Register at the ESS Data Portal at https://ess-search.nsd.no/. Choose ESS round 8 - 2016. Welfare attitudes, Attitudes to climate change. Download the integrated file and also the sample design (SDDF) files as SAV (SPSS) files: library(foreign) ess_int_df &lt;- read.spss( file.path( path.expand( &quot;~&quot; ) , &quot;ESS8e02_2.sav&quot; ) , to.data.frame = TRUE , use.value.labels = FALSE ) ess_sddf_df &lt;- read.spss( file.path( path.expand( &quot;~&quot; ) , &quot;ESS8SDDFe01_1.sav&quot; ) , to.data.frame = TRUE , use.value.labels = FALSE ) ess_df &lt;- merge( ess_int_df , ess_sddf_df , by = c( &#39;cntry&#39; , &#39;idno&#39; ) ) stopifnot( nrow( ess_df ) == nrow( ess_int_df ) ) Save locally   Save the object at any point: # ess_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;this_file.rds&quot; ) # saveRDS( ess_df , file = ess_fn , compress = FALSE ) Load the same object: # ess_df &lt;- readRDS( ess_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) options( survey.lonely.psu = &quot;adjust&quot; ) ess_df[ , &#39;anweight&#39; ] &lt;- ess_df[ , &#39;pspwght&#39; ] * ess_df[ , &#39;pweight&#39; ] * 10000 ess_design &lt;- svydesign( ids = ~psu , strata = ~stratum , weights = ~anweight , data = ess_df , nest = TRUE ) Variable Recoding Add new columns to the data set: ess_design &lt;- update( ess_design , one = 1 , gndr = factor( gndr , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , netusoft = factor( netusoft , levels = 1:5 , labels = c( &#39;Never&#39; , &#39;Only occasionally&#39; , &#39;A few times a week&#39; , &#39;Most days&#39; , &#39;Every day&#39; ) ) , belonging_to_particular_religion = as.numeric( rlgblg == 1 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( ess_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cntry , ess_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , ess_design ) svyby( ~ one , ~ cntry , ess_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ ppltrst , ess_design , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ gndr , ess_design , na.rm = TRUE ) svyby( ~ gndr , ~ cntry , ess_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ ppltrst , ess_design , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ gndr , ess_design , na.rm = TRUE ) svyby( ~ gndr , ~ cntry , ess_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ ppltrst , ess_design , 0.5 , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ppltrst , denominator = ~ pplfair , ess_design , na.rm = TRUE ) Subsetting Restrict the survey design to voters: sub_ess_design &lt;- subset( ess_design , vote == 1 ) Calculate the mean (average) of this subset: svymean( ~ ppltrst , sub_ess_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ ppltrst , ess_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ ppltrst , ~ cntry , ess_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ess_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ ppltrst , ess_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ ppltrst , ess_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ ppltrst , ess_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ belonging_to_particular_religion , ess_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( ppltrst ~ belonging_to_particular_religion , ess_design ) Perform a chi-squared test of association for survey data: svychisq( ~ belonging_to_particular_religion + gndr , ess_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( ppltrst ~ belonging_to_particular_religion + gndr , ess_design ) summary( glm_result ) Intermish the author as blue man pantocrator Replication Example This example matches statistics and confidence intervals within 0.1% from the Guide to Using Weights and Sample Design Indicators with ESS Data: published_proportions &lt;- c( 0.166 , 0.055 , 0.085 , 0.115 , 0.578 ) published_lb &lt;- c( 0.146 , 0.045 , 0.072 , 0.099 , 0.550 ) published_ub &lt;- c( 0.188 , 0.068 , 0.100 , 0.134 , 0.605 ) austrians &lt;- subset( ess_design , cntry == &#39;AT&#39; ) ( results &lt;- svymean( ~ netusoft , austrians , na.rm = TRUE ) ) stopifnot( all( round( coef( results ) , 3 ) == published_proportions ) ) ( ci_results &lt;- confint( results ) ) stopifnot( all( abs( ci_results[ , 1 ] - published_lb ) &lt; 0.0015 ) ) stopifnot( all( abs( ci_results[ , 2 ] - published_ub ) &lt; 0.0015 ) ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ESS users, this code replicates previously-presented examples: library(srvyr) ess_srvyr_design &lt;- as_survey( ess_design ) Calculate the mean (average) of a linear variable, overall and by groups: ess_srvyr_design %&gt;% summarize( mean = survey_mean( ppltrst , na.rm = TRUE ) ) ess_srvyr_design %&gt;% group_by( cntry ) %&gt;% summarize( mean = survey_mean( ppltrst , na.rm = TRUE ) ) "],["fda-adverse-event-reporting-system-faers.html", "FDA Adverse Event Reporting System (FAERS) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " FDA Adverse Event Reporting System (FAERS) The post-marketing safety surveillance program for drug and therapeutic biological products. Multiple tables linked by primaryid including demographics, outcomes, drug start and end dates. Voluntary reports from practitioners and patients, not representative, no verification of causality. Published quarterly since 2004, file structure revisions at 2012Q4 and 2014Q3. Maintained by the United States Food and Drug Administration (FDA). Please skim before you begin: ASC_NTS.DOC included in each quarterly zipped file, especially the Entity Relationship Diagram Questions and Answers on FDA’s Adverse Event Reporting System (FAERS) This human-composed haiku or a bouquet of artificial intelligence-generated limericks # side effect guestbook # violet you&#39;re turning violet # vi&#39;lent dose response Download, Import, Preparation Download the quarterly file: library(httr) tf &lt;- tempfile() this_url &lt;- &quot;https://fis.fda.gov/content/Exports/faers_ascii_2023q1.zip&quot; GET( this_url , write_disk( tf ) , progress() ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) Define a function to import each text file: read_faers &lt;- function( this_fn ){ read.table( this_fn , sep = &quot;$&quot; , header = TRUE , comment.char = &quot;&quot; , quote = &quot;&quot; ) } Import multiple tables from the downloaded quarter of microdata: # one record per report faers_demo_df &lt;- read_faers( grep( &#39;DEMO23Q1\\\\.txt$&#39; , unzipped_files , value = TRUE ) ) # one or more record per report faers_drug_df &lt;- read_faers( grep( &#39;DRUG23Q1\\\\.txt$&#39; , unzipped_files , value = TRUE ) ) # zero or more records per report faers_outcome_df &lt;- read_faers( grep( &#39;OUTC23Q1\\\\.txt$&#39; , unzipped_files , value = TRUE ) ) Construct an analysis file limited to reported deaths: # limit the outcome file to deaths faers_deaths_df &lt;- subset( faers_outcome_df , outc_cod == &#39;DE&#39; ) # merge demographics with each reported death faers_df &lt;- merge( faers_demo_df , faers_deaths_df ) # confirm that the analysis file matches the number of death outcomes stopifnot( nrow( faers_deaths_df ) == nrow( faers_df ) ) # confirm zero reports include multiple deaths from the same reported adverse event stopifnot( nrow( faers_df ) == length( unique( faers_df[ , &#39;primaryid&#39; ] ) ) ) Save locally   Save the object at any point: # faers_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;FAERS&quot; , &quot;this_file.rds&quot; ) # saveRDS( faers_df , file = faers_fn , compress = FALSE ) Load the same object: # faers_df &lt;- readRDS( faers_fn ) Variable Recoding Add new columns to the data set: faers_df &lt;- transform( faers_df , physician_reported = as.numeric( occp_cod == &quot;MD&quot; ) , reporter_country_categories = ifelse( reporter_country == &#39;US&#39; , &#39;USA&#39; , ifelse( reporter_country == &#39;COUNTRY NOT SPECIFIED&#39; , &#39;missing&#39; , ifelse( reporter_country == &#39;JP&#39; , &#39;Japan&#39; , ifelse( reporter_country == &#39;UK&#39; , &#39;UK&#39; , ifelse( reporter_country == &#39;CA&#39; , &#39;Canada&#39; , ifelse( reporter_country == &#39;FR&#39; , &#39;France&#39; , &#39;Other&#39; ) ) ) ) ) ) , init_fda_year = as.numeric( substr( init_fda_dt , 1 , 4 ) ) ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( faers_df ) table( faers_df[ , &quot;reporter_country_categories&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;reporter_country_categories&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( faers_df[ , &quot;sex&quot; ] ) ) prop.table( table( faers_df[ , c( &quot;sex&quot; , &quot;reporter_country_categories&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;reporter_country_categories&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( faers_df[ , &quot;init_fda_year&quot; ] , 0.5 , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;reporter_country_categories&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to elderly persons: sub_faers_df &lt;- subset( faers_df , age_grp == &quot;E&quot; ) Calculate the mean (average) of this subset: mean( sub_faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;reporter_country_categories&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( init_fda_year ~ physician_reported , faers_df ) Perform a chi-squared test of association: this_table &lt;- table( faers_df[ , c( &quot;physician_reported&quot; , &quot;sex&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( init_fda_year ~ physician_reported + sex , data = faers_df ) summary( glm_result ) Intermish “had my conspirators arrived armed with tomatoes instead,the red taste would have been no less surprising”-julius caesar Replication Example This example matches the death frequency counts in the OUTC23Q1.pdf file in the downloaded quarter: stopifnot( nrow( faers_df ) == 37704 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for FAERS users, this code replicates previously-presented examples: library(dplyr) faers_tbl &lt;- as_tibble( faers_df ) Calculate the mean (average) of a linear variable, overall and by groups: faers_tbl %&gt;% summarize( mean = mean( init_fda_year , na.rm = TRUE ) ) faers_tbl %&gt;% group_by( reporter_country_categories ) %&gt;% summarize( mean = mean( init_fda_year , na.rm = TRUE ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for FAERS users, this code replicates previously-presented examples: library(data.table) faers_dt &lt;- data.table( faers_df ) Calculate the mean (average) of a linear variable, overall and by groups: faers_dt[ , mean( init_fda_year , na.rm = TRUE ) ] faers_dt[ , mean( init_fda_year , na.rm = TRUE ) , by = reporter_country_categories ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for FAERS users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;faers&#39; , faers_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( init_fda_year ) FROM faers&#39; ) dbGetQuery( con , &#39;SELECT reporter_country_categories , AVG( init_fda_year ) FROM faers GROUP BY reporter_country_categories&#39; ) "],["general-social-survey-gss.html", "General Social Survey (GSS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " General Social Survey (GSS) A historical record of the concerns, experiences, attitudes, and practices of residents of the United States. Both cross-sectional and panel tables with one row per sampled respondent. A complex sample survey generalizing to non-institutionalized adults (18+) in the United States. Updated biennially since 1972. Funded by National Science Foundation, administered by the National Opinion Research Center. Please skim before you begin: DOCUMENTATION AND PUBLIC USE FILE CODEBOOK (Release 1) Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # chat about who will # be allowed marriage, children. # first date questionnaire Download, Import, Preparation Download and import the 1972-2022 cumulative data file: library(haven) zip_tf &lt;- tempfile() zip_url &lt;- &quot;https://gss.norc.org/Documents/sas/GSS_sas.zip&quot; download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( zip_tf , exdir = tempdir() ) gss_tbl &lt;- read_sas( grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files , value = TRUE ) ) gss_df &lt;- data.frame( gss_tbl ) names( gss_df ) &lt;- tolower( names( gss_df ) ) gss_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # gss_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;GSS&quot; , &quot;this_file.rds&quot; ) # saveRDS( gss_df , file = gss_fn , compress = FALSE ) Load the same object: # gss_df &lt;- readRDS( gss_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) options( survey.lonely.psu = &quot;adjust&quot; ) gss_design &lt;- svydesign( ~ vpsu , strata = ~ interaction( year , vstrat ) , data = subset( gss_df , year &gt;= 1975 ) , weights = ~ wtsscomp , nest = TRUE ) Variable Recoding Add new columns to the data set: gss_design &lt;- update( gss_design , polviews = factor( polviews , levels = 1:7 , labels = c( &quot;Extremely liberal&quot; , &quot;Liberal&quot; , &quot;Slightly liberal&quot; , &quot;Moderate, middle of the road&quot; , &quot;Slightly conservative&quot; , &quot;Conservative&quot; , &quot;Extremely conservative&quot; ) ) , born_in_usa = as.numeric( born == 1 ) , race = factor( race , levels = 1:3 , labels = c( &quot;white&quot; , &quot;black&quot; , &quot;other&quot; ) ) , region = factor( region , levels = 1:9 , labels = c( &quot;New England&quot; , &quot;Middle Atlantic&quot; , &quot;East North Central&quot; , &quot;West North Central&quot; , &quot;South Atlantic&quot; , &quot;East South Central&quot; , &quot;West South Central&quot; , &quot;Mountain&quot; , &quot;Pacific&quot; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( gss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region , gss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , gss_design ) svyby( ~ one , ~ region , gss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ age , gss_design , na.rm = TRUE ) svyby( ~ age , ~ region , gss_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ race , gss_design , na.rm = TRUE ) svyby( ~ race , ~ region , gss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ age , gss_design , na.rm = TRUE ) svyby( ~ age , ~ region , gss_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ race , gss_design , na.rm = TRUE ) svyby( ~ race , ~ region , gss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ age , gss_design , 0.5 , na.rm = TRUE ) svyby( ~ age , ~ region , gss_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ adults , denominator = ~ hompop , gss_design , na.rm = TRUE ) Subsetting Restrict the survey design to females: sub_gss_design &lt;- subset( gss_design , sex == 2 ) Calculate the mean (average) of this subset: svymean( ~ age , sub_gss_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ age , gss_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ age , ~ region , gss_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( gss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ age , gss_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ age , gss_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ age , gss_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ born_in_usa , gss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( age ~ born_in_usa , gss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ born_in_usa + race , gss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( age ~ born_in_usa + race , gss_design ) summary( glm_result ) Intermish ze author as zebra Replication Example Match the unweighted record count on PDF page 10 of the Public Use File codebook: stopifnot( nrow( subset( gss_design , year == 2022 ) ) == 3544 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for GSS users, this code replicates previously-presented examples: library(srvyr) gss_srvyr_design &lt;- as_survey( gss_design ) Calculate the mean (average) of a linear variable, overall and by groups: gss_srvyr_design %&gt;% summarize( mean = survey_mean( age , na.rm = TRUE ) ) gss_srvyr_design %&gt;% group_by( region ) %&gt;% summarize( mean = survey_mean( age , na.rm = TRUE ) ) "],["medicare-current-beneficiary-survey-mcbs.html", "Medicare Current Beneficiary Survey (MCBS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " Medicare Current Beneficiary Survey (MCBS) The monitoring system for Medicare enrollees in the United States on topics not available in the program’s administrative data, such as out of pocket expenditure and beneficiary satisfaction. Survey and supplemental tables with one row per sampled individual, although downloadable datasets not linkable. A complex sample survey designed to generalize to all elderly and disabled individuals with at least one month of program enrollment during the calendar year. Released annually as a public use file since 2015. Conducted by the Office of Enterprise Data and Analytics (OEDA) of the Centers for Medicare &amp; Medicaid Services (CMS) through a contract with NORC at the University of Chicago. Please skim before you begin: MCBS Methodology Report MCBS Advanced Tutorial on Weighting and Variance Estimation This human-composed haiku or a bouquet of artificial intelligence-generated limericks # old, or disabled # access to medical care, # utilization Download, Import, Preparation tf &lt;- tempfile() this_url &lt;- &quot;https://www.cms.gov/files/zip/cspuf2019.zip&quot; download.file( this_url , tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) mcbs_csv &lt;- grep( &#39;\\\\.csv$&#39; , unzipped_files , value = TRUE ) mcbs_df &lt;- read.csv( mcbs_csv ) names( mcbs_df ) &lt;- tolower( names( mcbs_df ) ) Save locally   Save the object at any point: # mcbs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;MCBS&quot; , &quot;this_file.rds&quot; ) # saveRDS( mcbs_df , file = mcbs_fn , compress = FALSE ) Load the same object: # mcbs_df &lt;- readRDS( mcbs_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) mcbs_design &lt;- svrepdesign( weight = ~cspufwgt , repweights = &#39;cspuf[0-9]+&#39; , mse = TRUE , type = &#39;Fay&#39; , rho = 0.3 , data = mcbs_df ) Variable Recoding Add new columns to the data set: mcbs_design &lt;- update( mcbs_design , one = 1 , csp_age = factor( csp_age , levels = 1:3 , labels = c( &#39;01: younger than 65&#39; , &#39;02: 65 to 74&#39; , &#39;03: 75 or older&#39; ) ) , two_or_more_chronic_conditions = as.numeric( csp_nchrncnd &gt; 1 ) , csp_sex = factor( csp_sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( mcbs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ csp_age , mcbs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , mcbs_design ) svyby( ~ one , ~ csp_age , mcbs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ pamtoop , mcbs_design ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ csp_sex , mcbs_design ) svyby( ~ csp_sex , ~ csp_age , mcbs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ pamtoop , mcbs_design ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ csp_sex , mcbs_design ) svyby( ~ csp_sex , ~ csp_age , mcbs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ pamtoop , mcbs_design , 0.5 ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ pamtoop , denominator = ~ pamttot , mcbs_design ) Subsetting Restrict the survey design to household income below $25,000: sub_mcbs_design &lt;- subset( mcbs_design , csp_income == 1 ) Calculate the mean (average) of this subset: svymean( ~ pamtoop , sub_mcbs_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ pamtoop , mcbs_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ pamtoop , ~ csp_age , mcbs_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( mcbs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ pamtoop , mcbs_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ pamtoop , mcbs_design , deff = TRUE ) # SRS with replacement svymean( ~ pamtoop , mcbs_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ two_or_more_chronic_conditions , mcbs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( pamtoop ~ two_or_more_chronic_conditions , mcbs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ two_or_more_chronic_conditions + csp_sex , mcbs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( pamtoop ~ two_or_more_chronic_conditions + csp_sex , mcbs_design ) summary( glm_result ) Intermish the author as kenny rogers Replication Example This example matches the weighted total from the 2019 Data User’s Guide: Cost Supplement File Public Use File: stopifnot( round( coef( svytotal( ~ one , mcbs_design ) ) , 0 ) == 56307461 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for MCBS users, this code replicates previously-presented examples: library(srvyr) mcbs_srvyr_design &lt;- as_survey( mcbs_design ) Calculate the mean (average) of a linear variable, overall and by groups: mcbs_srvyr_design %&gt;% summarize( mean = survey_mean( pamtoop ) ) mcbs_srvyr_design %&gt;% group_by( csp_age ) %&gt;% summarize( mean = survey_mean( pamtoop ) ) "],["medical-expenditure-panel-survey-meps.html", "Medical Expenditure Panel Survey (MEPS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " Medical Expenditure Panel Survey (MEPS) The Household Component captures person-level spending across service categories, coverage types. The consolidated file contains one row per individual within each sampled household, other tables contain one record per event (like prescription fills, hospitalizations), per job, per insurance policy. A complex sample survey designed to generalize to the U.S. civilian non-institutionalized population. Released annually since 1996. Administered by the Agency for Healthcare Research and Quality. Please skim before you begin: MEPS HC-224 2020 Full Year Consolidated Data File Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # king dumpty&#39;s horsemen # ahrq stitches payors, bills, claims # fractured health system Download, Import, Preparation Define a function to download, unzip, and import each sas file: library(haven) meps_sas_import &lt;- function( this_url ){ this_tf &lt;- tempfile() download.file( this_url , this_tf , mode = &#39;wb&#39; ) this_tbl &lt;- read_sas( this_tf ) this_df &lt;- data.frame( this_tbl ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } Download and import the consolidated file and the replicate weights file: meps_cons_df &lt;- meps_sas_import( &quot;https://meps.ahrq.gov/data_files/pufs/h224/h224v9.zip&quot; ) meps_brr_df &lt;- meps_sas_import( &quot;https://meps.ahrq.gov/mepsweb/data_files/pufs/h036brr/h36brr20v9.zip&quot; ) Merge the consolidated file with the replicate weights: meps_df &lt;- merge( meps_cons_df , meps_brr_df ) stopifnot( nrow( meps_df ) == nrow( meps_cons_df ) ) meps_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # meps_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;MEPS&quot; , &quot;this_file.rds&quot; ) # saveRDS( meps_df , file = meps_fn , compress = FALSE ) Load the same object: # meps_df &lt;- readRDS( meps_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) meps_design &lt;- svrepdesign( data = meps_df , weights = ~ perwt20f , type = &quot;BRR&quot; , combined.weights = FALSE , repweights = &quot;brr[1-9]+&quot; , mse = TRUE ) Variable Recoding Add new columns to the data set: meps_design &lt;- update( meps_design , one = 1 , insured_december_31st = ifelse( ins20x %in% 1:2 , as.numeric( ins20x == 1 ) , NA ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( meps_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region20 , meps_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , meps_design ) svyby( ~ one , ~ region20 , meps_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ totexp20 , meps_design ) svyby( ~ totexp20 , ~ region20 , meps_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , meps_design ) svyby( ~ sex , ~ region20 , meps_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ totexp20 , meps_design ) svyby( ~ totexp20 , ~ region20 , meps_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , meps_design ) svyby( ~ sex , ~ region20 , meps_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ totexp20 , meps_design , 0.5 ) svyby( ~ totexp20 , ~ region20 , meps_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ totmcd20 , denominator = ~ totexp20 , meps_design ) Subsetting Restrict the survey design to seniors: sub_meps_design &lt;- subset( meps_design , agelast &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ totexp20 , sub_meps_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ totexp20 , meps_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ totexp20 , ~ region20 , meps_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( meps_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ totexp20 , meps_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ totexp20 , meps_design , deff = TRUE ) # SRS with replacement svymean( ~ totexp20 , meps_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ insured_december_31st , meps_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( totexp20 ~ insured_december_31st , meps_design ) Perform a chi-squared test of association for survey data: svychisq( ~ insured_december_31st + sex , meps_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( totexp20 ~ insured_december_31st + sex , meps_design ) summary( glm_result ) Intermish the author as anatomical diagram with musical typo Replication Example This example matches the statistic and standard error shown under Analysis of the Total Population: library(foreign) xport_2002_tf &lt;- tempfile() xport_2002_url &lt;- &quot;https://meps.ahrq.gov/data_files/pufs/h70ssp.zip&quot; download.file( xport_2002_url , xport_2002_tf , mode = &#39;wb&#39; ) unzipped_2002_xport &lt;- unzip( xport_2002_tf , exdir = tempdir() ) meps_2002_df &lt;- read.xport( unzipped_2002_xport ) names( meps_2002_df ) &lt;- tolower( names( meps_2002_df ) ) meps_2002_design &lt;- svydesign( ~ varpsu , strata = ~ varstr , weights = ~ perwt02f , data = meps_2002_df , nest = TRUE ) result &lt;- svymean( ~ totexp02 , meps_2002_design ) stopifnot( round( coef( result ) , 2 ) == 2813.24 ) stopifnot( round( SE( result ) , 2 ) == 58.99 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for MEPS users, this code replicates previously-presented examples: library(srvyr) meps_srvyr_design &lt;- as_survey( meps_design ) Calculate the mean (average) of a linear variable, overall and by groups: meps_srvyr_design %&gt;% summarize( mean = survey_mean( totexp20 ) ) meps_srvyr_design %&gt;% group_by( region20 ) %&gt;% summarize( mean = survey_mean( totexp20 ) ) "],["medical-large-claims-experience-study-mlces.html", "Medical Large Claims Experience Study (MLCES) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " Medical Large Claims Experience Study (MLCES) A high quality dataset of medical claims from seven private health insurance companies. One table with one row per individual with nonzero total paid charges. A convenience sample of group (employer-sponsored) health insurers in the United States. 1997 thru 1999 with no expected updates in the future. Provided by the Society of Actuaries (SOA). Please skim before you begin: Group Medical Insurance Claims Database Collection and Analysis Report Claim Severities, Claim Relativities, and Age: Evidence from SOA Group Health Data This human-composed haiku or a bouquet of artificial intelligence-generated limericks # skewed by black swan tails # means, medians sing adieu # claims distribution Download, Import, Preparation Download and import the 1999 medical claims file: tf &lt;- tempfile() this_url &lt;- &quot;https://www.soa.org/Files/Research/1999.zip&quot; download.file( this_url , tf , mode = &#39;wb&#39; ) unzipped_file &lt;- unzip( tf , exdir = tempdir() ) mlces_df &lt;- read.csv( unzipped_file ) names( mlces_df ) &lt;- tolower( names( mlces_df ) ) Save locally   Save the object at any point: # mlces_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;MLCES&quot; , &quot;this_file.rds&quot; ) # saveRDS( mlces_df , file = mlces_fn , compress = FALSE ) Load the same object: # mlces_df &lt;- readRDS( mlces_fn ) Variable Recoding Add new columns to the data set: mlces_df &lt;- transform( mlces_df , one = 1 , claimant_relationship_to_policyholder = ifelse( relation == &quot;E&quot; , &quot;covered employee&quot; , ifelse( relation == &quot;S&quot; , &quot;spouse of covered employee&quot; , ifelse( relation == &quot;D&quot; , &quot;dependent of covered employee&quot; , NA ) ) ) , ppo_plan = as.numeric( ppo == &#39;Y&#39; ) ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( mlces_df ) table( mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( mlces_df[ , &quot;patsex&quot; ] ) ) prop.table( table( mlces_df[ , c( &quot;patsex&quot; , &quot;claimant_relationship_to_policyholder&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( mlces_df[ , &quot;totpdchg&quot; ] , 0.5 ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to persons under 18: sub_mlces_df &lt;- subset( mlces_df , ( ( claimyr - patbrtyr ) &lt; 18 ) ) Calculate the mean (average) of this subset: mean( sub_mlces_df[ , &quot;totpdchg&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( totpdchg ~ ppo_plan , mlces_df ) Perform a chi-squared test of association: this_table &lt;- table( mlces_df[ , c( &quot;ppo_plan&quot; , &quot;patsex&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( totpdchg ~ ppo_plan + patsex , data = mlces_df ) summary( glm_result ) Intermish rembrandt’s the anatomy lesson of dr. nicolaes tulp Replication Example This example matches statistics in Table II-A’s 1999 row numbers 52 and 53 from the Database: Match Claimants Exceeding Deductible: # $0 deductible stopifnot( nrow( mlces_df ) == 1591738 ) # $1,000 deductible mlces_above_1000_df &lt;- subset( mlces_df , totpdchg &gt; 1000 ) stopifnot( nrow( mlces_above_1000_df ) == 402550 ) Match the Excess Charges Above Deductible: # $0 deductible stopifnot( round( sum( mlces_df[ , &#39;totpdchg&#39; ] ) , 0 ) == 2599356658 ) # $1,000 deductible stopifnot( round( sum( mlces_above_1000_df[ , &#39;totpdchg&#39; ] - 1000 ) , 0 ) == 1883768786 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for MLCES users, this code replicates previously-presented examples: library(dplyr) mlces_tbl &lt;- as_tibble( mlces_df ) Calculate the mean (average) of a linear variable, overall and by groups: mlces_tbl %&gt;% summarize( mean = mean( totpdchg ) ) mlces_tbl %&gt;% group_by( claimant_relationship_to_policyholder ) %&gt;% summarize( mean = mean( totpdchg ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for MLCES users, this code replicates previously-presented examples: library(data.table) mlces_dt &lt;- data.table( mlces_df ) Calculate the mean (average) of a linear variable, overall and by groups: mlces_dt[ , mean( totpdchg ) ] mlces_dt[ , mean( totpdchg ) , by = claimant_relationship_to_policyholder ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for MLCES users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;mlces&#39; , mlces_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( totpdchg ) FROM mlces&#39; ) dbGetQuery( con , &#39;SELECT claimant_relationship_to_policyholder , AVG( totpdchg ) FROM mlces GROUP BY claimant_relationship_to_policyholder&#39; ) "],["national-beneficiary-survey-nbs.html", "National Beneficiary Survey (NBS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " National Beneficiary Survey (NBS) The principal microdata for U.S. disability researchers interested in Social Security program performance. One table with one row per respondent. A complex sample designed to generalize to Americans between age 18 and full retirement age, covered by either Social Security Disability Insurance (SSDI) or Supplemental Security Income (SSI). Released at irregular intervals, with 2004, 2005, 2006, 2010, 2015, 2017, and 2019 available. Administered by the Social Security Administration. Please skim before you begin: National Beneficiary Survey: Disability Statistics, 2015 National Beneficiary Survey - General Waves Round 7: User’s Guide This human-composed haiku or a bouquet of artificial intelligence-generated limericks # social safety net # poverty acrobatics # trap or trampoline Download, Import, Preparation Download and import the round 7 file: library(haven) zip_tf &lt;- tempfile() zip_url &lt;- &quot;https://www.ssa.gov/disabilityresearch/documents/R7NBSPUF_STATA.zip&quot; download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) nbs_tbl &lt;- read_stata( zip_tf ) nbs_df &lt;- data.frame( nbs_tbl ) names( nbs_df ) &lt;- tolower( names( nbs_df ) ) nbs_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # nbs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NBS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nbs_df , file = nbs_fn , compress = FALSE ) Load the same object: # nbs_df &lt;- readRDS( nbs_fn ) Survey Design Definition Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) # representative beneficiary sample nbs_design &lt;- svydesign( id = ~ r7_a_psu_pub , strata = ~ r7_a_strata , weights = ~ r7_wtr7_ben , data = subset( nbs_df , r7_wtr7_ben &gt; 0 ) ) # cross-sectional successful worker sample nbs_design &lt;- svydesign( id = ~ r7_a_psu_pub , strata = ~ r7_a_strata , weights = ~ r7_wtr7_cssws , data = subset( nbs_df , r7_wtr7_cssws &gt; 0 ) ) # longitudinal successful worker sample lngsws_design &lt;- svydesign( id = ~ r7_a_psu_pub , strata = ~ r7_a_strata , weights = ~ r7_wtr7_lngsws , data = subset( nbs_df , r7_wtr7_lngsws &gt; 0 ) ) Variable Recoding Add new columns to the data set: nbs_design &lt;- update( nbs_design , male = as.numeric( r7_orgsampinfo_sex == 1 ) , age_categories = factor( r7_c_intage_pub , labels = c( &quot;18-25&quot; , &quot;26-40&quot; , &quot;41-55&quot; , &quot;56 and older&quot; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nbs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_categories , nbs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nbs_design ) svyby( ~ one , ~ age_categories , nbs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ r7_c_hhsize_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_c_hhsize_pub , ~ age_categories , nbs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ r7_c_hhsize_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_c_hhsize_pub , ~ age_categories , nbs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ r7_n_totssbenlastmnth_pub , nbs_design , 0.5 , na.rm = TRUE ) svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ r7_n_ssilastmnth_pub , denominator = ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) Subsetting Restrict the survey design to currently covered by Medicare: sub_nbs_design &lt;- subset( nbs_design , r7_c_curmedicare == 1 ) Calculate the mean (average) of this subset: svymean( ~ r7_n_totssbenlastmnth_pub , sub_nbs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nbs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , nbs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( r7_n_totssbenlastmnth_pub ~ male , nbs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + r7_c_hhsize_pub , nbs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( r7_n_totssbenlastmnth_pub ~ male + r7_c_hhsize_pub , nbs_design ) summary( glm_result ) Intermish the author as jubilation lee Replication Example This example matches the percentages and t-tests from the final ten rows of Exhibit 4: ex_4 &lt;- data.frame( variable_label = c( &#39;coping with stress&#39; , &#39;concentrating&#39; , &#39;getting around outside of the home&#39; , &#39;shopping for personal items&#39; , &#39;preparing meals&#39; , &#39;getting into or out of bed&#39; , &#39;bathing or dressing&#39; , &#39;getting along with others&#39; , &#39;getting around inside the house&#39; , &#39;eating&#39; ) , variable_name = c( &quot;r3_i60_i&quot; , &quot;r3_i59_i&quot; , &quot;r3_i47_i&quot; , &quot;r3_i53_i&quot; , &quot;r3_i55_i&quot; , &quot;r3_i49_i&quot; , &quot;r3_i51_i&quot; , &quot;r3_i61_i&quot; , &quot;r3_i45_i&quot; , &quot;r3_i57_i&quot; ) , overall = c( 61 , 58 , 47 , 39 , 37 , 34 , 30 , 27 , 23 , 14 ) , di_only = c( 60 , 54 , 47 , 36 , 35 , 36 , 30 , 23 , 24 , 13 ) , concurrent = c( 63 , 63 , 47 , 43 , 41 , 34 , 33 , 31 , 23 , 15 ) , concurrent_vs_di = c( F , T , F , F , F , F , F , T , F , F ) , ssi = c( 61 , 62 , 47 , 40 , 39 , 33 , 29 , 31 , 22 , 15 ) , ssi_vs_di = c( F , T , F , F , F , F , F , T , F , F ) ) Download, import, and recode the round 3 file: r3_tf &lt;- tempfile() r3_url &lt;- &quot;https://www.ssa.gov/disabilityresearch/documents/nbsr3pufstata.zip&quot; download.file( r3_url , r3_tf , mode = &#39;wb&#39; ) r3_tbl &lt;- read_stata( r3_tf ) r3_df &lt;- data.frame( r3_tbl ) names( r3_df ) &lt;- tolower( names( r3_df ) ) r3_design &lt;- svydesign( id = ~ r3_a_psu_pub , strata = ~ r3_a_strata , weights = ~ r3_wtr3_ben , data = subset( r3_df , r3_wtr3_ben &gt; 0 ) ) r3_design &lt;- update( r3_design , benefit_type = factor( r3_orgsampinfo_bstatus , levels = c( 2 , 3 , 1 ) , labels = c( &#39;di_only&#39; , &#39;concurrent&#39; , &#39;ssi&#39; ) ) ) Calculate the final ten rows of exhibit 4 and confirm each statistics and t-test matches: for( i in seq( nrow( ex_4 ) ) ){ this_formula &lt;- as.formula( paste( &quot;~&quot; , ex_4[ i , &#39;variable_name&#39; ] ) ) overall_percent &lt;- svymean( this_formula , r3_design ) stopifnot( 100 * round( coef( overall_percent ) , 2 ) == ex_4[ i , &#39;overall_percent&#39; ] ) benefit_percent &lt;- svyby( this_formula , ~ benefit_type , r3_design , svymean ) stopifnot( all.equal( 100 * as.numeric( round( coef( benefit_percent ) , 2 ) ) , as.numeric( ex_4[ i , c( &#39;di_only&#39; , &#39;concurrent&#39; , &#39;ssi&#39; ) ] ) ) ) ttest_formula &lt;- as.formula( paste( ex_4[ i , &#39;variable_name&#39; ] , &quot;~ benefit_type&quot; ) ) di_only_con_design &lt;- subset( r3_design , benefit_type %in% c( &#39;di_only&#39; , &#39;concurrent&#39; ) ) con_ttest &lt;- svyttest( ttest_formula , di_only_con_design ) stopifnot( all.equal( as.logical( con_ttest$p.value &lt; 0.05 ) , as.logical( ex_4[ i , &#39;concurrent_vs_di&#39; ] ) ) ) di_only_ssi_design &lt;- subset( r3_design , benefit_type %in% c( &#39;di_only&#39; , &#39;ssi&#39; ) ) ssi_ttest &lt;- svyttest( ttest_formula , di_only_ssi_design ) stopifnot( all.equal( as.logical( ssi_ttest$p.value &lt; 0.05 ) , as.logical( ex_4[ i , &#39;ssi_vs_di&#39; ] ) ) ) } Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NBS users, this code replicates previously-presented examples: library(srvyr) nbs_srvyr_design &lt;- as_survey( nbs_design ) Calculate the mean (average) of a linear variable, overall and by groups: nbs_srvyr_design %&gt;% summarize( mean = survey_mean( r7_n_totssbenlastmnth_pub , na.rm = TRUE ) ) nbs_srvyr_design %&gt;% group_by( age_categories ) %&gt;% summarize( mean = survey_mean( r7_n_totssbenlastmnth_pub , na.rm = TRUE ) ) "],["national-financial-capability-study-nfcs.html", "National Financial Capability Study (NFCS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " National Financial Capability Study (NFCS) A study of financial knowledge and behavior, like making ends meet, planning ahead, managing assets. One state-by-state survey table with one row per sampled respondent, a separate investor survey. An online non-probability sample of U.S. adults (18+) calibrated to the American Community Survey. Released triennially since 2009. Funded by the FINRA Investor Education Foundation and conducted by FGS Global. Please skim before you begin: 2021 National Financial Capability Study: State-by-State Survey Methodology Financial Capability Insights: What the NFCS Reveals This human-composed haiku or a bouquet of artificial intelligence-generated limericks # lady madonna # laid bank balance goose egg, loves # gold unrequited Download, Import, Preparation Download and import the latest state-by-state microdata: library(haven) zip_tf &lt;- tempfile() zip_url &lt;- &#39;https://finrafoundation.org/sites/finrafoundation/files/2021-SxS-Data-and-Data-Info.zip&#39; download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( zip_tf , exdir = tempdir() ) stata_fn &lt;- grep( &quot;\\\\.dta$&quot; , unzipped_files , value = TRUE ) nfcs_tbl &lt;- read_dta( stata_fn ) nfcs_df &lt;- data.frame( nfcs_tbl ) names( nfcs_df ) &lt;- tolower( names( nfcs_df ) ) Add a column of all ones, add labels to state names, add labels to the rainy day fund question: nfcs_df[ , &#39;one&#39; ] &lt;- 1 nfcs_df[ , &#39;state_name&#39; ] &lt;- factor( nfcs_df[ , &#39;stateq&#39; ] , levels = 1:51 , labels = sort( c( &#39;District of Columbia&#39; , state.name ) ) ) nfcs_df[ , &#39;rainy_day_fund&#39; ] &lt;- factor( nfcs_df[ , &#39;j5&#39; ] , levels = c( 1 , 2 , 98 , 99 ) , labels = c( &#39;Yes&#39; , &#39;No&#39; , &quot;Don&#39;t Know&quot; , &quot;Prefer not to say&quot; ) ) Save locally   Save the object at any point: # nfcs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NFCS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nfcs_df , file = nfcs_fn , compress = FALSE ) Load the same object: # nfcs_df &lt;- readRDS( nfcs_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) nfcs_design &lt;- svydesign( ~ 1 , data = nfcs_df , weights = ~ wgt_n2 ) divison_design &lt;- svydesign( ~ 1 , data = nfcs_df , weights = ~ wgt_d2 ) state_design &lt;- svydesign( ~ 1 , data = nfcs_df , weights = ~ wgt_s3 ) Variable Recoding Add new columns to the data set: nfcs_design &lt;- update( nfcs_design , satisfaction_w_finances = ifelse( j1 &gt; 10 , NA , j1 ) , risk_taking = ifelse( j2 &gt; 10 , NA , j2 ) , difficult_to_pay_bills = factor( j4 , levels = c( 1 , 2 , 3 , 98 , 99 ) , labels = c( &#39;Very difficult&#39; , &#39;Somewhat difficult&#39; , &#39;Not at all difficult&#39; , &quot;Don&#39;t know&quot; , &#39;Prefer not to say&#39; ) ) , spending_vs_income = factor( j3 , levels = c( 1 , 2 , 3 , 98 , 99 ) , labels = c( &#39;Spending less than income&#39; , &#39;Spending more than income&#39; , &#39;Spending about equal to income&#39; , &quot;Don&#39;t know&quot; , &#39;Prefer not to say&#39; ) ) , unpaid_medical_bills = ifelse( g20 &gt; 2 , NA , as.numeric( g20 == 1 ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nfcs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ spending_vs_income , nfcs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nfcs_design ) svyby( ~ one , ~ spending_vs_income , nfcs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ satisfaction_w_finances , nfcs_design , na.rm = TRUE ) svyby( ~ satisfaction_w_finances , ~ spending_vs_income , nfcs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ difficult_to_pay_bills , nfcs_design ) svyby( ~ difficult_to_pay_bills , ~ spending_vs_income , nfcs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ satisfaction_w_finances , nfcs_design , na.rm = TRUE ) svyby( ~ satisfaction_w_finances , ~ spending_vs_income , nfcs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ difficult_to_pay_bills , nfcs_design ) svyby( ~ difficult_to_pay_bills , ~ spending_vs_income , nfcs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ satisfaction_w_finances , nfcs_design , 0.5 , na.rm = TRUE ) svyby( ~ satisfaction_w_finances , ~ spending_vs_income , nfcs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ satisfaction_w_finances , denominator = ~ risk_taking , nfcs_design , na.rm = TRUE ) Subsetting Restrict the survey design to persons receiving pandemic-related stimulus payment: sub_nfcs_design &lt;- subset( nfcs_design , j50 == 1 ) Calculate the mean (average) of this subset: svymean( ~ satisfaction_w_finances , sub_nfcs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ satisfaction_w_finances , nfcs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ satisfaction_w_finances , ~ spending_vs_income , nfcs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nfcs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ satisfaction_w_finances , nfcs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ satisfaction_w_finances , nfcs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ satisfaction_w_finances , nfcs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ unpaid_medical_bills , nfcs_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( satisfaction_w_finances ~ unpaid_medical_bills , nfcs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ unpaid_medical_bills + difficult_to_pay_bills , nfcs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( satisfaction_w_finances ~ unpaid_medical_bills + difficult_to_pay_bills , nfcs_design ) summary( glm_result ) Intermish the author as clarice starling Replication Example This example matches the unweighted count shown on PDF page 4: stopifnot( nrow( nfcs_df ) == 27118 ) This example matches the PDF page 7 estimate that 53% have three months of rainy day funds: national_rainy_day &lt;- svymean( ~ rainy_day_fund , nfcs_design ) stopifnot( round( coef( national_rainy_day )[ &#39;rainy_day_fundYes&#39; ] , 2 ) == 0.53 ) This example matches counts and rainy day estimates from The Geography of Financial Capability: state_counts &lt;- svyby( ~ one , ~ state_name , state_design , unwtd.count ) stopifnot( state_counts[ &#39;California&#39; , &#39;counts&#39; ] == 1252 ) stopifnot( state_counts[ &#39;Missouri&#39; , &#39;counts&#39; ] == 501 ) stopifnot( state_counts[ &#39;Oregon&#39; , &#39;counts&#39; ] == 1261 ) state_rainy_day &lt;- svyby( ~ rainy_day_fund , ~ state_name , state_design , svymean ) stopifnot( round( state_rainy_day[ &#39;California&#39; , &#39;rainy_day_fundYes&#39; ] , 2 ) == 0.57 ) stopifnot( round( state_rainy_day[ &#39;Missouri&#39; , &#39;rainy_day_fundYes&#39; ] , 2 ) == 0.51 ) stopifnot( round( state_rainy_day[ &#39;Oregon&#39; , &#39;rainy_day_fundYes&#39; ] , 2 ) == 0.52 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NFCS users, this code replicates previously-presented examples: library(srvyr) nfcs_srvyr_design &lt;- as_survey( nfcs_design ) Calculate the mean (average) of a linear variable, overall and by groups: nfcs_srvyr_design %&gt;% summarize( mean = survey_mean( satisfaction_w_finances , na.rm = TRUE ) ) nfcs_srvyr_design %&gt;% group_by( spending_vs_income ) %&gt;% summarize( mean = survey_mean( satisfaction_w_finances , na.rm = TRUE ) ) "],["national-health-and-nutrition-examination-survey-nhanes.html", "National Health and Nutrition Examination Survey (NHANES) Download, Import, Preparation Analysis Examples with the survey library   Intermish Direct Method of Age-Adjustment Replication Example Analysis Examples with srvyr  ", " National Health and Nutrition Examination Survey (NHANES) Doctors and dentists accompany survey interviewers in a mobile medical center that travels the country. While survey researchers read the questionnaires, medical professionals administer laboratory tests and conduct a full medical examination. The blood work and in-person check-up allow epidemiologists to answer questions like, “how many people have diabetes but don’t know they have diabetes?” Many tables containing information from the various examinations, generally one row per respondent. A complex sample survey designed to generalize to the civilian non-institutionalized U.S. population. Released biennially since 1999-2000. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: About the National Health and Nutrition Examination Survey NHANES Tutorials This human-composed haiku or a bouquet of artificial intelligence-generated limericks # doctor, dentist, labs # mobile examination #vanlife interviews Download, Import, Preparation Download and import the demographics (demo) and total cholesterol laboratory (tchol) data: library(haven) nhanes_2015_2016_demo_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT&quot; nhanes_2017_2018_demo_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT&quot; nhanes_2015_2016_tchol_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/TCHOL_I.XPT&quot; nhanes_2017_2018_tchol_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/TCHOL_J.XPT&quot; nhanes_2015_2016_demo_tbl &lt;- read_xpt( nhanes_2015_2016_demo_url ) nhanes_2017_2018_demo_tbl &lt;- read_xpt( nhanes_2017_2018_demo_url ) nhanes_2015_2016_tchol_tbl &lt;- read_xpt( nhanes_2015_2016_tchol_url ) nhanes_2017_2018_tchol_tbl &lt;- read_xpt( nhanes_2017_2018_tchol_url ) nhanes_2015_2016_demo_df &lt;- data.frame( nhanes_2015_2016_demo_tbl ) nhanes_2017_2018_demo_df &lt;- data.frame( nhanes_2017_2018_demo_tbl ) nhanes_2015_2016_tchol_df &lt;- data.frame( nhanes_2015_2016_tchol_tbl ) nhanes_2017_2018_tchol_df &lt;- data.frame( nhanes_2017_2018_tchol_tbl ) Specify which variables to keep from both the demo and tchol data files, then stack the four years: demo_vars &lt;- c( # unique person identifier (merge variable) &quot;SEQN&quot; , # the two-year interviewed + MEC examined weight &quot;WTMEC2YR&quot; , # note that this is a special weight for only # individuals who took the mobile examination center (MEC) exam # there is one other weight available - WTINT2YR - # that should be used when MEC variables are not part of the analysis # interviewed only or interviewed + MEC &quot;RIDSTATR&quot; , # primary sampling unit varaible, used in complex design &quot;SDMVPSU&quot; , # strata variable, used in complex design &quot;SDMVSTRA&quot; , # race / ethnicity &quot;RIDRETH3&quot; , # age &quot;RIDAGEYR&quot; , # gender &quot;RIAGENDR&quot; , # pregnant at interview &quot;RIDEXPRG&quot; ) nhanes_2015_2018_demo_df &lt;- rbind( nhanes_2015_2016_demo_df[ , demo_vars ] , nhanes_2017_2018_demo_df[ , demo_vars ] ) tchol_vars &lt;- c( # unique person identifier (merge variable) &quot;SEQN&quot; , # laboratory total cholesterol variable # https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/TCHOL_J.htm &quot;LBXTC&quot; ) nhanes_2015_2018_tchol_df &lt;- rbind( nhanes_2015_2016_tchol_df[ , tchol_vars ] , nhanes_2017_2018_tchol_df[ , tchol_vars ] ) Merge the two pooled datasets, limit the data.frame to mobile examination component respondents: nhanes_full_df &lt;- merge( nhanes_2015_2018_demo_df , nhanes_2015_2018_tchol_df , all = TRUE ) names( nhanes_full_df ) &lt;- tolower( names( nhanes_full_df ) ) nhanes_df &lt;- subset( nhanes_full_df , ridstatr %in% 2 ) Scale the mobile examination component two-year weight to generalize to the pooled, four year period: nhanes_df[ , &#39;wtmec4yr&#39; ] &lt;- nhanes_df[ , &#39;wtmec2yr&#39; ] / 2 Save locally   Save the object at any point: # nhanes_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NHANES&quot; , &quot;this_file.rds&quot; ) # saveRDS( nhanes_df , file = nhanes_fn , compress = FALSE ) Load the same object: # nhanes_df &lt;- readRDS( nhanes_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) nhanes_design &lt;- svydesign( id = ~ sdmvpsu , strata = ~ sdmvstra , nest = TRUE , weights = ~ wtmec4yr , data = nhanes_df ) Variable Recoding Add new columns to the data set: nhanes_design &lt;- update( nhanes_design , one = 1 , # define high total cholesterol as 1 if mg/dL is at or above 240 and zero otherwise. hi_tchol = ifelse( lbxtc &gt;= 240 , 1 , 0 ) , gender = factor( riagendr , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , age_categories = factor( 1 + findInterval( ridageyr , c( 20 , 40 , 60 ) ) , levels = 1:4 , labels = c( &quot;0-19&quot; , &quot;20-39&quot; , &quot;40-59&quot; , &quot;60+&quot; ) ) , # recode the ridreth3 variable as: # mexican american and other hispanic -&gt; 4 # non-hispanic white -&gt; 1 # non-hispanic black -&gt; 2 # non-hispanic asian -&gt; 3 # other race including multi-racial -&gt; 5 race_ethnicity = factor( c( 4 , 4 , 1 , 2 , NA , 3 , 5 )[ ridreth3 ] , levels = 1:5 , labels = c( &#39;nh white&#39; , &#39;nh black&#39; , &#39;nh asian&#39; , &#39;hispanic&#39; , &#39;other&#39; ) ) , pregnant_at_interview = ifelse( ridexprg %in% 1:2 , as.numeric( ridexprg == 1 ) , NA ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nhanes_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ race_ethnicity , nhanes_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nhanes_design ) svyby( ~ one , ~ race_ethnicity , nhanes_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ lbxtc , nhanes_design , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ riagendr , nhanes_design ) svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ lbxtc , nhanes_design , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ riagendr , nhanes_design ) svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ lbxtc , nhanes_design , 0.5 , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ lbxtc , denominator = ~ ridageyr , nhanes_design , na.rm = TRUE ) Subsetting Restrict the survey design to respondents aged 60 or older: sub_nhanes_design &lt;- subset( nhanes_design , age_categories == &quot;60+&quot; ) Calculate the mean (average) of this subset: svymean( ~ lbxtc , sub_nhanes_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ lbxtc , nhanes_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nhanes_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ lbxtc , nhanes_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ pregnant_at_interview , nhanes_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( lbxtc ~ pregnant_at_interview , nhanes_design ) Perform a chi-squared test of association for survey data: svychisq( ~ pregnant_at_interview + riagendr , nhanes_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( lbxtc ~ pregnant_at_interview + riagendr , nhanes_design ) summary( glm_result ) Intermish the author as rafiki Direct Method of Age-Adjustment Replication Example This example matches the total cholesterol statistics and standard errors in Table 1 from Data Brief 363: Match the crude estimates in the footnote and also in the unadjusted age categories: crude_overall &lt;- svymean( ~ hi_tchol , subset( nhanes_design , ridageyr &gt;= 20 ) , na.rm = TRUE ) stopifnot( round( coef( crude_overall ) , 3 ) == 0.115 ) crude_by_gender &lt;- svyby( ~ hi_tchol , ~ gender , subset( nhanes_design , ridageyr &gt;= 20 ) , svymean , na.rm = TRUE ) stopifnot( round( coef( crude_by_gender )[ 1 ] , 3 ) == 0.103 ) stopifnot( round( coef( crude_by_gender )[ 2 ] , 3 ) == 0.126 ) crude_by_age &lt;- svyby( ~ hi_tchol , ~ age_categories , subset( nhanes_design , ridageyr &gt;= 20 ) , svymean , na.rm = TRUE ) stopifnot( round( coef( crude_by_age )[ 1 ] , 3 ) == 0.075 ) stopifnot( round( coef( crude_by_age )[ 2 ] , 3 ) == 0.157 ) stopifnot( round( coef( crude_by_age )[ 3 ] , 3 ) == 0.114 ) stopifnot( round( SE( crude_by_age )[ 1 ] , 3 ) == 0.005 ) stopifnot( round( SE( crude_by_age )[ 2 ] , 3 ) == 0.011 ) stopifnot( round( SE( crude_by_age )[ 3 ] , 3 ) == 0.008 ) Sum up 2000 Census totals based on the age groupings specified in footnote: pop_by_age &lt;- data.frame( age_categories = c( &quot;0-19&quot; , &quot;20-39&quot; , &quot;40-59&quot; , &quot;60+&quot; ) , Freq = c( 78782657 , 77670618 , 72816615 , 45363752 ) ) Create a design with the nationwide population stratified to the above census counts: nhanes_age_adjusted &lt;- postStratify( subset( nhanes_design , !is.na( hi_tchol ) ) , ~ age_categories , pop_by_age ) Match the overall adjusted estimates: results_overall &lt;- svymean( ~ hi_tchol , subset( nhanes_age_adjusted , ridageyr &gt;= 20 ) , na.rm = TRUE ) stopifnot( round( coef( results_overall ) , 3 ) == 0.114 ) stopifnot( round( SE( results_overall ) , 3 ) == 0.006 ) Create a design stratified to census counts broken out by gender, then match those estimates: nhanes_by_gender &lt;- svystandardize( nhanes_design , by = ~ age_categories , # stratification variable over = ~ gender , # break out variable population = pop_by_age , # data.frame containing census populations excluding.missing = ~ hi_tchol # analysis variable of interest ) results_by_gender &lt;- svyby( ~ hi_tchol , ~ gender , subset( nhanes_by_gender , ridageyr &gt;= 20 ) , svymean , na.rm=TRUE ) stopifnot( round( coef( results_by_gender )[ 1 ] , 3 ) == 0.105 ) stopifnot( round( coef( results_by_gender )[ 2 ] , 3 ) == 0.121 ) stopifnot( round( SE( results_by_gender )[ 1 ] , 3 ) == 0.007 ) stopifnot( round( SE( results_by_gender )[ 2 ] , 3 ) == 0.008 ) Create a design stratified to census counts broken out by race/ethnicity, then match those estimates: nhanes_by_race &lt;- svystandardize( nhanes_design , by = ~ age_categories , # stratification variable over = ~ race_ethnicity , # break out variable population = pop_by_age , # data.frame containing census populations excluding.missing = ~ hi_tchol # analysis variable of interest ) results_by_race_ethnicity &lt;- svyby( ~ hi_tchol , ~ race_ethnicity , design = subset( nhanes_by_race , ridageyr &gt;= 20 ) , svymean , na.rm=TRUE ) stopifnot( round( coef( results_by_race_ethnicity )[ 1 ] , 3 ) == 0.117 ) stopifnot( round( coef( results_by_race_ethnicity )[ 2 ] , 3 ) == 0.100 ) stopifnot( round( coef( results_by_race_ethnicity )[ 3 ] , 3 ) == 0.116 ) stopifnot( round( coef( results_by_race_ethnicity )[ 4 ] , 3 ) == 0.109 ) stopifnot( round( SE( results_by_race_ethnicity )[ 1 ] , 3 ) == 0.007 ) stopifnot( round( SE( results_by_race_ethnicity )[ 2 ] , 3 ) == 0.009 ) stopifnot( round( SE( results_by_race_ethnicity )[ 3 ] , 3 ) == 0.011 ) stopifnot( round( SE( results_by_race_ethnicity )[ 4 ] , 3 ) == 0.009 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NHANES users, this code replicates previously-presented examples: library(srvyr) nhanes_srvyr_design &lt;- as_survey( nhanes_design ) Calculate the mean (average) of a linear variable, overall and by groups: nhanes_srvyr_design %&gt;% summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) ) nhanes_srvyr_design %&gt;% group_by( race_ethnicity ) %&gt;% summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) ) "],["national-health-interview-survey-nhis.html", "National Health Interview Survey (NHIS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " National Health Interview Survey (NHIS) America’s most detailed household survey of health status and medical experience. One table with one row per sampled adult (18+) within each sampled household, one table with one row per sample child (when available, same family not required), multiply-imputed income tables. A complex sample survey designed to generalize to the U.S. civilian non-institutionalized population. Released annually since 1963, the most recent major re-design in 2019. Conducted by the National Center for Health Statistics at the Centers for Disease Control. Please skim before you begin: 2021 Survey Description Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # excellent health poor # wealth. &quot;sup, doc?&quot; bugs, daft bills, free # laughs best medicine Download, Import, Preparation Define a function to download, unzip, and import each comma-separated value file: nhis_csv_import &lt;- function( this_url ){ this_tf &lt;- tempfile() download.file( this_url , this_tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( this_tf , exdir = tempdir() ) this_csv &lt;- grep( &#39;\\\\.csv$&#39; , unzipped_files , value = TRUE ) this_df &lt;- read.csv( this_csv ) file.remove( c( this_tf , unzipped_files ) ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } Download and import the sample adult interview and imputed income files: nhis_df &lt;- nhis_csv_import( &quot;https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHIS/2021/adult21csv.zip&quot; ) imputed_income_df &lt;- nhis_csv_import( &quot;https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHIS/2021/adultinc21csv.zip&quot; ) Save locally   Save the object at any point: # nhis_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NHIS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nhis_df , file = nhis_fn , compress = FALSE ) Load the same object: # nhis_df &lt;- readRDS( nhis_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: Reshape the imputed income data.frame into a list based on the implicate number: imputed_income_list &lt;- split( imputed_income_df , imputed_income_df[ , &#39;impnum_a&#39; ] ) Remove overlapping columns except the merge variable: variables_to_remove &lt;- setdiff( intersect( names( nhis_df ) , names( imputed_income_df ) ) , &#39;hhx&#39; ) nhis_df &lt;- nhis_df[ , !( names( nhis_df ) %in% variables_to_remove ) ] Merge each implicate onto the sample adult table: nhis_list &lt;- lapply( imputed_income_list , function( w ){ this_df &lt;- merge( nhis_df , w ) stopifnot( nrow( this_df ) == nrow( nhis_df ) ) this_df } ) Define the design: library(survey) library(mitools) nhis_design &lt;- svydesign( id = ~ ppsu , strata = ~ pstrat , nest = TRUE , weights = ~ wtfa_a , data = imputationList( nhis_list ) ) Variable Recoding Add new columns to the data set: nhis_design &lt;- update( nhis_design , one = 1 , poverty_category = factor( findInterval( povrattc_a , c( 1 , 2 , 4 ) ) , labels = c( &quot;below poverty&quot; , &quot;100-199%&quot; , &quot;200-399%&quot; , &quot;400%+&quot; ) ) , fair_or_poor_reported_health = ifelse( phstat_a %in% 1:5 , as.numeric( phstat_a &gt;= 4 ) , NA ) , sex_a = factor( sex_a , levels = 1:2 , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , annual_premium_first_plan = ifelse( hicostr1_a &gt; 40000 , NA , hicostr1_a ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( nhis_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( nhis_design , svyby( ~ one , ~ poverty_category , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( nhis_design , svytotal( ~ one ) ) ) MIcombine( with( nhis_design , svyby( ~ one , ~ poverty_category , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( nhis_design , svymean( ~ agep_a ) ) ) MIcombine( with( nhis_design , svyby( ~ agep_a , ~ poverty_category , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( nhis_design , svymean( ~ sex_a ) ) ) MIcombine( with( nhis_design , svyby( ~ sex_a , ~ poverty_category , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( nhis_design , svytotal( ~ agep_a ) ) ) MIcombine( with( nhis_design , svyby( ~ agep_a , ~ poverty_category , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( nhis_design , svytotal( ~ sex_a ) ) ) MIcombine( with( nhis_design , svyby( ~ sex_a , ~ poverty_category , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( nhis_design , svyquantile( ~ agep_a , 0.5 , se = TRUE ) ) ) MIcombine( with( nhis_design , svyby( ~ agep_a , ~ poverty_category , svyquantile , 0.5 , se = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( nhis_design , svyratio( numerator = ~ annual_premium_first_plan , denominator = ~ agep_a , na.rm = TRUE ) ) ) Subsetting Restrict the survey design to uninsured: sub_nhis_design &lt;- subset( nhis_design , notcov_a == 1 ) Calculate the mean (average) of this subset: MIcombine( with( sub_nhis_design , svymean( ~ agep_a ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( nhis_design , svymean( ~ agep_a ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( nhis_design , svyby( ~ agep_a , ~ poverty_category , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nhis_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( nhis_design , svyvar( ~ agep_a ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( nhis_design , svymean( ~ agep_a , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( nhis_design , svymean( ~ agep_a , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ fair_or_poor_reported_health , nhis_design , # method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( agep_a ~ fair_or_poor_reported_health , nhis_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ fair_or_poor_reported_health + sex_a , nhis_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( nhis_design , svyglm( agep_a ~ fair_or_poor_reported_health + sex_a ) ) ) summary( glm_result ) Intermish caravaggio’s judith beheading holofernes Replication Example This example matches statistics and standard errors within 0.01% from Figure 3 of this Characteristics of Adults Aged 18–64 Who Did Not Take Medication as Prescribed to Reduce Costs Data Brief: results &lt;- MIcombine( with( subset( nhis_design , agep_a &lt; 65 ) , svyby( ~ as.numeric( rxsk12m_a == 1 | rxls12m_a == 1 | rxdl12m_a == 1 ) , ~ poverty_category , svymean , na.rm = TRUE ) ) ) stopifnot( all( as.numeric( round( coef( results ) , 3 ) ) == c( 0.145 , 0.138 , 0.099 , 0.039 ) ) ) stopifnot( all( as.numeric( round( SE( results ) , 5 ) ) - c( 0.0126 , 0.0098 , 0.0062 , 0.0031 ) &lt; 0.0001 ) ) "],["national-household-travel-survey-nhts.html", "National Household Travel Survey (NHTS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " National Household Travel Survey (NHTS) The authoritative source on travel behavior, recording characteristics of people and vehicles of all modes. Four core linkable tables, with one record per household, person, trip, and vehicle, respectively. A complex sample survey designed to generalize to the civilian non-institutional U.S. population. Released every five to eight years since 1969, with a 2022 release expected in late 2023. Funded by the Federal Highway Administration, with data collected by Westat. Please skim before you begin: 2017 NHTS Data User Guide 2017 NHTS Weighting Report This human-composed haiku or a bouquet of artificial intelligence-generated limericks # commuter patterns, # truckin&#39;. what a long strange trip # who went when where why Download, Import, Preparation Download and unzip each of the main 2017 files: library(haven) nhts_dl_uz &lt;- function( this_url ){ tf &lt;- tempfile() download.file( this_url , tf , mode = &#39;wb&#39; ) unzip( tf , exdir = tempdir() ) } unzipped_survey_data &lt;- nhts_dl_uz( &quot;https://nhts.ornl.gov/assets/2016/download/sas.zip&quot; ) unzipped_replicate_weights &lt;- nhts_dl_uz( &quot;https://nhts.ornl.gov/assets/2016/download/Replicates.zip&quot; ) unzipped_trip_chains &lt;- nhts_dl_uz( &quot;https://nhts.ornl.gov/assets/2016/download/TripChain/TripChain17.zip&quot; ) Import the tables containing one record per household, person, trip, and vehicle: nhts_import &lt;- function( this_prefix , this_unzip ){ this_sas7bdat &lt;- grep( paste0( this_prefix , &quot;\\\\.sas7bdat$&quot; ) , this_unzip , value = TRUE ) this_tbl &lt;- read_sas( this_sas7bdat ) this_df &lt;- data.frame( this_tbl ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } hhpub_df &lt;- nhts_import( &quot;hhpub&quot; , unzipped_survey_data ) perpub_df &lt;- nhts_import( &quot;perpub&quot; , unzipped_survey_data ) trippub_df &lt;- nhts_import( &quot;trippub&quot; , unzipped_survey_data ) vehpub_df &lt;- nhts_import( &quot;vehpub&quot; , unzipped_survey_data ) hhwgt_df &lt;- nhts_import( &quot;hhwgt&quot; , unzipped_replicate_weights ) perwgt_df &lt;- nhts_import( &quot;perwgt&quot; , unzipped_replicate_weights ) Add a column of ones to three of those tables, then a column of non-missing mileage to the trips table: hhpub_df[ , &#39;one&#39; ] &lt;- 1 perpub_df[ , &#39;one&#39; ] &lt;- 1 trippub_df[ , &#39;one&#39; ] &lt;- 1 trippub_df[ !( trippub_df[ , &#39;trpmiles&#39; ] %in% -9 ) , &#39;tripmiles_no_nines&#39; ] &lt;- trippub_df[ !( trippub_df[ , &#39;trpmiles&#39; ] %in% -9 ) , &#39;trpmiles&#39; ] Sum the total trip count and mileage to the person-level, both overall and restricted to walking only: trips_per_person &lt;- with( trippub_df , aggregate( cbind( one , tripmiles_no_nines ) , list( houseid , personid ) , sum , na.rm = TRUE ) ) names( trips_per_person ) &lt;- c( &#39;houseid&#39; , &#39;personid&#39; , &#39;trips_per_person&#39; , &#39;miles_per_person&#39; ) walks_per_person &lt;- with( subset( trippub_df , trptrans == &#39;01&#39; ) , aggregate( cbind( one , tripmiles_no_nines ) , list( houseid , personid ) , sum , na.rm = TRUE ) ) names( walks_per_person ) &lt;- c( &#39;houseid&#39; , &#39;personid&#39; , &#39;walks_per_person&#39; , &#39;walk_miles_per_person&#39; ) Merge these trip count and mileage values on to the person-level file, replacing non-matches with zero: nhts_df &lt;- merge( perpub_df , trips_per_person , all.x = TRUE ) nhts_df[ is.na( nhts_df[ , &#39;trips_per_person&#39; ] ) , &#39;trips_per_person&#39; ] &lt;- 0 nhts_df[ is.na( nhts_df[ , &#39;miles_per_person&#39; ] ) , &#39;miles_per_person&#39; ] &lt;- 0 nhts_df &lt;- merge( nhts_df , walks_per_person , all.x = TRUE ) nhts_df[ is.na( nhts_df[ , &#39;walks_per_person&#39; ] ) , &#39;walks_per_person&#39; ] &lt;- 0 nhts_df[ is.na( nhts_df[ , &#39;walk_miles_per_person&#39; ] ) , &#39;walk_miles_per_person&#39; ] &lt;- 0 stopifnot( nrow( nhts_df ) == nrow( perpub_df ) ) Save locally   Save the object at any point: # nhts_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NHTS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nhts_df , file = nhts_fn , compress = FALSE ) Load the same object: # nhts_df &lt;- readRDS( nhts_fn ) Survey Design Definition Construct a complex sample survey design: Sort both the one record per household and household replicate weights tables, then define the design: library(survey) hhpub_df &lt;- hhpub_df[ order( hhpub_df[ , &#39;houseid&#39; ] ) , ] hhwgt_df &lt;- hhwgt_df[ order( hhwgt_df[ , &#39;houseid&#39; ] ) , ] hh_design &lt;- svrepdesign( weight = ~ wthhfin , repweights = hhwgt_df[ grep( &#39;wthhfin[0-9]&#39; , names( hhwgt_df ) , value = TRUE ) ] , scale = 6 / 7 , rscales = 1 , type = &#39;JK1&#39; , mse = TRUE , data = hhpub_df ) Sort both the one record per person and person replicate weights tables, then define the design: nhts_df &lt;- nhts_df[ do.call( order , nhts_df[ , c( &#39;houseid&#39; , &#39;personid&#39; ) ] ) , ] perwgt_df &lt;- perwgt_df[ do.call( order , perwgt_df[ , c( &#39;houseid&#39; , &#39;personid&#39; ) ] ) , ] nhts_design &lt;- svrepdesign( weight = ~ wtperfin , repweights = perwgt_df[ grep( &#39;wtperfin[0-9]&#39; , names( perwgt_df ) , value = TRUE ) ] , scale = 6 / 7 , rscales = rep( 1 , 98 ) , type = &#39;JK1&#39; , mse = TRUE , data = nhts_df ) Sort both the one record per trip and person replicate weights tables, then define the design: trippub_df &lt;- trippub_df[ do.call( order , trippub_df[ , c( &#39;houseid&#39; , &#39;personid&#39; ) ] ) , ] perwgt_df &lt;- perwgt_df[ do.call( order , perwgt_df[ , c( &#39;houseid&#39; , &#39;personid&#39; ) ] ) , ] trip_design &lt;- svrepdesign( weight = ~ wttrdfin , repweights = perwgt_df[ grep( &#39;wttrdfin[0-9]&#39; , names( perwgt_df ) , value = TRUE ) ] , scale = 6 / 7 , rscales = 1 , type = &#39;JK1&#39; , mse = TRUE , data = trippub_df ) Variable Recoding Add new columns to the data set: hh_design &lt;- update( hh_design , hhsize_categories = factor( findInterval( hhsize , 1:4 ) , levels = 1:4 , labels = c( 1:3 , &#39;4 or more&#39; ) ) ) nhts_design &lt;- update( nhts_design , urban_area = as.numeric( urbrur == &#39;01&#39; ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nhts_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ r_sex_imp , nhts_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nhts_design ) svyby( ~ one , ~ r_sex_imp , nhts_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ miles_per_person , nhts_design ) svyby( ~ miles_per_person , ~ r_sex_imp , nhts_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ hhstate , nhts_design ) svyby( ~ hhstate , ~ r_sex_imp , nhts_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ miles_per_person , nhts_design ) svyby( ~ miles_per_person , ~ r_sex_imp , nhts_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ hhstate , nhts_design ) svyby( ~ hhstate , ~ r_sex_imp , nhts_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ miles_per_person , nhts_design , 0.5 ) svyby( ~ miles_per_person , ~ r_sex_imp , nhts_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ walk_miles_per_person , denominator = ~ miles_per_person , nhts_design ) Subsetting Restrict the survey design to ever cyclists: sub_nhts_design &lt;- subset( nhts_design , nbiketrp &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ miles_per_person , sub_nhts_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ miles_per_person , nhts_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ miles_per_person , ~ r_sex_imp , nhts_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nhts_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ miles_per_person , nhts_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ miles_per_person , nhts_design , deff = TRUE ) # SRS with replacement svymean( ~ miles_per_person , nhts_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ urban_area , nhts_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( miles_per_person ~ urban_area , nhts_design ) Perform a chi-squared test of association for survey data: svychisq( ~ urban_area + hhstate , nhts_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( miles_per_person ~ urban_area + hhstate , nhts_design ) summary( glm_result ) Intermish the author as dwight d. eisenhower national system of interstate and defense highways Replication Example This example matches the 2017 rows from Summary of Travel Trends Table 1a: hhsize_counts &lt;- svytotal( ~ hhsize_categories , hh_design ) stopifnot( all( round( coef( hhsize_counts ) / 1000 , 0 ) == c( 32952 , 40056 , 18521 , 26679 ) ) ) hhsize_ci &lt;- confint( hhsize_counts , df = ncol( hh_design$repweights ) ) hhsize_moe &lt;- hhsize_ci[ , 2 ] - coef( hhsize_counts ) stopifnot( all( round( hhsize_moe / 1000 , 0 ) == c( 0 , 0 , 97 , 97 ) ) ) This example matches 2017 NHTS Westat project co-author’s workshop slide 38: unwtd_n &lt;- with( nhts_df , tapply( trips_per_person , worker , sum ) ) stopifnot( all( unwtd_n == c( 79295 , 28 , 497944 , 346305 ) ) ) surveyed_n &lt;- with( nhts_df , tapply( trips_per_person , worker , mean ) ) stopifnot( all( round( surveyed_n , 2 ) == c( 2.84 , 1.65 , 3.88 , 3.21 ) ) ) this_mean &lt;- svyby( ~ trips_per_person , ~ worker , nhts_design , svymean ) stopifnot( round( coef( this_mean ) , 2 ) == c( 2.78 , 1.28 , 3.77 , 3.01 ) ) this_ci &lt;- confint( this_mean , df = ncol( nhts_design$repweights ) ) this_moe &lt;- this_ci[ , 2 ] - coef( this_mean ) stopifnot( all( round( this_moe , 2 ) == c( 0.06 , 2.21 , 0.03 , 0.06 ) ) ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NHTS users, this code replicates previously-presented examples: library(srvyr) nhts_srvyr_design &lt;- as_survey( nhts_design ) Calculate the mean (average) of a linear variable, overall and by groups: nhts_srvyr_design %&gt;% summarize( mean = survey_mean( miles_per_person ) ) nhts_srvyr_design %&gt;% group_by( r_sex_imp ) %&gt;% summarize( mean = survey_mean( miles_per_person ) ) "],["national-immunization-survey-nis.html", "National Immunization Survey (NIS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " National Immunization Survey (NIS) The vaccination coverage rate tracker for national, state, and selected local areas. One table with one row per sampled toddler. A complex sample survey designed to generalize to children aged 19-35 months in the United States. Released annually since 1995, plus an adolescent (13-17 years) sample since 2008. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: About NIS National Immunization Survey-Child: A User’s Guide for the 2021 Public-Use Data File This human-composed haiku or a bouquet of artificial intelligence-generated limericks # i hear babies cry # protesting lungs of iron # a wonderful world Download, Import, Preparation Download the fixed-width file: dat_tf &lt;- tempfile() dat_url &lt;- &quot;https://ftp.cdc.gov/pub/Vaccines_NIS/NISPUF21.DAT&quot; download.file( dat_url , dat_tf , mode = &#39;wb&#39; ) Edit then execute the import script provided by the CDC: library(Hmisc) r_tf &lt;- tempfile() r_script_url &lt;- &quot;https://ftp.cdc.gov/pub/Vaccines_NIS/NISPUF21.R&quot; r_input_lines &lt;- readLines( r_script_url ) # do not let the script do the save() r_input_lines &lt;- gsub( &quot;^save\\\\(&quot; , &quot;# save(&quot; , r_input_lines ) # redirect the path to the flat file to the local save location of `dat_tf` r_input_lines &lt;- gsub( &#39;\\\\&quot;path\\\\-to\\\\-file\\\\/(.*)\\\\.DAT\\\\&quot;&#39; , &quot;dat_tf&quot; , r_input_lines ) # save the edited script locally writeLines( r_input_lines , r_tf ) # run the edited script source( r_tf , echo = TRUE ) # rename the resultant data.frame object nis_df &lt;- NISPUF21 names( nis_df ) &lt;- tolower( names( nis_df ) ) nis_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # nis_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NIS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nis_df , file = nis_fn , compress = FALSE ) Load the same object: # nis_df &lt;- readRDS( nis_fn ) Survey Design Definition Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nis_design &lt;- svydesign( id = ~ seqnumhh , strata = ~ stratum , weights = ~ provwt_c , data = subset( nis_df , provwt_c &gt; 0 ) ) Variable Recoding Add new columns to the data set: nis_design &lt;- update( nis_design , first_fed_formula = ifelse( bf_formr20 %in% 888 , NA , bf_formr20 ) , dtap_3p = as.numeric( ( p_numdah &gt;= 3 ) | ( p_numdhi &gt;= 3 ) | ( p_numdih &gt;= 3 ) | ( p_numdta &gt;= 3 ) | ( p_numdtp &gt;= 3 ) ) , dtap_4p = as.numeric( ( p_numdah &gt;= 4 ) | ( p_numdhi &gt;= 4 ) | ( p_numdih &gt;= 4 ) | ( p_numdta &gt;= 4 ) | ( p_numdtp &gt;= 4 ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nis_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state , nis_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nis_design ) svyby( ~ one , ~ state , nis_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ first_fed_formula , nis_design , na.rm = TRUE ) svyby( ~ first_fed_formula , ~ state , nis_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , nis_design , na.rm = TRUE ) svyby( ~ sex , ~ state , nis_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ first_fed_formula , nis_design , na.rm = TRUE ) svyby( ~ first_fed_formula , ~ state , nis_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , nis_design , na.rm = TRUE ) svyby( ~ sex , ~ state , nis_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ first_fed_formula , nis_design , 0.5 , na.rm = TRUE ) svyby( ~ first_fed_formula , ~ state , nis_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ bf_exclr06 , denominator = ~ bf_endr06 , nis_design , na.rm = TRUE ) Subsetting Restrict the survey design to toddlers up to date on polio shots: sub_nis_design &lt;- subset( nis_design , p_utdpol == 1 ) Calculate the mean (average) of this subset: svymean( ~ first_fed_formula , sub_nis_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ first_fed_formula , nis_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ first_fed_formula , ~ state , nis_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nis_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ first_fed_formula , nis_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ first_fed_formula , nis_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ first_fed_formula , nis_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ dtap_3p , nis_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( first_fed_formula ~ dtap_3p , nis_design ) Perform a chi-squared test of association for survey data: svychisq( ~ dtap_3p + sex , nis_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( first_fed_formula ~ dtap_3p + sex , nis_design ) summary( glm_result ) Intermish balto as rudolph Replication Example This example matches the statistics and standard errors from Data User’s Guide Table 4: results &lt;- svyby( ~ p_utd431h314_rout_s , ~ raceethk , nis_design , svymean ) coefficients &lt;- results[ , &quot;p_utd431h314_rout_sUTD&quot; , drop = FALSE ] standard_errors &lt;- results[ , &quot;se.p_utd431h314_rout_sUTD&quot; , drop = FALSE ] stopifnot( round( coefficients[ &quot;HISPANIC&quot; , ] , 3 ) == .711 ) stopifnot( round( coefficients[ &quot;NON-HISPANIC WHITE ONLY&quot; , ] , 3 ) == .742 ) stopifnot( round( coefficients[ &quot;NON-HISPANIC BLACK ONLY&quot; , ] , 3 ) == .647 ) stopifnot( round( standard_errors[ &quot;HISPANIC&quot; , ] , 3 ) == .015 ) stopifnot( round( standard_errors[ &quot;NON-HISPANIC WHITE ONLY&quot; , ] , 3 ) == .009 ) stopifnot( round( standard_errors[ &quot;NON-HISPANIC BLACK ONLY&quot; , ] , 3 ) == .022 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NIS users, this code replicates previously-presented examples: library(srvyr) nis_srvyr_design &lt;- as_survey( nis_design ) Calculate the mean (average) of a linear variable, overall and by groups: nis_srvyr_design %&gt;% summarize( mean = survey_mean( first_fed_formula , na.rm = TRUE ) ) nis_srvyr_design %&gt;% group_by( state ) %&gt;% summarize( mean = survey_mean( first_fed_formula , na.rm = TRUE ) ) "],["national-plan-and-provider-enumeration-system-nppes.html", "National Plan and Provider Enumeration System (NPPES) Download, Import, Preparation Analysis Examples with base R   Intermish Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " National Plan and Provider Enumeration System (NPPES) The registry of every medical practitioner actively operating in the United States healthcare industry. A single large table with one row per enumerated health care provider. A census of individuals and organizations that bill for medical services in the United States. Updated weekly with new providers. Maintained by the United States Centers for Medicare &amp; Medicaid Services (CMS) Please skim before you begin: NPI: What You Need To Know Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # how many doctors # ranked sergeant, last name pepper # practice in the states? Download, Import, Preparation Download and import the national file: library(readr) tf &lt;- tempfile() npi_datapage &lt;- readLines( &quot;http://download.cms.gov/nppes/NPI_Files.html&quot; ) latest_files &lt;- grep( &#39;NPPES_Data_Dissemination_&#39; , npi_datapage , value = TRUE ) latest_files &lt;- latest_files[ !grepl( &#39;Weekly Update&#39; , latest_files ) ] this_url &lt;- paste0( &quot;http://download.cms.gov/nppes/&quot;, gsub( &quot;(.*)(NPPES_Data_Dissemination_.*\\\\.zip)(.*)$&quot;, &quot;\\\\2&quot;, latest_files ) ) download.file( this_url , tf , mode = &#39;wb&#39; ) npi_files &lt;- unzip( tf , exdir = tempdir() ) npi_filepath &lt;- grep( &quot;npidata_pfile_20050523-([0-9]+)\\\\.csv&quot; , npi_files , value = TRUE ) column_names &lt;- names( read.csv( npi_filepath , nrow = 1 )[ FALSE , , ] ) column_names &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , tolower( column_names ) ) column_types &lt;- ifelse( grepl( &quot;code&quot; , column_names ) &amp; !grepl( &quot;country|state|gender|taxonomy|postal&quot; , column_names ) , &#39;n&#39; , &#39;c&#39; ) columns_to_import &lt;- c( &quot;entity_type_code&quot; , &quot;provider_gender_code&quot; , &quot;provider_enumeration_date&quot; , &quot;is_sole_proprietor&quot; , &quot;provider_business_practice_location_address_state_name&quot; ) stopifnot( all( columns_to_import %in% column_names ) ) # readr::read_csv() columns must match their order in the csv file columns_to_import &lt;- columns_to_import[ order( match( columns_to_import , column_names ) ) ] nppes_tbl &lt;- readr::read_csv( npi_filepath , col_names = columns_to_import , col_types = paste0( ifelse( column_names %in% columns_to_import , column_types , &#39;_&#39; ) , collapse = &quot;&quot; ) , skip = 1 ) nppes_df &lt;- data.frame( nppes_tbl ) Save locally   Save the object at any point: # nppes_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NPPES&quot; , &quot;this_file.rds&quot; ) # saveRDS( nppes_df , file = nppes_fn , compress = FALSE ) Load the same object: # nppes_df &lt;- readRDS( nppes_fn ) Variable Recoding Add new columns to the data set: nppes_df &lt;- transform( nppes_df , individual = as.numeric( entity_type_code ) , provider_enumeration_year = as.numeric( substr( provider_enumeration_date , 7 , 10 ) ) , state_name = provider_business_practice_location_address_state_name ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( nppes_df ) table( nppes_df[ , &quot;provider_gender_code&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( nppes_df[ , &quot;is_sole_proprietor&quot; ] ) ) prop.table( table( nppes_df[ , c( &quot;is_sole_proprietor&quot; , &quot;provider_gender_code&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( nppes_df[ , &quot;provider_enumeration_year&quot; ] , 0.5 , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to California: sub_nppes_df &lt;- subset( nppes_df , state_name = &#39;CA&#39; ) Calculate the mean (average) of this subset: mean( sub_nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( provider_enumeration_year ~ individual , nppes_df ) Perform a chi-squared test of association: this_table &lt;- table( nppes_df[ , c( &quot;individual&quot; , &quot;is_sole_proprietor&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( provider_enumeration_year ~ individual + is_sole_proprietor , data = nppes_df ) summary( glm_result ) Intermish norman rockwell’s triple self-portrait Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for NPPES users, this code replicates previously-presented examples: library(dplyr) nppes_tbl &lt;- as_tibble( nppes_df ) Calculate the mean (average) of a linear variable, overall and by groups: nppes_tbl %&gt;% summarize( mean = mean( provider_enumeration_year , na.rm = TRUE ) ) nppes_tbl %&gt;% group_by( provider_gender_code ) %&gt;% summarize( mean = mean( provider_enumeration_year , na.rm = TRUE ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for NPPES users, this code replicates previously-presented examples: library(data.table) nppes_dt &lt;- data.table( nppes_df ) Calculate the mean (average) of a linear variable, overall and by groups: nppes_dt[ , mean( provider_enumeration_year , na.rm = TRUE ) ] nppes_dt[ , mean( provider_enumeration_year , na.rm = TRUE ) , by = provider_gender_code ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for NPPES users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;nppes&#39; , nppes_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( provider_enumeration_year ) FROM nppes&#39; ) dbGetQuery( con , &#39;SELECT provider_gender_code , AVG( provider_enumeration_year ) FROM nppes GROUP BY provider_gender_code&#39; ) "],["national-survey-of-childrens-health-nsch.html", "National Survey of Children’s Health (NSCH) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " National Survey of Children’s Health (NSCH) Estimates of children’s health care and family environments to inform reports like Healthy People 2030. One screener table with one row per eligible child (1+ rows per household), one topical table with the sampled child (only one row per household) from three stacked age-specific questionnaires. A complex sample survey designed to generalize to non-institutionalized U.S. children under 18. Released every four or five years since 2003, annually since 2016. Sponsored by the Maternal and Child Health Bureau, Health Resources and Services Administration. Please skim before you begin: 2021 National Survey of Children’s Health Methodology Report 2021 National Survey of Children’s Health Data Users Frequently Asked Questions (FAQs) This human-composed haiku or a bouquet of artificial intelligence-generated limericks # &quot;age but a number&quot; # lied babe from crib. &quot;your nose grows&quot; # cried gramps changing bib Download, Import, Preparation Define a function to download, unzip, and import each comma-separated value file: library(haven) nsch_stata_import &lt;- function( this_url ){ this_tf &lt;- tempfile() download.file( this_url , this_tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( this_tf , exdir = tempdir() ) this_stata &lt;- grep( &#39;\\\\.dta$&#39; , unzipped_files , value = TRUE ) this_tbl &lt;- read_stata( this_stata ) this_df &lt;- data.frame( this_tbl ) file.remove( c( this_tf , unzipped_files ) ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } Download and import the sample adult interview and imputed income files: nsch_screener_url &lt;- &quot;https://www2.census.gov/programs-surveys/nsch/datasets/2021/nsch_2021_screener_Stata.zip&quot; nsch_topical_url &lt;- &quot;https://www2.census.gov/programs-surveys/nsch/datasets/2021/nsch_2021_topical_Stata.zip&quot; nsch_screener_df &lt;- nsch_stata_import( nsch_screener_url ) nsch_df &lt;- nsch_stata_import( nsch_topical_url ) Save locally   Save the object at any point: # nsch_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NSCH&quot; , &quot;this_file.rds&quot; ) # saveRDS( nsch_df , file = nsch_fn , compress = FALSE ) Load the same object: # nsch_df &lt;- readRDS( nsch_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: Remove the fpl columns from the main data.frame: fpl_columns &lt;- grep( &#39;^fpl_i[0-9]&#39; , names( nsch_df ) , value = TRUE ) fpl_wide_df &lt;- nsch_df[ c( &#39;hhid&#39; , fpl_columns ) ] nsch_df[ fpl_columns ] &lt;- NULL Reshape the fpl columns from wide to long: fpl_long_df &lt;- reshape( fpl_wide_df , varying = list( fpl_columns ) , direction = &#39;long&#39; , timevar = &#39;implicate&#39; , idvar = &#39;hhid&#39; ) names( fpl_long_df )[ ncol( fpl_long_df ) ] &lt;- &#39;fpl&#39; Merge the fpl table with multiple records per child onto the main table: nsch_long_df &lt;- merge( nsch_df , fpl_long_df ) stopifnot( nrow( nsch_long_df ) == nrow( fpl_long_df ) ) stopifnot( nrow( nsch_long_df ) / length( fpl_columns ) == nrow( nsch_df ) ) Reshape the imputed income data.frame into a list based on the implicate number: nsch_list &lt;- split( nsch_long_df , nsch_long_df[ , &#39;implicate&#39; ] ) Define the design: library(survey) library(mitools) nsch_design &lt;- svydesign( id = ~ 1 , strata = ~ fipsst + stratum , weights = ~ fwc , data = imputationList( nsch_list ) , nest = TRUE ) Variable Recoding Add new columns to the data set: nsch_design &lt;- update( nsch_design , one = 1 , state_name = factor( fipsst , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;) ) , overall_health = factor( c( 1 , 1 , 2 , 3 , 3 )[ k2q01 ] , levels = 1:3 , labels = c( &#39;excellent or very good&#39; , &#39;good&#39; , &#39;fair or poor&#39; ) ) , poverty_categories = factor( 1 + findInterval( fpl , c( 100 , 200 , 400 ) ) , labels = c( &quot;below poverty&quot; , &quot;100-199% fpl&quot; , &quot;200-399% fpl&quot; , &quot;400%+ fpl&quot; ) ) , under_six_ever_breastfed = as.numeric( k6q40 == 1 ) , sc_sex = factor( ifelse( sc_sex %in% 1:2 , sc_sex , NA ) , labels = c( &quot;male&quot; , &quot;female&quot; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( nsch_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( nsch_design , svyby( ~ one , ~ state_name , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( nsch_design , svytotal( ~ one ) ) ) MIcombine( with( nsch_design , svyby( ~ one , ~ state_name , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( nsch_design , svymean( ~ sc_age_years ) ) ) MIcombine( with( nsch_design , svyby( ~ sc_age_years , ~ state_name , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( nsch_design , svymean( ~ poverty_categories ) ) ) MIcombine( with( nsch_design , svyby( ~ poverty_categories , ~ state_name , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( nsch_design , svytotal( ~ sc_age_years ) ) ) MIcombine( with( nsch_design , svyby( ~ sc_age_years , ~ state_name , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( nsch_design , svytotal( ~ poverty_categories ) ) ) MIcombine( with( nsch_design , svyby( ~ poverty_categories , ~ state_name , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( nsch_design , svyquantile( ~ sc_age_years , 0.5 , se = TRUE ) ) ) MIcombine( with( nsch_design , svyby( ~ sc_age_years , ~ state_name , svyquantile , 0.5 , se = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( nsch_design , svyratio( numerator = ~ liveusa_yr , denominator = ~ sc_age_years ) ) ) Subsetting Restrict the survey design to only children: sub_nsch_design &lt;- subset( nsch_design , agepos4 == 1 ) Calculate the mean (average) of this subset: MIcombine( with( sub_nsch_design , svymean( ~ sc_age_years ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( nsch_design , svymean( ~ sc_age_years ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( nsch_design , svyby( ~ sc_age_years , ~ state_name , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsch_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( nsch_design , svyvar( ~ sc_age_years ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( nsch_design , svymean( ~ sc_age_years , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( nsch_design , svymean( ~ sc_age_years , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ under_six_ever_breastfed , nsch_design , # method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( sc_age_years ~ under_six_ever_breastfed , nsch_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ under_six_ever_breastfed + poverty_categories , nsch_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( nsch_design , svyglm( sc_age_years ~ under_six_ever_breastfed + poverty_categories ) ) ) summary( glm_result ) Intermish mary cassatt’s young mother sewing Replication Example As noted in the bold red footnote of their published table, this technique is not correct and should not be used. The technical documents recommend a method matching the MIcombine syntax shown above. Nonetheless, this code matches statistics and confidence intervals within 0.1% from the Excellent or very good column of Indicator 1.1: In general, how would you describe this child’s health?: results &lt;- svyby( ~ as.numeric( overall_health == &#39;excellent or very good&#39; ) , ~ poverty_categories , nsch_design$designs[[1]] , svymean , na.rm = TRUE ) published_proportions &lt;- c( 0.833 , 0.859 , 0.907 , 0.955 ) published_lb &lt;- c( 0.810 , 0.838 , 0.894 , 0.949 ) published_ub &lt;- c( 0.854 , 0.878 , 0.919 , 0.961 ) stopifnot( all( round( coef( results ) , 3 ) == published_proportions ) ) ( ci_results &lt;- confint( results ) ) stopifnot( all( abs( ci_results[ , 1 ] - published_lb ) &lt; 0.0015 ) ) stopifnot( all( abs( ci_results[ , 2 ] - published_ub ) &lt; 0.0015 ) ) "],["national-survey-on-drug-use-and-health-nsduh.html", "National Survey on Drug Use and Health (NSDUH) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " National Survey on Drug Use and Health (NSDUH) The primary survey to measure of prevalence of substance use and its correlates in the United States. One table with one row per sampled respondent. A complex survey designed to generalize to civilian, non-institutional americans aged 12 and older. Released periodically since 1979 and annually since 1990. Administered by the Substance Abuse and Mental Health Services Administration. Please skim before you begin: 2021 National Survey on Drug Use and Health (NSDUH): Public Use File Codebook 2021 National Survey on Drug Use and Health (NSDUH): Methodological Summary and Definitions This human-composed haiku or a bouquet of artificial intelligence-generated limericks # drinking and thinking # about your first time, were you # smoking and joking? Download, Import, Preparation Download and import the national file: zip_tf &lt;- tempfile() zip_url &lt;- paste0( &quot;https://www.datafiles.samhsa.gov/sites/default/files/field-uploads-protected/&quot; , &quot;studies/NSDUH-2021/NSDUH-2021-datasets/NSDUH-2021-DS0001/&quot; , &quot;NSDUH-2021-DS0001-bundles-with-study-info/NSDUH-2021-DS0001-bndl-data-r_v3.zip&quot; ) download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) nsduh_rdata &lt;- unzip( zip_tf , exdir = tempdir() ) nsduh_rdata_contents &lt;- load( nsduh_rdata ) nsduh_df_name &lt;- grep( &#39;PUF&#39; , nsduh_rdata_contents , value = TRUE ) nsduh_df &lt;- get( nsduh_df_name ) names( nsduh_df ) &lt;- tolower( names( nsduh_df ) ) nsduh_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # nsduh_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NSDUH&quot; , &quot;this_file.rds&quot; ) # saveRDS( nsduh_df , file = nsduh_fn , compress = FALSE ) Load the same object: # nsduh_df &lt;- readRDS( nsduh_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) nsduh_design &lt;- svydesign( id = ~ verep , strata = ~ vestr_c , data = nsduh_df , weights = ~ analwt_c , nest = TRUE ) Variable Recoding Add new columns to the data set: nsduh_design &lt;- update( nsduh_design , one = 1 , health = factor( health , levels = 1:5 , labels = c( &quot;excellent&quot; , &quot;very good&quot; , &quot;good&quot; , &quot;fair&quot; , &quot;poor&quot; ) ) , age_first_cigarette = ifelse( cigtry &gt; 99 , NA , cigtry ) , age_tried_cocaine = ifelse( cocage &gt; 99 , NA , cocage ) , ever_used_marijuana = as.numeric( ifelse( mjever &lt; 4 , mjever == 1 , NA ) ) , county_type = factor( coutyp4 , levels = 1:3 , labels = c( &quot;large metro&quot; , &quot;small metro&quot; , &quot;nonmetro&quot; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nsduh_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ county_type , nsduh_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nsduh_design ) svyby( ~ one , ~ county_type , nsduh_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ health , nsduh_design , na.rm = TRUE ) svyby( ~ health , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ health , nsduh_design , na.rm = TRUE ) svyby( ~ health , ~ county_type , nsduh_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ age_first_cigarette , nsduh_design , 0.5 , na.rm = TRUE ) svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ age_first_cigarette , denominator = ~ age_tried_cocaine , nsduh_design , na.rm = TRUE ) Subsetting Restrict the survey design to individuals who are pregnant: sub_nsduh_design &lt;- subset( nsduh_design , preg == 1 ) Calculate the mean (average) of this subset: svymean( ~ age_first_cigarette , sub_nsduh_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsduh_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ ever_used_marijuana , nsduh_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( age_first_cigarette ~ ever_used_marijuana , nsduh_design ) Perform a chi-squared test of association for survey data: svychisq( ~ ever_used_marijuana + health , nsduh_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( age_first_cigarette ~ ever_used_marijuana + health , nsduh_design ) summary( glm_result ) Intermish whistler’s mom has got it goin’ on Replication Example This matches the prevalence and SE of alcohol use in the past month from Codebook Table G.2: result &lt;- svymean( ~ alcmon , nsduh_design ) stopifnot( round( coef( result ) , 3 ) == 0.474 ) stopifnot( round( SE( result ) , 4 ) == 0.0043 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NSDUH users, this code replicates previously-presented examples: library(srvyr) nsduh_srvyr_design &lt;- as_survey( nsduh_design ) Calculate the mean (average) of a linear variable, overall and by groups: nsduh_srvyr_design %&gt;% summarize( mean = survey_mean( age_first_cigarette , na.rm = TRUE ) ) nsduh_srvyr_design %&gt;% group_by( county_type ) %&gt;% summarize( mean = survey_mean( age_first_cigarette , na.rm = TRUE ) ) "],["national-survey-of-family-growth-nsfg.html", "National Survey of Family Growth (NSFG) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " National Survey of Family Growth (NSFG) The principal survey to measure reproductive behavior in the United States population. Multiple tables with one row per respondent for the female and male tables, then a separate table with one row per pregnancy. A complex sample survey designed to generalize to the 15-49 year old population of the United States, by gender. Released every couple of years since 1973. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: Sample Design Documentation Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # family structure # questions cuz radar fails at # storks with bassinets Download, Import, Preparation library(SAScii) library(readr) dat_url &lt;- &quot;https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NSFG/2017_2019_FemRespData.dat&quot; sas_url &lt;- file.path( dirname( dat_url ) , &quot;sas/2017_2019_FemRespSetup.sas&quot; ) sas_positions &lt;- parse.SAScii( sas_url ) sas_positions[ , &#39;varname&#39; ] &lt;- tolower( sas_positions[ , &#39;varname&#39; ] ) sas_positions[ , &#39;column_types&#39; ] &lt;- ifelse( sas_positions[ , &#39;char&#39; ] , &quot;c&quot; , &quot;d&quot; ) nsfg_tbl &lt;- read_fwf( dat_url , fwf_widths( abs( sas_positions[ , &#39;width&#39; ] ) , col_names = sas_positions[ , &#39;varname&#39; ] ) , col_types = paste0( sas_positions[ , &#39;column_types&#39; ] , collapse = &quot;&quot; ) , na = c( &quot;&quot; , &quot;.&quot; ) ) nsfg_df &lt;- data.frame( nsfg_tbl ) Save locally   Save the object at any point: # nsfg_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NSFG&quot; , &quot;this_file.rds&quot; ) # saveRDS( nsfg_df , file = nsfg_fn , compress = FALSE ) Load the same object: # nsfg_df &lt;- readRDS( nsfg_fn ) Survey Design Definition Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nsfg_design &lt;- svydesign( id = ~ secu , strata = ~ sest , data = nsfg_df , weights = ~ wgt2017_2019 , nest = TRUE ) Variable Recoding Add new columns to the data set: nsfg_design &lt;- update( nsfg_design , one = 1 , birth_control_pill = as.numeric( constat1 == 6 ) , age_categories = factor( findInterval( ager , c( 15 , 20 , 25 , 30 , 35 , 40 ) ) , labels = c( &#39;15-19&#39; , &#39;20-24&#39; , &#39;25-29&#39; , &#39;30-34&#39; , &#39;35-39&#39; , &#39;40-49&#39; ) ) , marstat = factor( marstat , levels = c( 1:6 , 8:9 ) , labels = c( &quot;Married to a person of the opposite sex&quot; , &quot;Not married but living together with a partner of the opposite sex&quot; , &quot;Widowed&quot; , &quot;Divorced or annulled&quot; , &quot;Separated, because you and your spouse are not getting along&quot; , &quot;Never been married&quot; , &quot;Refused&quot; , &quot;Don&#39;t know&quot; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nsfg_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_categories , nsfg_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nsfg_design ) svyby( ~ one , ~ age_categories , nsfg_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ pregnum , nsfg_design , na.rm = TRUE ) svyby( ~ pregnum , ~ age_categories , nsfg_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ marstat , nsfg_design ) svyby( ~ marstat , ~ age_categories , nsfg_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ pregnum , nsfg_design , na.rm = TRUE ) svyby( ~ pregnum , ~ age_categories , nsfg_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ marstat , nsfg_design ) svyby( ~ marstat , ~ age_categories , nsfg_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ pregnum , nsfg_design , 0.5 , na.rm = TRUE ) svyby( ~ pregnum , ~ age_categories , nsfg_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ pregnum , denominator = ~ lbpregs , nsfg_design , na.rm = TRUE ) Subsetting Restrict the survey design to ever cohabited: sub_nsfg_design &lt;- subset( nsfg_design , timescoh &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ pregnum , sub_nsfg_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ pregnum , nsfg_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ pregnum , ~ age_categories , nsfg_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsfg_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ pregnum , nsfg_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ pregnum , nsfg_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ pregnum , nsfg_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ birth_control_pill , nsfg_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( pregnum ~ birth_control_pill , nsfg_design ) Perform a chi-squared test of association for survey data: svychisq( ~ birth_control_pill + marstat , nsfg_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( pregnum ~ birth_control_pill + marstat , nsfg_design ) summary( glm_result ) Intermish wilma’s revenge Replication Example This example matches the Variance Estimates for Percentages using SAS (9.4) and STATA (14): Match the sum of the weights: result &lt;- svytotal( ~ one , nsfg_design ) stopifnot( round( coef( result ) , 0 ) == 72671926 ) stopifnot( round( SE( result ) , 0 ) == 3521465 ) Match row percentages of women currently using the pill by age: row_percents &lt;- c( 19.5112 , 23.7833 , 19.6916 , 15.2800 , 6.4965 , 6.5215 ) std_err_row_percents &lt;- c( 1.8670 , 2.1713 , 2.2773 , 1.7551 , 0.9895 , 1.0029 ) results &lt;- svyby( ~ birth_control_pill , ~ age_categories , nsfg_design , svymean ) stopifnot( all( round( coef( results ) * 100 , 4 ) == row_percents ) ) stopifnot( all( round( SE( results ) * 100 , 4 ) == std_err_row_percents ) ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NSFG users, this code replicates previously-presented examples: library(srvyr) nsfg_srvyr_design &lt;- as_survey( nsfg_design ) Calculate the mean (average) of a linear variable, overall and by groups: nsfg_srvyr_design %&gt;% summarize( mean = survey_mean( pregnum , na.rm = TRUE ) ) nsfg_srvyr_design %&gt;% group_by( age_categories ) %&gt;% summarize( mean = survey_mean( pregnum , na.rm = TRUE ) ) "],["new-york-city-housing-and-vacancy-survey-nychvs.html", "New York City Housing and Vacancy Survey (NYCHVS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " New York City Housing and Vacancy Survey (NYCHVS) The New York City Housing and Vacancy Survey (NYCHVS) covers the city-wide rental vacancy rate and other characteristics like neighborhood housing stock. One table with one record per occupied housing unit, a second table with one record per person inside each occupied housing unit, and a third table with one record per unoccupied housing unit. A complex sample survey designed to generalize to all occupied and unoccupied housing units in the five boroughs of New York City. Released more or less triennially since 1991. Funded by the New York City Department of Housing Preservation and Development and conducted by the United States Census Bureau. Please skim before you begin: Public Use File User Guide and Codebook Sample Design, Weighting, and Error Estimation This human-composed haiku or a bouquet of artificial intelligence-generated limericks # all i want is a # room somewhere / with clawfoot tub # and a frigidaire Download, Import, Preparation Define a function to download and import each comma-separated value file: nychvs_csv_import &lt;- function( this_url ){ tf &lt;- tempfile() download.file( this_url , tf , mode = &#39;wb&#39; ) this_df &lt;- read.csv( tf ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } Download and import the all units, occupied units, person, and vacant units tables: all_units_df &lt;- nychvs_csv_import( &quot;https://www2.census.gov/programs-surveys/nychvs/datasets/2021/microdata/allunits_puf_21.csv&quot; ) occupied_units_df &lt;- nychvs_csv_import( &quot;https://www2.census.gov/programs-surveys/nychvs/datasets/2021/microdata/occupied_puf_21.csv&quot; ) person_df &lt;- nychvs_csv_import( &quot;https://www2.census.gov/programs-surveys/nychvs/datasets/2021/microdata/person_puf_21.csv&quot; ) vacant_units_df &lt;- nychvs_csv_import( &quot;https://www2.census.gov/programs-surveys/nychvs/datasets/2021/microdata/vacant_puf_21.csv&quot; ) stopifnot( nrow( all_units_df ) == nrow( occupied_units_df ) + nrow( vacant_units_df ) ) Merge the information stored in the all units table onto both the occupied and vacant unit tables: before_nrow &lt;- nrow( occupied_units_df ) occupied_units_df &lt;- merge( all_units_df , occupied_units_df ) stopifnot( nrow( occupied_units_df ) == before_nrow ) before_nrow &lt;- nrow( vacant_units_df ) vacant_units_df &lt;- merge( all_units_df , vacant_units_df ) stopifnot( nrow( vacant_units_df ) == before_nrow ) stopifnot( nrow( nychvs_df ) == nrow( nychvs_df_person ) ) nychvs_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # nychvs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NYCHVS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nychvs_df , file = nychvs_fn , compress = FALSE ) Load the same object: # nychvs_df &lt;- readRDS( nychvs_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) nychvs_design &lt;- svrepdesign( weight = ~fw , repweights = &#39;fw[0-9]+&#39; , scale = 4 / 80 , rscales = rep( 1 , 80 ) , mse = TRUE , type = &#39;JK1&#39; , data = nychvs_df ) Variable Recoding Add new columns to the data set: nychvs_design &lt;- update( nychvs_design , one = 1 , home_owners = as.numeric( sc115 == 1 ) , yearly_household_income = ifelse( uf42 == 9999999 , 0 , as.numeric( uf42 ) ) , gross_monthly_rent = ifelse( uf17 == 99999 , NA , as.numeric( uf17 ) ) , borough = factor( boro , levels = 1:5 , labels = c( &#39;Bronx&#39; , &#39;Brooklyn&#39; , &#39;Manhattan&#39; , &#39;Queens&#39; , &#39;Staten Island&#39; ) ) , householder_sex = factor( hhr2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nychvs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ borough , nychvs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nychvs_design ) svyby( ~ one , ~ borough , nychvs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) svyby( ~ yearly_household_income , ~ borough , nychvs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ householder_sex , nychvs_design ) svyby( ~ householder_sex , ~ borough , nychvs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) svyby( ~ yearly_household_income , ~ borough , nychvs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ householder_sex , nychvs_design ) svyby( ~ householder_sex , ~ borough , nychvs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ yearly_household_income , nychvs_design , 0.5 , na.rm = TRUE ) svyby( ~ yearly_household_income , ~ borough , nychvs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ gross_monthly_rent , denominator = ~ yearly_household_income , nychvs_design , na.rm = TRUE ) Subsetting Restrict the survey design to Manhattan: sub_nychvs_design &lt;- subset( nychvs_design , boro == 3 ) Calculate the mean (average) of this subset: svymean( ~ yearly_household_income , sub_nychvs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ yearly_household_income , ~ borough , nychvs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nychvs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ home_owners , nychvs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( yearly_household_income ~ home_owners , nychvs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ home_owners + householder_sex , nychvs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( yearly_household_income ~ home_owners + householder_sex , nychvs_design ) summary( glm_result ) Intermish Replication Example Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NYCHVS users, this code replicates previously-presented examples: library(srvyr) nychvs_srvyr_design &lt;- as_survey( nychvs_design ) Calculate the mean (average) of a linear variable, overall and by groups: nychvs_srvyr_design %&gt;% summarize( mean = survey_mean( yearly_household_income , na.rm = TRUE ) ) nychvs_srvyr_design %&gt;% group_by( borough ) %&gt;% summarize( mean = survey_mean( yearly_household_income , na.rm = TRUE ) ) "],["programme-for-the-international-assessment-of-adult-competencies-piaac.html", "Programme for the International Assessment of Adult Competencies (PIAAC) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " Programme for the International Assessment of Adult Competencies (PIAAC) A cross-national study designed to understand the skills of workers in advanced-nation labor markets. One row per sampled adult. A multiply-imputed, complex sample survey designed to generalize to the population aged 16 to 65 across thirty three OECD nations. No expected release timeline. Administered by the Organisation for Economic Co-operation and Development. Please skim before you begin: Technical Report of the Survey of Adult Skills Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # what color collar # workforce poets, potters, or # pythagoreans Download, Import, Preparation library(haven) library(httr) tf &lt;- tempfile() this_url &lt;- &quot;https://webfs.oecd.org/piaac/puf-data/SAS/SAS7BDAT/prgusap1_2012.sas7bdat&quot; GET( this_url , write_disk( tf ) , progress() ) piaac_tbl &lt;- read_sas( tf ) piaac_df &lt;- data.frame( piaac_tbl ) names( piaac_df ) &lt;- tolower( names( piaac_df ) ) Save locally   Save the object at any point: # piaac_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;PIAAC&quot; , &quot;this_file.rds&quot; ) # saveRDS( piaac_df , file = piaac_fn , compress = FALSE ) Load the same object: # piaac_df &lt;- readRDS( piaac_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) pvals &lt;- c( &quot;pvlit&quot; , &quot;pvnum&quot; , &quot;pvpsl&quot; ) pvars &lt;- outer( pvals , 1:10 , paste0 ) non.pvals &lt;- names(piaac_df)[ !( names(piaac_df) %in% pvars ) ] for(k in 1:10){ piaac_imp &lt;- piaac_df[ , c( non.pvals , paste0( pvals , k ) ) ] for( j in pvals ){ piaac_imp[ , j ] &lt;- piaac_imp[ , paste0( j , k ) ] piaac_imp[ , paste0( j , k ) ] &lt;- NULL } if( k == 1 ){ piaac_mi &lt;- list( piaac_imp ) } else { piaac_mi &lt;- c( piaac_mi , list( piaac_imp ) ) } } jk.method &lt;- unique( piaac_df[ , &#39;vemethod&#39; ] ) stopifnot(length(jk.method) == 1) stopifnot(jk.method %in% c(&quot;JK1&quot;, &quot;JK2&quot;)) if (jk.method == &quot;JK2&quot;) jk.method &lt;- &quot;JKn&quot; piaac_design &lt;- svrepdesign( weights = ~spfwt0 , repweights = &quot;spfwt[1-9]&quot; , rscales = rep( 1 , 80 ) , scale = ifelse( jk.method == &quot;JKn&quot; , 1 , 79/80 ) , type = jk.method , data = imputationList( piaac_mi ) , mse = TRUE ) Variable Recoding Add new columns to the data set: piaac_design &lt;- update( piaac_design , one = 1 , sex = factor( gender_r , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , age_categories = factor( ageg10lfs , levels = 1:5 , labels = c( &quot;24 or less&quot; , &quot;25-34&quot; , &quot;35-44&quot; , &quot;45-54&quot; , &quot;55 plus&quot; ) ) , working_at_paid_job_last_week = as.numeric( c_q01a == 1 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( piaac_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( piaac_design , svyby( ~ one , ~ age_categories , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ one ) ) ) MIcombine( with( piaac_design , svyby( ~ one , ~ age_categories , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( piaac_design , svymean( ~ sex ) ) ) MIcombine( with( piaac_design , svyby( ~ sex , ~ age_categories , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ pvnum , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svytotal , na.rm = TRUE ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ sex ) ) ) MIcombine( with( piaac_design , svyby( ~ sex , ~ age_categories , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( piaac_design , svyquantile( ~ pvnum , 0.5 , se = TRUE , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svyquantile , 0.5 , se = TRUE , ci = TRUE , na.rm = TRUE ) ) ) Estimate a ratio: MIcombine( with( piaac_design , svyratio( numerator = ~ pvnum , denominator = ~ pvlit , na.rm = TRUE ) ) ) Subsetting Restrict the survey design to self-reported fair or poor health: sub_piaac_design &lt;- subset( piaac_design , i_q08 %in% 4:5 ) Calculate the mean (average) of this subset: MIcombine( with( sub_piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( piaac_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( piaac_design , svyvar( ~ pvnum , na.rm = TRUE ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ working_at_paid_job_last_week , piaac_design , # method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( pvnum ~ working_at_paid_job_last_week , piaac_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ working_at_paid_job_last_week + sex , piaac_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( piaac_design , svyglm( pvnum ~ working_at_paid_job_last_week + sex ) ) ) summary( glm_result ) Intermish makoto aida’s the ash color mountain Replication Example This example matches the statistics and standard errors from OECD’s Technical Report Table 18.9: usa_pvlit &lt;- MIcombine( with( piaac_design , svymean( ~ pvlit , na.rm = TRUE ) ) ) usa_pvnum &lt;- MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) usa_pvpsl &lt;- MIcombine( with( piaac_design , svymean( ~ pvpsl , na.rm = TRUE ) ) ) stopifnot( round( coef( usa_pvlit ) ) == 270 ) stopifnot( round( SE( usa_pvlit ) , 1 ) == 1.0 ) stopifnot( round( coef( usa_pvnum ) ) == 253 ) stopifnot( round( SE( usa_pvnum ) , 1 ) == 1.2 ) stopifnot( round( coef( usa_pvpsl ) ) == 277 ) stopifnot( round( SE( usa_pvpsl ) , 1 ) == 1.1 ) "],["progress-in-international-reading-literacy-study-pirls.html", "Progress in International Reading Literacy Study (PIRLS) Function Definitions Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " Progress in International Reading Literacy Study (PIRLS) A comparative study of student achievement in reading and literacy across more than 50 nations. Grade-specific tables with one record per school, student, teacher, plus files containing student achievement, home background, student-teacher linkage, and within-country scoring reliability. A complex survey generalizing to fourth-grade populations of participating countries. Released quinquennially since 2001. Funded by the International Association for the Evaluation of Educational Achievement, run at BC. Please skim before you begin: PIRLS 2021 User Guide for the International Database Methods and Procedures: PIRLS 2021 Technical Report This human-composed haiku or a bouquet of artificial intelligence-generated limericks # lascaux canary # glyph jump reveal caged bard notes # cryogenesis Function Definitions This survey uses a multiply-imputed variance estimation technique described in Methods Chapter 13. Most users do not need to study this function carefully. Define a function specific to only this dataset: pirls_MIcombine &lt;- function (results, variances, call = sys.call(), df.complete = Inf, ...) { m &lt;- length(results) oldcall &lt;- attr(results, &quot;call&quot;) if (missing(variances)) { variances &lt;- suppressWarnings(lapply(results, vcov)) results &lt;- lapply(results, coef) } vbar &lt;- variances[[1]] cbar &lt;- results[[1]] for (i in 2:m) { cbar &lt;- cbar + results[[i]] vbar &lt;- vbar + variances[[i]] } cbar &lt;- cbar/m vbar &lt;- vbar/m # MODIFICATION # evar &lt;- var(do.call(&quot;rbind&quot;, results)) evar &lt;- sum( ( unlist( results ) - cbar )^2 / 4 ) r &lt;- (1 + 1/m) * evar/vbar df &lt;- (m - 1) * (1 + 1/r)^2 if (is.matrix(df)) df &lt;- diag(df) if (is.finite(df.complete)) { dfobs &lt;- ((df.complete + 1)/(df.complete + 3)) * df.complete * vbar/(vbar + evar) if (is.matrix(dfobs)) dfobs &lt;- diag(dfobs) df &lt;- 1/(1/dfobs + 1/df) } if (is.matrix(r)) r &lt;- diag(r) rval &lt;- list(coefficients = cbar, variance = vbar + evar * (m + 1)/m, call = c(oldcall, call), nimp = m, df = df, missinfo = (r + 2/(df + 3))/(r + 1)) class(rval) &lt;- &quot;MIresult&quot; rval } Download, Import, Preparation Download and unzip the 2021 fourth grade international database: library(httr) tf &lt;- tempfile() this_url &lt;- &quot;https://pirls2021.org/data/downloads/P21_Data_R.zip&quot; GET( this_url , write_disk( tf ) , progress() ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) Import and stack each of the student context data files for Abu Dhabi through Bulgaria: library(haven) # limit unzipped files to those starting with `asg` followed by three letters followed by `r5` asg_fns &lt;- unzipped_files[ grepl( &#39;^asg[a-z][a-z][a-z]r5&#39; , basename( unzipped_files ) , ignore.case = TRUE ) ] # further limit asg files to the first ten countries countries_thru_bulgaria &lt;- c(&quot;aad&quot;, &quot;adu&quot;, &quot;alb&quot;, &quot;are&quot;, &quot;aus&quot;, &quot;aut&quot;, &quot;aze&quot;, &quot;bfl&quot;, &quot;bfr&quot;, &quot;bgr&quot;) fns_thru_bulgaria &lt;- paste0( paste0( &#39;^asg&#39; , countries_thru_bulgaria , &#39;r5&#39; ) , collapse = &quot;|&quot; ) asg_aad_bgr_fns &lt;- asg_fns[ grepl( fns_thru_bulgaria , basename( asg_fns ) , ignore.case = TRUE ) ] pirls_df &lt;- NULL for( rdata_fn in asg_aad_bgr_fns ){ this_tbl_name &lt;- load( rdata_fn ) this_tbl &lt;- get( this_tbl_name ) ; rm( this_tbl_name ) this_tbl &lt;- zap_labels( this_tbl ) this_df &lt;- data.frame( this_tbl ) names( this_df ) &lt;- tolower( names( this_df ) ) pirls_df &lt;- rbind( pirls_df , this_df ) } # order the data.frame by unique student id pirls_df &lt;- pirls_df[ with( pirls_df , order( idcntry , idstud ) ) , ] Save locally   Save the object at any point: # pirls_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;PIRLS&quot; , &quot;this_file.rds&quot; ) # saveRDS( pirls_df , file = pirls_fn , compress = FALSE ) Load the same object: # pirls_df &lt;- readRDS( pirls_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: From among possibly plausible values, determine all columns that are multiply-imputed plausible values: # identify all columns ending with `01` thru `05` ppv &lt;- grep( &quot;(.*)0[1-5]$&quot; , names( pirls_df ) , value = TRUE ) # remove those ending digits ppv_prefix &lt;- gsub( &quot;0[1-5]$&quot; , &quot;&quot; , ppv ) # identify each of the possibilities with exactly five matches (five implicates) pv &lt;- names( table( ppv_prefix )[ table( ppv_prefix ) == 5 ] ) # identify each of the `01` thru `05` plausible value columns pv_columns &lt;- grep( paste0( &quot;^&quot; , pv , &quot;0[1-5]$&quot; , collapse = &quot;|&quot; ) , names( pirls_df ) , value = TRUE ) Extract those multiply-imputed columns into a separate data.frame, then remove them from the source: pv_wide_df &lt;- pirls_df[ c( &#39;idcntry&#39; , &#39;idstud&#39; , pv_columns ) ] pirls_df[ pv_columns ] &lt;- NULL Reshape these columns from one record per student to one record per student per implicate: pv_long_df &lt;- reshape( pv_wide_df , varying = lapply( paste0( pv , &#39;0&#39; ) , paste0 , 1:5 ) , direction = &#39;long&#39; , timevar = &#39;implicate&#39; , idvar = c( &#39;idcntry&#39; , &#39;idstud&#39; ) ) names( pv_long_df ) &lt;- gsub( &quot;01$&quot; , &quot;&quot; , names( pv_long_df ) ) Merge the columns from the source data.frame onto the one record per student per implicate data.frame: pirls_long_df &lt;- merge( pirls_df , pv_long_df ) pirls_long_df &lt;- pirls_long_df[ with( pirls_long_df , order( idcntry , idstud ) ) , ] stopifnot( nrow( pirls_long_df ) == nrow( pv_long_df ) ) stopifnot( nrow( pirls_long_df ) / 5 == nrow( pirls_df ) ) Divide the five plausible value implicates into a list with five data.frames based on the implicate number: pirls_list &lt;- split( pirls_long_df , pirls_long_df[ , &#39;implicate&#39; ] ) Construct a replicate weights table following the estimation technique described in Methods Chapter 13: weights_df &lt;- pirls_df[ c( &#39;jkrep&#39; , &#39;jkzone&#39; ) ] for( j in 1:75 ){ for( i in 0:1 ){ weights_df[ weights_df[ , &#39;jkzone&#39; ] != j , paste0( &#39;rw&#39; , i , j ) ] &lt;- 1 weights_df[ weights_df[ , &#39;jkzone&#39; ] == j , paste0( &#39;rw&#39; , i , j ) ] &lt;- 2 * ( weights_df[ weights_df[ , &#39;jkzone&#39; ] == j , &#39;jkrep&#39; ] == i ) } } weights_df[ c( &#39;jkrep&#39; , &#39;jkzone&#39; ) ] &lt;- NULL Define the design: library(survey) library(mitools) pirls_design &lt;- svrepdesign( weights = ~totwgt , repweights = weights_df , data = imputationList( pirls_list ) , type = &quot;other&quot; , scale = 0.5 , rscales = rep( 1 , 150 ) , combined.weights = FALSE , mse = TRUE ) Variable Recoding Add new columns to the data set: pirls_design &lt;- update( pirls_design , one = 1 , countries_thru_bulgaria = factor( as.numeric( idcntry ) , levels = c(7842L, 7841L, 8L, 784L, 36L, 40L, 31L, 956L, 957L, 100L) , labels = c(&quot;Abu Dhabi, UAE&quot;, &quot;Dubai, UAE&quot;, &quot;Albania&quot;, &quot;UAE&quot;, &quot;Australia&quot;, &quot;Austria&quot;, &quot;Azerbaijan&quot;, &quot;Belgium (Flemish)&quot;, &quot;Belgium (French)&quot;,&quot;Bulgaria&quot;) ) , sex = factor( itsex , levels = 1:2 , labels = c( &quot;female&quot; , &quot;male&quot; ) ) , always_speak_language_of_test_at_home = ifelse( asbg03 %in% 1:4 , as.numeric( asbg03 == 1 ) , NA ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: pirls_MIcombine( with( pirls_design , svyby( ~ one , ~ one , unwtd.count ) ) ) pirls_MIcombine( with( pirls_design , svyby( ~ one , ~ sex , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: pirls_MIcombine( with( pirls_design , svytotal( ~ one ) ) ) pirls_MIcombine( with( pirls_design , svyby( ~ one , ~ sex , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: pirls_MIcombine( with( pirls_design , svymean( ~ asrrea , na.rm = TRUE ) ) ) pirls_MIcombine( with( pirls_design , svyby( ~ asrrea , ~ sex , svymean , na.rm = TRUE ) ) ) Calculate the distribution of a categorical variable, overall and by groups: pirls_MIcombine( with( pirls_design , svymean( ~ countries_thru_bulgaria ) ) ) pirls_MIcombine( with( pirls_design , svyby( ~ countries_thru_bulgaria , ~ sex , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: pirls_MIcombine( with( pirls_design , svytotal( ~ asrrea , na.rm = TRUE ) ) ) pirls_MIcombine( with( pirls_design , svyby( ~ asrrea , ~ sex , svytotal , na.rm = TRUE ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: pirls_MIcombine( with( pirls_design , svytotal( ~ countries_thru_bulgaria ) ) ) pirls_MIcombine( with( pirls_design , svyby( ~ countries_thru_bulgaria , ~ sex , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: pirls_MIcombine( with( pirls_design , svyquantile( ~ asrrea , 0.5 , se = TRUE , na.rm = TRUE ) ) ) pirls_MIcombine( with( pirls_design , svyby( ~ asrrea , ~ sex , svyquantile , 0.5 , se = TRUE , ci = TRUE , na.rm = TRUE ) ) ) Estimate a ratio: pirls_MIcombine( with( pirls_design , svyratio( numerator = ~ asrlit , denominator = ~ asrrea ) ) ) Subsetting Restrict the survey design to Australia, Austria, Azerbaijan, Belgium (French): sub_pirls_design &lt;- subset( pirls_design , idcntry %in% c( 36 , 40 , 31 , 956 ) ) Calculate the mean (average) of this subset: pirls_MIcombine( with( sub_pirls_design , svymean( ~ asrrea , na.rm = TRUE ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- pirls_MIcombine( with( pirls_design , svymean( ~ asrrea , na.rm = TRUE ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- pirls_MIcombine( with( pirls_design , svyby( ~ asrrea , ~ sex , svymean , na.rm = TRUE ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pirls_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: pirls_MIcombine( with( pirls_design , svyvar( ~ asrrea , na.rm = TRUE ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement pirls_MIcombine( with( pirls_design , svymean( ~ asrrea , na.rm = TRUE , deff = TRUE ) ) ) # SRS with replacement pirls_MIcombine( with( pirls_design , svymean( ~ asrrea , na.rm = TRUE , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ always_speak_language_of_test_at_home , pirls_design , # method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( asrrea ~ always_speak_language_of_test_at_home , pirls_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ always_speak_language_of_test_at_home + countries_thru_bulgaria , pirls_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- pirls_MIcombine( with( pirls_design , svyglm( asrrea ~ always_speak_language_of_test_at_home + countries_thru_bulgaria ) ) ) summary( glm_result ) Intermish toucan samlet, prince of denmark Replication Example This example matches the mean proficiency and standard error of the Australia row of the Summary Statistics and Standard Errors for Proficiency in Overall Reading table from the Appendix 13A: Summary Statistics and Standard Errors for Proficiency in Reading: australia_design &lt;- subset( pirls_design , countries_thru_bulgaria %in% &quot;Australia&quot; ) stopifnot( nrow( australia_design ) == 5487 ) result &lt;- pirls_MIcombine( with( australia_design , svymean( ~ asrrea ) ) ) stopifnot( round( coef( result ) , 3 ) == 540.134 ) stopifnot( round( SE( result ) , 3 ) == 1.728 ) This example matches the jackknife sampling, imputation, and total variances of the same row: australia_fn &lt;- unzipped_files[ grepl( &#39;ASGAUS&#39; , basename( unzipped_files ) ) ] australia_tbl_name &lt;- load( australia_fn ) australia_tbl &lt;- get( australia_tbl_name ) ; rm( australia_tbl_name ) australia_tbl &lt;- zap_labels( australia_tbl ) australia_df &lt;- data.frame( australia_tbl ) names( australia_df ) &lt;- tolower( names( australia_df ) ) estimate &lt;- mean( c( with( australia_df , weighted.mean( asrrea01 , totwgt ) ) , with( australia_df , weighted.mean( asrrea02 , totwgt ) ) , with( australia_df , weighted.mean( asrrea03 , totwgt ) ) , with( australia_df , weighted.mean( asrrea04 , totwgt ) ) , with( australia_df , weighted.mean( asrrea05 , totwgt ) ) ) ) stopifnot( round( estimate , 3 ) == 540.134 ) for( k in 1:5 ){ this_variance &lt;- 0 for( j in 1:75 ){ for( i in 0:1 ){ this_variance &lt;- this_variance + ( weighted.mean( australia_df[ , paste0( &#39;asrrea0&#39; , k ) ] , ifelse( j == australia_df[ , &#39;jkzone&#39; ] , australia_df[ , &#39;totwgt&#39; ] * 2 * ( australia_df[ , &#39;jkrep&#39; ] == i ) , australia_df[ , &#39;totwgt&#39; ] ) ) - weighted.mean( australia_df[ , paste0( &#39;asrrea0&#39; , k ) ] , australia_df[ , &#39;totwgt&#39; ] ) )^2 } } assign( paste0( &#39;v&#39; , k ) , this_variance * 0.5 ) } sampling_variance &lt;- mean( c( v1 , v2 , v3 , v4 , v5 ) ) stopifnot( round( sampling_variance , 3 ) == 2.653 ) imputation_variance &lt;- ( 6 / 5 ) * ( ( ( with( australia_df , weighted.mean( asrrea01 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asrrea02 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asrrea03 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asrrea04 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asrrea05 , totwgt ) ) - estimate )^2 / 4 ) ) stopifnot( round( imputation_variance , 3 ) == 0.333 ) stopifnot( round( sampling_variance + imputation_variance , 3 ) == 2.987 ) "],["public-libraries-survey-pls.html", "Public Libraries Survey (PLS) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " Public Libraries Survey (PLS) A comprehensive compilation of administrative information on all public libraries in the United States. Two tables, with one record per library system and one record per library building or bookmobile. Released annually since 1992. Conducted by the Institute of Museum and Library Services (IMLS), collected by the Census Bureau. Please skim before you begin: Data File Documentation and User’s Guide README FY #### PLS PUD.txt included in each zipped file This human-composed haiku or a bouquet of artificial intelligence-generated limericks # census, not survey. # dewey decimal index # finger to lips shush Download, Import, Preparation Download and import the most recent administrative entity csv file: this_tf &lt;- tempfile() csv_url &lt;- &quot;https://www.imls.gov/sites/default/files/2023-06/pls_fy2021_csv.zip&quot; download.file( csv_url , this_tf, mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( this_tf , exdir = tempdir() ) administrative_entity_csv_fn &lt;- unzipped_files[ grepl( &#39;AE(.*)csv$&#39; , basename( unzipped_files ) ) ] pls_df &lt;- read.csv( administrative_entity_csv_fn ) names( pls_df ) &lt;- tolower( names( pls_df ) ) pls_df[ , &#39;one&#39; ] &lt;- 1 Recode missing values as described in the readme included with each zipped file: for( this_col in names( pls_df ) ){ if( class( pls_df[ , this_col ] ) == &#39;character&#39; ){ pls_df[ pls_df[ , this_col ] %in% &#39;M&#39; , this_col ] &lt;- NA } if( ( class( pls_df[ , this_col ] ) == &#39;numeric&#39; ) | ( this_col %in% c( &#39;phone&#39; , &#39;startdat&#39; , &#39;enddate&#39; ) ) ){ pls_df[ pls_df[ , this_col ] %in% c( -1 , -3 , -4 , -9 ) , this_col ] &lt;- NA } } Save locally   Save the object at any point: # pls_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;PLS&quot; , &quot;this_file.rds&quot; ) # saveRDS( pls_df , file = pls_fn , compress = FALSE ) Load the same object: # pls_df &lt;- readRDS( pls_fn ) Variable Recoding Add new columns to the data set: pls_df &lt;- transform( pls_df , c_relatn = factor( c_relatn , levels = c( &quot;HQ&quot; , &quot;ME&quot; , &quot;NO&quot; ) , c( &quot;HQ-Headquarters of a federation or cooperative&quot; , &quot;ME-Member of a federation or cooperative&quot; , &quot;NO-Not a member of a federation or cooperative&quot; ) ) , more_than_one_librarian = as.numeric( libraria &gt; 1 ) ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( pls_df ) table( pls_df[ , &quot;stabr&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( pls_df[ , &quot;popu_lsa&quot; ] , na.rm = TRUE ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( pls_df[ , &quot;c_relatn&quot; ] ) ) prop.table( table( pls_df[ , c( &quot;c_relatn&quot; , &quot;stabr&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( pls_df[ , &quot;popu_lsa&quot; ] , na.rm = TRUE ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( pls_df[ , &quot;popu_lsa&quot; ] , 0.5 , na.rm = TRUE ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to more than one million annual visits: sub_pls_df &lt;- subset( pls_df , visits &gt; 1000000 ) Calculate the mean (average) of this subset: mean( sub_pls_df[ , &quot;popu_lsa&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( pls_df[ , &quot;popu_lsa&quot; ] , na.rm = TRUE ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( popu_lsa ~ more_than_one_librarian , pls_df ) Perform a chi-squared test of association: this_table &lt;- table( pls_df[ , c( &quot;more_than_one_librarian&quot; , &quot;c_relatn&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( popu_lsa ~ more_than_one_librarian + c_relatn , data = pls_df ) summary( glm_result ) Intermish the author as mikhail gorbachev Replication Example This example matches Interlibrary Relationship Frequencies on PDF page 169 of the User’s Guide: # remove closed and temporarily closed libraries results &lt;- table( pls_df[ !( pls_df[ , &#39;statstru&#39; ] %in% c( 3 , 23 ) ) , &#39;c_relatn&#39; ] ) stopifnot( results[ &quot;HQ-Headquarters of a federation or cooperative&quot; ] == 112 ) stopifnot( results[ &quot;ME-Member of a federation or cooperative&quot; ] == 6859 ) stopifnot( results[ &quot;NO-Not a member of a federation or cooperative&quot; ] == 2236 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for PLS users, this code replicates previously-presented examples: library(dplyr) pls_tbl &lt;- as_tibble( pls_df ) Calculate the mean (average) of a linear variable, overall and by groups: pls_tbl %&gt;% summarize( mean = mean( popu_lsa , na.rm = TRUE ) ) pls_tbl %&gt;% group_by( stabr ) %&gt;% summarize( mean = mean( popu_lsa , na.rm = TRUE ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for PLS users, this code replicates previously-presented examples: library(data.table) pls_dt &lt;- data.table( pls_df ) Calculate the mean (average) of a linear variable, overall and by groups: pls_dt[ , mean( popu_lsa , na.rm = TRUE ) ] pls_dt[ , mean( popu_lsa , na.rm = TRUE ) , by = stabr ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for PLS users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;pls&#39; , pls_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( popu_lsa ) FROM pls&#39; ) dbGetQuery( con , &#39;SELECT stabr , AVG( popu_lsa ) FROM pls GROUP BY stabr&#39; ) "],["pesquisa-nacional-por-amostra-de-domicilios-pnad.html", "Pesquisa Nacional por Amostra de Domicilios (PNAD) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Poverty and Inequality Estimation with convey   Analysis Examples with srvyr  ", " Pesquisa Nacional por Amostra de Domicilios (PNAD) Brazil’s principal labor force survey, measuring education, employment, income, housing characteristics. One consolidated table with one row per individual within each sampled household. A complex sample survey designed to generalize to the civilian non-institutional population of Brazil. Released quarterly since 2012, with microdata available both quarterly and annually. Administered by the Instituto Brasileiro de Geografia e Estatistica. Please skim before you begin: Conceitos e métodos Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # mineiro data # love verdade gave to me # twelve karaoke.. Download, Import, Preparation Download and import the dictionary file: dictionary_tf &lt;- tempfile() dictionary_url &lt;- paste0( &quot;https://ftp.ibge.gov.br/Trabalho_e_Rendimento/&quot; , &quot;Pesquisa_Nacional_por_Amostra_de_Domicilios_continua/&quot; , &quot;Trimestral/Microdados/Documentacao/Dicionario_e_input_20221031.zip&quot; ) download.file( dictionary_url , dictionary_tf , mode = &#39;wb&#39; ) dictionary_files &lt;- unzip( dictionary_tf , exdir = tempdir() ) sas_fn &lt;- grep( &#39;\\\\.sas$&#39; , dictionary_files , value = TRUE ) sas_lines &lt;- readLines( sas_fn , encoding = &#39;latin1&#39; ) Determine fixed-width file positions from the SAS import script: sas_start &lt;- grep( &#39;@0001&#39; , sas_lines ) sas_end &lt;- grep( &#39;;&#39; , sas_lines ) sas_end &lt;- sas_end[ sas_end &gt; sas_start ][ 1 ] sas_lines &lt;- sas_lines[ seq( sas_start , sas_end - 1 ) ] # remove SAS comments sas_lines &lt;- gsub( &quot;\\\\/\\\\*(.*)&quot; , &quot;&quot; , sas_lines ) # remove multiple spaces and spaces at the end of each string sas_lines &lt;- gsub( &quot;( +)&quot; , &quot; &quot; , sas_lines ) sas_lines &lt;- gsub( &quot; $&quot; , &quot;&quot; , sas_lines ) sas_df &lt;- read.table( textConnection( sas_lines ) , sep = &#39; &#39; , col.names = c( &#39;position&#39; , &#39;column_name&#39; , &#39;length&#39; ) , header = FALSE ) sas_df[ , &#39;character&#39; ] &lt;- grepl( &#39;\\\\$&#39; , sas_df[ , &#39;length&#39; ] ) sas_df[ , &#39;position&#39; ] &lt;- as.integer( gsub( &quot;\\\\@&quot; , &quot;&quot; , sas_df[ , &#39;position&#39; ] ) ) sas_df[ , &#39;length&#39; ] &lt;- as.integer( gsub( &quot;\\\\$&quot; , &quot;&quot; , sas_df[ , &#39;length&#39; ] ) ) stopifnot( sum( sas_df[ , &#39;length&#39; ] ) == ( sas_df[ nrow( sas_df ) , &#39;position&#39; ] + sas_df[ nrow( sas_df ) , &#39;length&#39; ] - 1 ) ) Download the latest quarterly file: this_tf &lt;- tempfile() this_url &lt;- paste0( &quot;https://ftp.ibge.gov.br/Trabalho_e_Rendimento/&quot; , &quot;Pesquisa_Nacional_por_Amostra_de_Domicilios_continua/&quot; , &quot;Trimestral/Microdados/2023/PNADC_012023.zip&quot; ) download.file( this_url , this_tf , mode = &#39;wb&#39; ) Import the latest quarterly file: library(readr) pnad_tbl &lt;- read_fwf( this_tf , fwf_widths( widths = sas_df[ , &#39;length&#39; ] , col_names = sas_df[ , &#39;column_name&#39; ] ) , col_types = paste0( ifelse( sas_df[ , &#39;character&#39; ] , &quot;c&quot; , &quot;d&quot; ) , collapse = &#39;&#39; ) ) pnad_df &lt;- data.frame( pnad_tbl ) names( pnad_df ) &lt;- tolower( names( pnad_df ) ) pnad_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # pnad_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;PNAD&quot; , &quot;this_file.rds&quot; ) # saveRDS( pnad_df , file = pnad_fn , compress = FALSE ) Load the same object: # pnad_df &lt;- readRDS( pnad_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) pnad_design &lt;- svrepdesign( data = pnad_df , weight = ~ v1028 , type = &#39;bootstrap&#39; , repweights = &#39;v1028[0-9]+&#39; , mse = TRUE , ) Variable Recoding Add new columns to the data set: pnad_design &lt;- update( pnad_design , pia = as.numeric( v2009 &gt;= 14 ) ) pnad_design &lt;- update( pnad_design , ocup_c = ifelse( pia == 1 , as.numeric( vd4002 %in% 1 ) , NA ) , desocup30 = ifelse( pia == 1 , as.numeric( vd4002 %in% 2 ) , NA ) ) pnad_design &lt;- update( pnad_design , uf_name = factor( as.numeric( uf ) , levels = c(11L, 12L, 13L, 14L, 15L, 16L, 17L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 31L, 32L, 33L, 35L, 41L, 42L, 43L, 50L, 51L, 52L, 53L) , labels = c(&quot;Rondonia&quot;, &quot;Acre&quot;, &quot;Amazonas&quot;, &quot;Roraima&quot;, &quot;Para&quot;, &quot;Amapa&quot;, &quot;Tocantins&quot;, &quot;Maranhao&quot;, &quot;Piaui&quot;, &quot;Ceara&quot;, &quot;Rio Grande do Norte&quot;, &quot;Paraiba&quot;, &quot;Pernambuco&quot;, &quot;Alagoas&quot;, &quot;Sergipe&quot;, &quot;Bahia&quot;, &quot;Minas Gerais&quot;, &quot;Espirito Santo&quot;, &quot;Rio de Janeiro&quot;, &quot;Sao Paulo&quot;, &quot;Parana&quot;, &quot;Santa Catarina&quot;, &quot;Rio Grande do Sul&quot;, &quot;Mato Grosso do Sul&quot;, &quot;Mato Grosso&quot;, &quot;Goias&quot;, &quot;Distrito Federal&quot;) ) , age_categories = factor( 1 + findInterval( v2009 , seq( 5 , 60 , 5 ) ) ) , male = as.numeric( v2007 == 1 ) , region = substr( uf , 1 , 1 ) , # calculate usual income from main job # (rendimento habitual do trabalho principal) vd4016n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4016 , NA ) , # calculate effective income from main job # (rendimento efetivo do trabalho principal) vd4017n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4017 , NA ) , # calculate usual income from all jobs # (variavel rendimento habitual de todos os trabalhos) vd4019n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4019 , NA ) , # calculate effective income from all jobs # (rendimento efetivo do todos os trabalhos) vd4020n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4020 , NA ) , # determine the potential labor force pea_c = as.numeric( ocup_c == 1 | desocup30 == 1 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pnad_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ uf_name , pnad_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pnad_design ) svyby( ~ one , ~ uf_name , pnad_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ vd4020n , pnad_design , na.rm = TRUE ) svyby( ~ vd4020n , ~ uf_name , pnad_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ age_categories , pnad_design ) svyby( ~ age_categories , ~ uf_name , pnad_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ vd4020n , pnad_design , na.rm = TRUE ) svyby( ~ vd4020n , ~ uf_name , pnad_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ age_categories , pnad_design ) svyby( ~ age_categories , ~ uf_name , pnad_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ vd4020n , pnad_design , 0.5 , na.rm = TRUE ) svyby( ~ vd4020n , ~ uf_name , pnad_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ocup_c , denominator = ~ pea_c , pnad_design , na.rm = TRUE ) Subsetting Restrict the survey design to employed persons: sub_pnad_design &lt;- subset( pnad_design , ocup_c == 1 ) Calculate the mean (average) of this subset: svymean( ~ vd4020n , sub_pnad_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ vd4020n , pnad_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ vd4020n , ~ uf_name , pnad_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pnad_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ vd4020n , pnad_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ vd4020n , pnad_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ vd4020n , pnad_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , pnad_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( vd4020n ~ male , pnad_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + age_categories , pnad_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( vd4020n ~ male + age_categories , pnad_design ) summary( glm_result ) Intermish the author as two-dimensional matryoshka doll Replication Example This example matches statistics and coefficients of variation from Tabela 4092 - Pessoas de 14 anos ou mais de idade, por condição em relação à força de trabalho e condição de ocupação: nationwide_adult_population &lt;- svytotal( ~ pia , pnad_design , na.rm = TRUE ) stopifnot( round( coef( nationwide_adult_population ) / 1000000 , 3 ) == 174.228 ) stopifnot( round( cv( nationwide_adult_population ) / 1000000 , 3 ) == 0 ) nationwide_labor_force &lt;- svytotal( ~ pea_c , pnad_design , na.rm = TRUE ) stopifnot( round( coef( nationwide_labor_force ) / 1000000 , 3 ) == 107.257 ) stopifnot( round( cv( nationwide_labor_force ) * 100 , 1 ) == 0.2 ) nationwide_employed &lt;- svytotal( ~ ocup_c , pnad_design , na.rm = TRUE ) stopifnot( round( coef( nationwide_employed ) / 1000000 , 3 ) == 97.825 ) stopifnot( round( cv( nationwide_employed ) * 100 , 1 ) == 0.2 ) nationwide_unemployed &lt;- svytotal( ~ desocup30 , pnad_design , na.rm = TRUE ) stopifnot( round( coef( nationwide_unemployed ) / 1000000 , 3 ) == 9.432 ) stopifnot( round( cv( nationwide_unemployed ) * 100 , 1 ) == 1.2 ) nationwide_not_in_labor_force &lt;- svytotal( ~ as.numeric( pia &amp; !pea_c ) , pnad_design , na.rm = TRUE ) stopifnot( round( coef( nationwide_not_in_labor_force ) / 1000000 , 3 ) == 66.972 ) stopifnot( round( cv( nationwide_not_in_labor_force ) * 100 , 1 ) == 0.3 ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for PNAD users, this code calculates the gini coefficient on complex sample survey data: library(convey) pnad_design &lt;- convey_prep( pnad_design ) svygini( ~ vd4020n , pnad_design , na.rm = TRUE ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for PNAD users, this code replicates previously-presented examples: library(srvyr) pnad_srvyr_design &lt;- as_survey( pnad_design ) Calculate the mean (average) of a linear variable, overall and by groups: pnad_srvyr_design %&gt;% summarize( mean = survey_mean( vd4020n , na.rm = TRUE ) ) pnad_srvyr_design %&gt;% group_by( uf_name ) %&gt;% summarize( mean = survey_mean( vd4020n , na.rm = TRUE ) ) "],["pesquisa-nacional-de-saude-pns.html", "Pesquisa Nacional de Saude (PNS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " Pesquisa Nacional de Saude (PNS) Brazil’s health survey, measuring medical conditions, risk behaviors, access to and use of care. One consolidated table with one row per individual within each sampled household. A complex sample survey designed to generalize to Brazil’s civilian population. Released at approximately five year intervals starting in 2013. Administered by Instituto Brasileiro de Geografia e Estatistica partnered with the Ministério da Saúde. Please skim before you begin: Conceitos e métodos Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # cheer the ministry! # with each caipirinha, or # fail sex life module Download, Import, Preparation Download and import the dictionary file: dictionary_tf &lt;- tempfile() dictionary_url &lt;- &quot;https://ftp.ibge.gov.br/PNS/2019/Microdados/Documentacao/Dicionario_e_input_20220530.zip&quot; download.file( dictionary_url , dictionary_tf , mode = &#39;wb&#39; ) dictionary_files &lt;- unzip( dictionary_tf , exdir = tempdir() ) sas_fn &lt;- grep( &#39;\\\\.sas$&#39; , dictionary_files , value = TRUE ) sas_lines &lt;- readLines( sas_fn , encoding = &#39;latin1&#39; ) Determine fixed-width file positions from the SAS import script: sas_start &lt;- grep( &#39;@00001&#39; , sas_lines ) sas_end &lt;- grep( &#39;;&#39; , sas_lines ) sas_end &lt;- sas_end[ sas_end &gt; sas_start ][ 1 ] sas_lines &lt;- sas_lines[ seq( sas_start , sas_end - 1 ) ] # remove SAS comments sas_lines &lt;- gsub( &quot;\\\\/\\\\*(.*)&quot; , &quot;&quot; , sas_lines ) # remove tabs, multiple spaces and spaces at the end of each string sas_lines &lt;- gsub( &quot;\\t&quot; , &quot; &quot; , sas_lines ) sas_lines &lt;- gsub( &quot;( +)&quot; , &quot; &quot; , sas_lines ) sas_lines &lt;- gsub( &quot; $&quot; , &quot;&quot; , sas_lines ) sas_df &lt;- read.table( textConnection( sas_lines ) , sep = &#39; &#39; , col.names = c( &#39;position&#39; , &#39;column_name&#39; , &#39;length&#39; ) , header = FALSE ) sas_df[ , &#39;character&#39; ] &lt;- grepl( &#39;\\\\$&#39; , sas_df[ , &#39;length&#39; ] ) sas_df[ , &#39;position&#39; ] &lt;- as.integer( gsub( &quot;\\\\@&quot; , &quot;&quot; , sas_df[ , &#39;position&#39; ] ) ) sas_df[ , &#39;length&#39; ] &lt;- as.integer( gsub( &quot;\\\\$&quot; , &quot;&quot; , sas_df[ , &#39;length&#39; ] ) ) stopifnot( sum( sas_df[ , &#39;length&#39; ] ) == ( sas_df[ nrow( sas_df ) , &#39;position&#39; ] + sas_df[ nrow( sas_df ) , &#39;length&#39; ] - 1 ) ) Download the latest data file: this_tf &lt;- tempfile() this_url &lt;- &quot;https://ftp.ibge.gov.br/PNS/2019/Microdados/Dados/PNS_2019_20220525.zip&quot; download.file( this_url , this_tf , mode = &#39;wb&#39; ) Import the latest data file: library(readr) pns_tbl &lt;- read_fwf( this_tf , fwf_widths( widths = sas_df[ , &#39;length&#39; ] , col_names = sas_df[ , &#39;column_name&#39; ] ) , col_types = paste0( ifelse( sas_df[ , &#39;character&#39; ] , &quot;c&quot; , &quot;d&quot; ) , collapse = &#39;&#39; ) ) pns_df &lt;- data.frame( pns_tbl ) names( pns_df ) &lt;- tolower( names( pns_df ) ) pns_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # pns_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;PNS&quot; , &quot;this_file.rds&quot; ) # saveRDS( pns_df , file = pns_fn , compress = FALSE ) Load the same object: # pns_df &lt;- readRDS( pns_fn ) Survey Design Definition Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) pns_prestratified_design &lt;- svydesign( id = ~ upa_pns , strata = ~v0024 , data = subset( pns_df , !is.na( v0028 ) ) , weights = ~v0028 , nest = TRUE ) popc.types &lt;- data.frame( v00283 = as.character( unique( pns_df[ , &#39;v00283&#39; ] ) ) , Freq = as.numeric( unique( pns_df[ , &#39;v00282&#39; ] ) ) ) popc.types &lt;- popc.types[ order( popc.types[ , &#39;v00283&#39; ] ) , ] pns_design &lt;- postStratify( pns_prestratified_design , strata = ~v00283 , population = popc.types ) Variable Recoding Add new columns to the data set: pns_design &lt;- update( pns_design , medical_insurance = ifelse( i00102 %in% 1:2 , as.numeric( i00102 == 1 ) , NA ) , uf_name = factor( as.numeric( v0001 ) , levels = c(11L, 12L, 13L, 14L, 15L, 16L, 17L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 31L, 32L, 33L, 35L, 41L, 42L, 43L, 50L, 51L, 52L, 53L) , labels = c(&quot;Rondonia&quot;, &quot;Acre&quot;, &quot;Amazonas&quot;, &quot;Roraima&quot;, &quot;Para&quot;, &quot;Amapa&quot;, &quot;Tocantins&quot;, &quot;Maranhao&quot;, &quot;Piaui&quot;, &quot;Ceara&quot;, &quot;Rio Grande do Norte&quot;, &quot;Paraiba&quot;, &quot;Pernambuco&quot;, &quot;Alagoas&quot;, &quot;Sergipe&quot;, &quot;Bahia&quot;, &quot;Minas Gerais&quot;, &quot;Espirito Santo&quot;, &quot;Rio de Janeiro&quot;, &quot;Sao Paulo&quot;, &quot;Parana&quot;, &quot;Santa Catarina&quot;, &quot;Rio Grande do Sul&quot;, &quot;Mato Grosso do Sul&quot;, &quot;Mato Grosso&quot;, &quot;Goias&quot;, &quot;Distrito Federal&quot;) ) , age_categories = factor( 1 + findInterval( c008 , seq( 5 , 90 , 5 ) ) ) , male = as.numeric( v006 == 1 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pns_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ uf_name , pns_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pns_design ) svyby( ~ one , ~ uf_name , pns_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ e01602 , pns_design , na.rm = TRUE ) svyby( ~ e01602 , ~ uf_name , pns_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ c006 , pns_design ) svyby( ~ c006 , ~ uf_name , pns_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ e01602 , pns_design , na.rm = TRUE ) svyby( ~ e01602 , ~ uf_name , pns_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ c006 , pns_design ) svyby( ~ c006 , ~ uf_name , pns_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ e01602 , pns_design , 0.5 , na.rm = TRUE ) svyby( ~ e01602 , ~ uf_name , pns_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ p00104 , denominator = ~ p00404 , pns_design , na.rm = TRUE ) Subsetting Restrict the survey design to individuals that exercise three or more days per week: sub_pns_design &lt;- subset( pns_design , p035 %in% 3:7 ) Calculate the mean (average) of this subset: svymean( ~ e01602 , sub_pns_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ e01602 , pns_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ e01602 , ~ uf_name , pns_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pns_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ e01602 , pns_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ e01602 , pns_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ e01602 , pns_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ medical_insurance , pns_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( e01602 ~ medical_insurance , pns_design ) Perform a chi-squared test of association for survey data: svychisq( ~ medical_insurance + c006 , pns_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( e01602 ~ medical_insurance + c006 , pns_design ) summary( glm_result ) Intermish the author as james howlett Replication Example This example matches Estimando totais of gross monthly income from the official PNSIBGE R package: total_renda &lt;- svytotal( ~ e01602 , pns_design , na.rm = TRUE ) stopifnot( round( coef( total_renda ) , 0 ) == 213227874692 ) stopifnot( round( SE( total_renda ) , 0 ) == 3604489769 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for PNS users, this code replicates previously-presented examples: library(srvyr) pns_srvyr_design &lt;- as_survey( pns_design ) Calculate the mean (average) of a linear variable, overall and by groups: pns_srvyr_design %&gt;% summarize( mean = survey_mean( e01602 , na.rm = TRUE ) ) pns_srvyr_design %&gt;% group_by( uf_name ) %&gt;% summarize( mean = survey_mean( e01602 , na.rm = TRUE ) ) "],["pesquisa-de-orcamentos-familiares-pof.html", "Pesquisa de Orcamentos Familiares (POF) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Poverty and Inequality Estimation with convey   Analysis Examples with srvyr  ", " Pesquisa de Orcamentos Familiares (POF) Brazil’s household budget survey designed to guide major economic indicators like the Contas nacionais. Various tables with one record per sampled household, resident, job, expenditure. A complex sample survey designed to generalize to the civilian population of Brazil. Released at irregular intervals, 2002-2003, 2008-2009, and 2017-2018 microdata available. Administered by the Instituto Brasileiro de Geografia e Estatistica. Please skim before you begin: Pesquisa de Orçamentos Familiares 2017-2018 Perfil das despesas no Brasil Conceitos e métodos This human-composed haiku or a bouquet of artificial intelligence-generated limericks # shopping na praia # roupa, comida, pede # tres havaianas Download, Import, Preparation Download the dictionary files: library(archive) dictionary_tf &lt;- tempfile() dictionary_url &lt;- paste0( &quot;https://ftp.ibge.gov.br/Orcamentos_Familiares/&quot; , &quot;Pesquisa_de_Orcamentos_Familiares_2017_2018/Microdados/Documentacao_20230713.zip&quot; ) download.file( dictionary_url , dictionary_tf , mode = &#39;wb&#39; ) dictionary_files &lt;- archive_extract( dictionary_tf , dir = tempdir() ) Import the household variable dictionary: library(readxl) dictionary_fn &lt;- file.path( tempdir() , &quot;Dicionários de váriaveis.xls&quot; ) domicilio_dictionary_tbl &lt;- read_excel( dictionary_fn , sheet = &quot;Domicílio&quot; , skip = 3 ) domicilio_dictionary_df &lt;- data.frame( domicilio_dictionary_tbl ) names( domicilio_dictionary_df ) &lt;- c( &#39;position&#39; , &#39;length&#39; , &#39;decimals&#39; , &#39;column_name&#39; , &#39;description&#39; , &#39;variable_labels&#39; ) domicilio_dictionary_df[ c( &#39;position&#39; , &#39;length&#39; , &#39;decimals&#39; ) ] &lt;- sapply( domicilio_dictionary_df[ c( &#39;position&#39; , &#39;length&#39; , &#39;decimals&#39; ) ] , as.integer ) domicilio_dictionary_df &lt;- subset( domicilio_dictionary_df , !is.na( position ) ) Import the resident variable dictionary: morador_dictionary_tbl &lt;- read_excel( dictionary_fn , sheet = &quot;Morador&quot; , skip = 3 ) morador_dictionary_df &lt;- data.frame( morador_dictionary_tbl ) names( morador_dictionary_df ) &lt;- c( &#39;position&#39; , &#39;length&#39; , &#39;decimals&#39; , &#39;column_name&#39; , &#39;description&#39; , &#39;variable_labels&#39; ) morador_dictionary_df[ c( &#39;position&#39; , &#39;length&#39; , &#39;decimals&#39; ) ] &lt;- sapply( morador_dictionary_df[ c( &#39;position&#39; , &#39;length&#39; , &#39;decimals&#39; ) ] , as.integer ) morador_dictionary_df &lt;- subset( morador_dictionary_df , !is.na( position ) ) Import the post-stratification totals: post_stratification_fn &lt;- file.path( tempdir() , &quot;Pos_estratos_totais.xlsx&quot; ) post_stratification_tbl &lt;- read_excel( post_stratification_fn , skip = 5 ) post_stratification_df &lt;- data.frame( post_stratification_tbl ) names( post_stratification_df ) &lt;- c( &#39;estrato_pof&#39; , &#39;pos_estrato&#39; , &#39;total_pessoas&#39; , &#39;uf&#39; , &#39;cod_upa&#39; ) Download the full dataset: this_tf &lt;- tempfile() this_url &lt;- paste0( &quot;https://ftp.ibge.gov.br/Orcamentos_Familiares/&quot; , &quot;Pesquisa_de_Orcamentos_Familiares_2017_2018/Microdados/Dados_20230713.zip&quot; ) download.file( this_url , this_tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( this_tf , exdir = tempdir() ) Import the household table: library(readr) domicilio_fn &lt;- grep( &#39;DOMICILIO\\\\.txt$&#39; , unzipped_files , value = TRUE ) domicilio_tbl &lt;- read_fwf( domicilio_fn , fwf_widths( widths = domicilio_dictionary_df[ , &#39;length&#39; ] , col_names = domicilio_dictionary_df[ , &#39;column_name&#39; ] ) ) domicilio_df &lt;- data.frame( domicilio_tbl ) names( domicilio_df ) &lt;- tolower( names( domicilio_df ) ) Import the resident table: morador_fn &lt;- grep( &#39;MORADOR\\\\.txt$&#39; , unzipped_files , value = TRUE ) morador_tbl &lt;- read_fwf( morador_fn , fwf_widths( widths = morador_dictionary_df[ , &#39;length&#39; ] , col_names = morador_dictionary_df[ , &#39;column_name&#39; ] ) ) morador_df &lt;- data.frame( morador_tbl ) names( morador_df ) &lt;- tolower( names( morador_df ) ) Merge one household-level variable and also the post-stratification info onto the person-level table: dom_mor_df &lt;- merge( domicilio_df[ c( &#39;cod_upa&#39; , &#39;num_dom&#39; , &#39;v6199&#39; ) ] , morador_df ) pof_df &lt;- merge( dom_mor_df , post_stratification_df ) stopifnot( nrow( pof_df ) == nrow( morador_df ) ) Save locally   Save the object at any point: # pof_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;POF&quot; , &quot;this_file.rds&quot; ) # saveRDS( pof_df , file = pof_fn , compress = FALSE ) Load the same object: # pof_df &lt;- readRDS( pof_fn ) Survey Design Definition Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) pre_stratified_design &lt;- svydesign( id = ~ cod_upa , strata = ~ estrato_pof , weights = ~ peso , data = pof_df , nest = TRUE ) population_totals &lt;- aggregate( peso_final ~ pos_estrato , data = pof_df , sum ) names( population_totals ) &lt;- c( &#39;pos_estrato&#39; , &#39;Freq&#39; ) pof_design &lt;- postStratify( pre_stratified_design , ~ pos_estrato , population_totals ) Variable Recoding Add new columns to the data set: pof_design &lt;- update( pof_design , one = 1 , food_security = factor( v6199 , levels = 1:4 , labels = c( &#39;food secure&#39; , &#39;mild&#39; , &#39;moderate&#39; , &#39;severe&#39; ) ) , age_categories = factor( 1 + findInterval( v0403 , c( 20 , 25 , 30 , 35 , 45 , 55 , 65 , 75 ) ) , levels = 1:9 , labels = c( &quot;under 20&quot; , &quot;20-24&quot; , &quot;25-29&quot; , &quot;30-34&quot; , &quot;35-44&quot; , &quot;45-54&quot; , &quot;55-64&quot; , &quot;65-74&quot; , &quot;75+&quot; ) ) , sex = factor( v0404 , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , urban = as.numeric( tipo_situacao_reg == 1 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pof_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ sex , pof_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pof_design ) svyby( ~ one , ~ sex , pof_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ renda_total , pof_design ) svyby( ~ renda_total , ~ sex , pof_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ age_categories , pof_design ) svyby( ~ age_categories , ~ sex , pof_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ renda_total , pof_design ) svyby( ~ renda_total , ~ sex , pof_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ age_categories , pof_design ) svyby( ~ age_categories , ~ sex , pof_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ renda_total , pof_design , 0.5 ) svyby( ~ renda_total , ~ sex , pof_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ renda_total , denominator = ~ anos_estudo , pof_design , na.rm = TRUE ) Subsetting Restrict the survey design to credit card holders: sub_pof_design &lt;- subset( pof_design , v0409 &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ renda_total , sub_pof_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ renda_total , pof_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ renda_total , ~ sex , pof_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pof_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ renda_total , pof_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ renda_total , pof_design , deff = TRUE ) # SRS with replacement svymean( ~ renda_total , pof_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ urban , pof_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( renda_total ~ urban , pof_design ) Perform a chi-squared test of association for survey data: svychisq( ~ urban + age_categories , pof_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( renda_total ~ urban + age_categories , pof_design ) summary( glm_result ) Intermish the author as lemuel gulliver Replication Example This example matches the 2017-2018 person-level food security estimates from Tabela 3: person_level_food_security &lt;- svymean( ~ food_security , pof_design , na.rm = TRUE ) stopifnot( all.equal( round( coef( person_level_food_security ) , 2 ) , c( 0.59 , 0.27 , 0.09 , 0.05 ) , check.attributes = FALSE ) ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for POF users, this code calculates the gini coefficient on complex sample survey data: library(convey) pof_design &lt;- convey_prep( pof_design ) svygini( ~ renda_total , pof_design , na.rm = TRUE ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for POF users, this code replicates previously-presented examples: library(srvyr) pof_srvyr_design &lt;- as_survey( pof_design ) Calculate the mean (average) of a linear variable, overall and by groups: pof_srvyr_design %&gt;% summarize( mean = survey_mean( renda_total ) ) pof_srvyr_design %&gt;% group_by( sex ) %&gt;% summarize( mean = survey_mean( renda_total ) ) "],["residential-energy-consumption-survey-recs.html", "Residential Energy Consumption Survey (RECS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " Residential Energy Consumption Survey (RECS) A periodic study conducted to provide detailed information about energy usage in U.S. homes. One table with one row per sampled housing unit. A complex sample survey designed to generalize to U.S. homes occupied as primary residences. Released approximately every five years since 1979. Prepared by the Energy Information Administration, with help from IMG-Crown and RTI International. Please skim before you begin: Annual Energy Outlook 2023 Household Characteristics Technical Documentation Summary This human-composed haiku or a bouquet of artificial intelligence-generated limericks # housing code dogma # even satan ceased sweat since # eighth sin: central air Download, Import, Preparation Download and import the most recent sas file: library(haven) sas_tf &lt;- tempfile() sas_url &lt;- &quot;https://www.eia.gov/consumption/residential/data/2020/sas/recs2020_public_v2.zip&quot; download.file( sas_url , sas_tf , mode = &#39;wb&#39; ) recs_tbl &lt;- read_sas( sas_tf ) recs_df &lt;- data.frame( recs_tbl ) names( recs_df ) &lt;- tolower( names( recs_df ) ) recs_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # recs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;RECS&quot; , &quot;this_file.rds&quot; ) # saveRDS( recs_df , file = recs_fn , compress = FALSE ) Load the same object: # recs_df &lt;- readRDS( recs_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) recs_design &lt;- svrepdesign( data = recs_df , weight = ~ nweight , repweights = &#39;nweight[1-9]+&#39; , type = &#39;JK1&#39; , combined.weights = TRUE , scale = 59 / 60 , mse = TRUE ) Variable Recoding Add new columns to the data set: recs_design &lt;- update( recs_design , main_heating_fuel = factor( fuelheat , levels = c( -2 , 5 , 1 , 2 , 3 , 7 , 99 ) , labels = c( &#39;Not applicable&#39; , &#39;Electricity&#39; , &#39;Natural gas from underground pipes&#39; , &#39;Propane (bottled gas)&#39; , &#39;Fuel oil&#39; , &#39;Wood or pellets&#39; , &#39;Other&#39; ) ) , rooftype = factor( rooftype , levels = c( -2 , 1:6 , 99 ) , labels = c( &#39;Not applicable&#39; , &#39;Ceramic or clay tiles&#39; , &#39;Wood shingles/shakes&#39; , &#39;Metal&#39; , &#39;Slate or synthetic slate&#39; , &#39;Shingles (composition or asphalt)&#39; , &#39;Concrete tiles&#39; , &#39;Other&#39; ) ) , swimpool_binary = ifelse( swimpool %in% 0:1 , swimpool , NA ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( recs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ main_heating_fuel , recs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , recs_design ) svyby( ~ one , ~ main_heating_fuel , recs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ totsqft_en , recs_design ) svyby( ~ totsqft_en , ~ main_heating_fuel , recs_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ rooftype , recs_design ) svyby( ~ rooftype , ~ main_heating_fuel , recs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ totsqft_en , recs_design ) svyby( ~ totsqft_en , ~ main_heating_fuel , recs_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ rooftype , recs_design ) svyby( ~ rooftype , ~ main_heating_fuel , recs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ totsqft_en , recs_design , 0.5 ) svyby( ~ totsqft_en , ~ main_heating_fuel , recs_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ totcsqft , denominator = ~ totsqft_en , recs_design ) Subsetting Restrict the survey design to households that cook three or more hot meals per day: sub_recs_design &lt;- subset( recs_design , nummeal == 1 ) Calculate the mean (average) of this subset: svymean( ~ totsqft_en , sub_recs_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ totsqft_en , recs_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ totsqft_en , ~ main_heating_fuel , recs_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( recs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ totsqft_en , recs_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ totsqft_en , recs_design , deff = TRUE ) # SRS with replacement svymean( ~ totsqft_en , recs_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ swimpool_binary , recs_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( totsqft_en ~ swimpool_binary , recs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ swimpool_binary + rooftype , recs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( totsqft_en ~ swimpool_binary + rooftype , recs_design ) summary( glm_result ) Intermish the author as pleistocene woolly rhinoceros Replication Example This example matches the statistic, standard error, and relative standard error shown on PDF page 8 of Using the microdata file to compute estimates and relative standard errors (RSEs) sas_v1_tf &lt;- tempfile() sas_v1_url &lt;- &quot;https://www.eia.gov/consumption/residential/data/2020/sas/recs2020_public_v1.zip&quot; download.file( sas_v1_url , sas_v1_tf , mode = &#39;wb&#39; ) recs_v1_tbl &lt;- read_sas( sas_v1_tf ) recs_v1_df &lt;- data.frame( recs_v1_tbl ) names( recs_v1_df ) &lt;- tolower( names( recs_v1_df ) ) recs_v1_design &lt;- svrepdesign( data = recs_v1_df , weight = ~ nweight , repweights = &#39;nweight[1-9]+&#39; , type = &#39;JK1&#39; , combined.weights = TRUE , scale = 59 / 60 , mse = TRUE ) recs_v1_design &lt;- update( recs_v1_design , natural_gas_mainspace_heat = as.numeric( fuelheat == 1 ) ) result &lt;- svytotal( ~ natural_gas_mainspace_heat , recs_v1_design ) stopifnot( round( coef( result ) , 0 ) == 56245389 ) stopifnot( round( SE( result ) , 0 ) == 545591 ) stopifnot( round( 100 * SE( result ) / coef( result ) , 2 ) == 0.97 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for RECS users, this code replicates previously-presented examples: library(srvyr) recs_srvyr_design &lt;- as_survey( recs_design ) Calculate the mean (average) of a linear variable, overall and by groups: recs_srvyr_design %&gt;% summarize( mean = survey_mean( totsqft_en ) ) recs_srvyr_design %&gt;% group_by( main_heating_fuel ) %&gt;% summarize( mean = survey_mean( totsqft_en ) ) "],["survey-of-business-owners-sbo.html", "Survey of Business Owners (SBO) Function Definitions Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " Survey of Business Owners (SBO) Before its replacement in 2018 by the Annual Business Survey, nearly every tax-filing sole proprietorship, partnership, and corporation nationwide completed this questionnaire, with 2007 the only microdata year. One table with one row per firm per state per industry, except eight collapsed geographies. A complex sample survey designed to generalize to most firms in the United States, public microdata includes classifiable (non-identifiable) firms, i.e. nearly all businesses but only about half of workers. Released quinquennially from 1972 until 2012 in the Economic Census with no updates expected. Administered by the U.S. Census Bureau. Annual Business Survey now conducted jointly with the National Center for Science and Engineering Statistics within the National Science Foundation. Please skim before you begin: 2007 Survey of Business Owners (SBO) Public Use Microdata Sample (PUMS) Data Users Guide Comparability to the Annual Business Survey (ABS), the Nonemployer Statistics by Demographics (NES-D) series, and the Annual Survey of Entrepreneurs (ASE) At a Glance This human-composed haiku or a bouquet of artificial intelligence-generated limericks # butchers, chandlers, baked # sea shanty, filial pie # call your mom and pop Function Definitions This survey uses a dual design variance estimation technique described in the Data Users Guide. Most users do not need to study these functions carefully. Define functions specific to only this dataset: sbo_MIcombine &lt;- function( x , adjustment = 1.992065 ){ # pull the structure of a variance-covariance matrix variance.shell &lt;- suppressWarnings( vcov( x$var[[1]] ) ) # initiate a function that will overwrite the diagonals. diag.replacement &lt;- function( z ){ diag( variance.shell ) &lt;- coef( z ) variance.shell } # overwrite all the diagonals in the variance this_design object coef.variances &lt;- lapply( x$var , diag.replacement ) # add then divide by ten midpoint &lt;- Reduce( &#39;+&#39; , coef.variances ) / 10 # initiate another function that takes some object, # subtracts the midpoint, squares it, divides by ninety midpoint.var &lt;- function( z ){ 1/10 * ( ( midpoint - z )^2 / 9 ) } # sum up all the differences into a single object variance &lt;- Reduce( &#39;+&#39; , lapply( coef.variances , midpoint.var ) ) # adjust every number with the factor in the user guide adj_var &lt;- adjustment * variance # construct a result that looks like other sbo_MIcombine methods rval &lt;- list( coefficients = coef( x$coef ) , variance = adj_var ) # call it an MIresult class, like other sbo_MIcombine results class( rval ) &lt;- &#39;MIresult&#39; rval } sbo_with &lt;- function ( this_design , expr , ... ){ pf &lt;- parent.frame() expr &lt;- substitute( expr ) expr$design &lt;- as.name(&quot;.design&quot;) # this pulls in means, medians, totals, etc. # notice it uses this_design$coef results &lt;- eval( expr , list( .design = this_design$coef ) ) # this is used to calculate the variance, adjusted variance, standard error # notice it uses the this_design$var object variances &lt;- lapply( this_design$var$designs , function( .design ){ eval( expr , list( .design = .design ) , enclos = pf ) } ) # combine both results.. rval &lt;- list( coef = results , var = variances ) # ..into a brand new object class class( rval ) &lt;- &#39;imputationResultList&#39; rval } sbo_subset &lt;- function( x , ... ){ # subset the survey object coef.sub &lt;- subset( x$coef , ... ) # replicate `var.sub` so it&#39;s got all the same attributes as `x$var` var.sub &lt;- x$var # but then overwrite the `designs` attribute with a subset var.sub$designs &lt;- lapply( x$var$designs , subset , ... ) # now re-create the `sbosvyimputationList` just as before.. sub.svy &lt;- list( coef = coef.sub , var = var.sub ) # ..and give it the same class sub.svy$call &lt;- sys.call(-1) sub.svy } sbo_update &lt;- function( x , ... ){ # update the survey object that&#39;s going to be used for # means, medians, totals, etc. coef.upd &lt;- update( x$coef , ... ) # replicate `var.upd` so it&#39;s got all the same attributes as `x$var` var.upd &lt;- x$var # but then overwrite the `designs` attribute with an update var.upd$designs &lt;- lapply( x$var$designs , update , ... ) # now re-create the `sbosvyimputationList` just as before upd.svy &lt;- list( coef = coef.upd , var = var.upd ) upd.svy } sbo_degf &lt;- function( x ) degf( x$coef ) Download, Import, Preparation Download and import the file containing records for both coefficient estimates and variance estimation: library(httr) library(readr) tf &lt;- tempfile() this_url &lt;- &quot;https://www2.census.gov/programs-surveys/sbo/datasets/2007/pums_csv.zip&quot; GET( this_url , write_disk( tf ) , progress() ) sbo_tbl &lt;- read_csv( tf ) sbo_df &lt;- data.frame( sbo_tbl ) names( sbo_df ) &lt;- tolower( names( sbo_df ) ) sbo_df[ , &#39;one&#39; ] &lt;- 1 Calculate the weights used for variance estimation: sbo_df[ , &#39;newwgt&#39; ] &lt;- 10 * sbo_df[ , &#39;tabwgt&#39; ] * sqrt( 1 - 1 / sbo_df[ , &#39;tabwgt&#39; ] ) Add business ownership percentages for both gender and ethnicity: # replace percent missings with zeroes for( i in 1:4 ) sbo_df[ is.na( sbo_df[ , paste0( &#39;pct&#39; , i ) ] ) , paste0( &#39;pct&#39; , i ) ] &lt;- 0 # sum up ownership ethnicity and gender sbo_df[ , &#39;hispanic_pct&#39; ] &lt;- sbo_df[ , &#39;nonhispanic_pct&#39; ] &lt;- 0 sbo_df[ , &#39;male_pct&#39; ] &lt;- sbo_df[ , &#39;female_pct&#39; ] &lt;- 0 # loop through the first four owners&#39; ethnicity and sex variables for( i in 1:4 ) { sbo_df[ sbo_df[ , paste0( &#39;eth&#39; , i ) ] %in% &#39;H&#39; , &#39;hispanic_pct&#39; ] &lt;- sbo_df[ sbo_df[ , paste0( &#39;eth&#39; , i ) ] %in% &#39;H&#39; , &#39;hispanic_pct&#39; ] + sbo_df[ sbo_df[ , paste0( &#39;eth&#39; , i ) ] %in% &#39;H&#39; , paste0( &#39;pct&#39; , i ) ] sbo_df[ sbo_df[ , paste0( &#39;eth&#39; , i ) ] %in% &#39;N&#39; , &#39;nonhispanic_pct&#39; ] &lt;- sbo_df[ sbo_df[ , paste0( &#39;eth&#39; , i ) ] %in% &#39;N&#39; , &#39;nonhispanic_pct&#39; ] + sbo_df[ sbo_df[ , paste0( &#39;eth&#39; , i ) ] %in% &#39;N&#39; , paste0( &#39;pct&#39; , i ) ] sbo_df[ sbo_df[ , paste0( &#39;sex&#39; , i ) ] %in% &#39;M&#39; , &#39;male_pct&#39; ] &lt;- sbo_df[ sbo_df[ , paste0( &#39;sex&#39; , i ) ] %in% &#39;M&#39; , &#39;male_pct&#39; ] + sbo_df[ sbo_df[ , paste0( &#39;sex&#39; , i ) ] %in% &#39;M&#39; , paste0( &#39;pct&#39; , i ) ] sbo_df[ sbo_df[ , paste0( &#39;sex&#39; , i ) ] %in% &#39;F&#39; , &#39;female_pct&#39; ] &lt;- sbo_df[ sbo_df[ , paste0( &#39;sex&#39; , i ) ] %in% &#39;F&#39; , &#39;female_pct&#39; ] + sbo_df[ sbo_df[ , paste0( &#39;sex&#39; , i ) ] %in% &#39;F&#39; , paste0( &#39;pct&#39; , i ) ] } Save locally   Save the object at any point: # sbo_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;SBO&quot; , &quot;this_file.rds&quot; ) # saveRDS( sbo_df , file = sbo_fn , compress = FALSE ) Load the same object: # sbo_df &lt;- readRDS( sbo_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) # break random groups into ten separate data.frame objects within a list var_list &lt;- NULL for( i in 1:10 ) { var_list &lt;- c( var_list , list( subset( sbo_df , rg == i ) ) ) } sbo_coef &lt;- svydesign( id = ~ 1 , weight = ~ tabwgt , data = sbo_df ) sbo_var &lt;- svydesign( id = ~ 1 , weight = ~ newwgt , data = imputationList( var_list ) ) sbo_design &lt;- list( coef = sbo_coef , var = sbo_var ) class( sbo_design ) &lt;- &#39;sbosvyimputationList&#39; Variable Recoding Add new columns to the data set: sbo_design &lt;- sbo_update( sbo_design , established_before_2000 = ifelse( established %in% c( &#39;0&#39; , &#39;A&#39; ) , NA , as.numeric( established &lt; 4 ) ) , healthins = factor( healthins , levels = 1:2 , labels = c( &quot;offered health insurance&quot; , &quot;did not offer health insurance&quot; ) ) , hispanic_ownership = factor( ifelse( hispanic_pct == nonhispanic_pct , 2 , ifelse( hispanic_pct &gt; nonhispanic_pct , 1 , ifelse( nonhispanic_pct &gt; hispanic_pct , 3 , NA ) ) ) , levels = 1:3 , labels = c( &#39;hispanic&#39; , &#39;equally hisp/non&#39; , &#39;non-hispanic&#39; ) ) , gender_ownership = factor( ifelse( male_pct == female_pct , 2 , ifelse( male_pct &gt; female_pct , 1 , ifelse( female_pct &gt; male_pct , 3 , NA ) ) ) , levels = 1:3 , labels = c( &#39;male&#39; , &#39;equally male/female&#39; , &#39;female&#39; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svyby( ~ one , ~ one , unwtd.count ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ one , ~ gender_ownership , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svytotal( ~ one ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ one , ~ gender_ownership , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ gender_ownership , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svymean( ~ n07_employer , na.rm = TRUE ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ n07_employer , ~ gender_ownership , svymean , na.rm = TRUE ) ) ) Calculate the sum of a linear variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svytotal( ~ receipts_noisy ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ gender_ownership , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svytotal( ~ n07_employer , na.rm = TRUE ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ n07_employer , ~ gender_ownership , svytotal , na.rm = TRUE ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svyquantile( ~ receipts_noisy , 0.5 , se = TRUE ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ gender_ownership , svyquantile , 0.5 , se = TRUE , ci = TRUE ) ) ) Estimate a ratio: sbo_MIcombine( sbo_with( sbo_design , svyratio( numerator = ~ receipts_noisy , denominator = ~ employment_noisy ) ) ) Subsetting Restrict the survey design to jointly owned by husband and wife: sub_sbo_design &lt;- sbo_subset( sbo_design , husbwife %in% 1:3 ) Calculate the mean (average) of this subset: sbo_MIcombine( sbo_with( sub_sbo_design , svymean( ~ receipts_noisy ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ gender_ownership , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: sbo_degf( sbo_design ) Calculate the complex sample survey-adjusted variance of any statistic: sbo_MIcombine( sbo_with( sbo_design , svyvar( ~ receipts_noisy ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy , deff = TRUE ) ) ) # SRS with replacement sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # # sbo_MIsvyciprop( ~ established_before_2000 , sbo_design , # method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: # # sbo_MIsvyttest( receipts_noisy ~ established_before_2000 , sbo_design ) Perform a chi-squared test of association for survey data: # # sbo_MIsvychisq( ~ established_before_2000 + n07_employer , sbo_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svyglm( receipts_noisy ~ established_before_2000 + n07_employer ) ) ) glm_result Intermish the author as liberace Replication Example This example matches the statistics and relative standard errors from three Appendix B columns: hispanic_receipts_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ hispanic_ownership , svytotal ) ) ) hispanic_payroll_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svyby( ~ payroll_noisy , ~ hispanic_ownership , svytotal ) ) ) hispanic_employment_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svyby( ~ employment_noisy , ~ hispanic_ownership , svytotal ) ) ) Estimates at the U.S. Level using the PUMS Tables for: stopifnot( round( coef( hispanic_receipts_result )[ &#39;hispanic&#39; ] , 0 ) == 350763923 ) stopifnot( round( coef( hispanic_receipts_result )[ &#39;equally hisp/non&#39; ] , 0 ) == 56166354 ) stopifnot( round( coef( hispanic_receipts_result )[ &#39;non-hispanic&#39; ] , 0 ) == 10540609303 ) stopifnot( round( coef( hispanic_payroll_result )[ &#39;hispanic&#39; ] , 0 ) == 54367702 ) stopifnot( round( coef( hispanic_payroll_result )[ &#39;equally hisp/non&#39; ] , 0 ) == 11083148 ) stopifnot( round( coef( hispanic_payroll_result )[ &#39;non-hispanic&#39; ] , 0 ) == 1875353228 ) stopifnot( round( coef( hispanic_employment_result )[ &#39;hispanic&#39; ] , 0 ) == 2026406 ) stopifnot( round( coef( hispanic_employment_result )[ &#39;equally hisp/non&#39; ] , 0 ) == 400152 ) stopifnot( round( coef( hispanic_employment_result )[ &#39;non-hispanic&#39; ] , 0 ) == 56889606 ) Relative Standard Errors of Estimates at the U.S. Level using the PUMS Tables for: stopifnot( round( cv( hispanic_receipts_result )[ &#39;hispanic&#39; ] , 2 ) == 0.02 ) stopifnot( round( cv( hispanic_receipts_result )[ &#39;equally hisp/non&#39; ] , 2 ) == 0.06 ) stopifnot( round( cv( hispanic_receipts_result )[ &#39;non-hispanic&#39; ] , 2 ) == 0 ) stopifnot( round( cv( hispanic_payroll_result )[ &#39;hispanic&#39; ] , 2 ) == 0.01 ) stopifnot( round( cv( hispanic_payroll_result )[ &#39;equally hisp/non&#39; ] , 2 ) == 0.06 ) stopifnot( round( cv( hispanic_payroll_result )[ &#39;non-hispanic&#39; ] , 2 ) == 0 ) stopifnot( round( cv( hispanic_employment_result )[ &#39;hispanic&#39; ] , 2 ) == 0.01 ) stopifnot( round( cv( hispanic_employment_result )[ &#39;equally hisp/non&#39; ] , 2 ) == 0.05 ) stopifnot( round( cv( hispanic_employment_result )[ &#39;non-hispanic&#39; ] , 2 ) == 0 ) "],["survey-of-consumer-finances-scf.html", "Survey of Consumer Finances (SCF) Function Definitions Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Poverty and Inequality Estimation with convey  ", " Survey of Consumer Finances (SCF) A comprehensive survey of household wealth, the U.S. central bank studies net worth across the country by asking about both active and passive income, mortgages, pensions, credit card debt, even car leases. Five implicates, each containing one row per sampled household to account for statistical uncertainty. A complex sample survey designed to generalize to the civilian non-institutional U.S. population. Released triennially since 1989. Administered by the Board of Governors of the Federal Reserve System. Please skim before you begin: Measuring Income and Wealth at the Top Using Administrative and Survey Data Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # incomes, assets, debts # high net worth oversample # pig bank laproscope Function Definitions This survey uses a multiply-imputed variance estimation technique described in the 2004 Codebook. Most users do not need to study this function carefully. Define a function specific to only this dataset: scf_MIcombine &lt;- function (results, variances, call = sys.call(), df.complete = Inf, ...) { m &lt;- length(results) oldcall &lt;- attr(results, &quot;call&quot;) if (missing(variances)) { variances &lt;- suppressWarnings(lapply(results, vcov)) results &lt;- lapply(results, coef) } vbar &lt;- variances[[1]] cbar &lt;- results[[1]] for (i in 2:m) { cbar &lt;- cbar + results[[i]] # MODIFICATION: # vbar &lt;- vbar + variances[[i]] } cbar &lt;- cbar/m # MODIFICATION: # vbar &lt;- vbar/m evar &lt;- var(do.call(&quot;rbind&quot;, results)) r &lt;- (1 + 1/m) * evar/vbar df &lt;- (m - 1) * (1 + 1/r)^2 if (is.matrix(df)) df &lt;- diag(df) if (is.finite(df.complete)) { dfobs &lt;- ((df.complete + 1)/(df.complete + 3)) * df.complete * vbar/(vbar + evar) if (is.matrix(dfobs)) dfobs &lt;- diag(dfobs) df &lt;- 1/(1/dfobs + 1/df) } if (is.matrix(r)) r &lt;- diag(r) rval &lt;- list(coefficients = cbar, variance = vbar + evar * (m + 1)/m, call = c(oldcall, call), nimp = m, df = df, missinfo = (r + 2/(df + 3))/(r + 1)) class(rval) &lt;- &quot;MIresult&quot; rval } Download, Import, Preparation Define a function to download and import each stata file: library(haven) scf_dta_import &lt;- function( this_url ){ this_tf &lt;- tempfile() download.file( this_url , this_tf , mode = &#39;wb&#39; ) this_tbl &lt;- read_dta( this_tf ) this_df &lt;- data.frame( this_tbl ) file.remove( this_tf ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } Download and import the full, summary extract, and replicate weights tables: scf_df &lt;- scf_dta_import( &quot;https://www.federalreserve.gov/econres/files/scf2022s.zip&quot; ) ext_df &lt;- scf_dta_import( &quot;https://www.federalreserve.gov/econres/files/scfp2022s.zip&quot; ) scf_rw_df &lt;- scf_dta_import( &quot;https://www.federalreserve.gov/econres/files/scf2022rw1s.zip&quot; ) Confirm both the full public data and the summary extract contain five records per family: stopifnot( nrow( scf_df ) == nrow( scf_rw_df ) * 5 ) stopifnot( nrow( scf_df ) == nrow( ext_df ) ) Confirm only the primary economic unit and the five implicate identifiers overlap: stopifnot( all( sort( intersect( names( scf_df ) , names( ext_df ) ) ) == c( &#39;y1&#39; , &#39;yy1&#39; ) ) ) stopifnot( all( sort( intersect( names( scf_df ) , names( scf_rw_df ) ) ) == c( &#39;y1&#39; , &#39;yy1&#39; ) ) ) stopifnot( all( sort( intersect( names( ext_df ) , names( scf_rw_df ) ) ) == c( &#39;y1&#39; , &#39;yy1&#39; ) ) ) Remove the implicate identifier from the replicate weights table, add a column of fives for weighting: scf_rw_df[ , &#39;y1&#39; ] &lt;- NULL scf_df[ , &#39;five&#39; ] &lt;- 5 Save locally   Save the object at any point: # scf_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;SCF&quot; , &quot;this_file.rds&quot; ) # saveRDS( scf_df , file = scf_fn , compress = FALSE ) Load the same object: # scf_df &lt;- readRDS( scf_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: Break the main table into five different implicates based on the final character of the column y1: library(stringr) s1_df &lt;- scf_df[ str_sub( scf_df[ , &#39;y1&#39; ] , -1 , -1 ) == 1 , ] s2_df &lt;- scf_df[ str_sub( scf_df[ , &#39;y1&#39; ] , -1 , -1 ) == 2 , ] s3_df &lt;- scf_df[ str_sub( scf_df[ , &#39;y1&#39; ] , -1 , -1 ) == 3 , ] s4_df &lt;- scf_df[ str_sub( scf_df[ , &#39;y1&#39; ] , -1 , -1 ) == 4 , ] s5_df &lt;- scf_df[ str_sub( scf_df[ , &#39;y1&#39; ] , -1 , -1 ) == 5 , ] Combine these into a single list, then merge each implicate with the summary extract: scf_imp &lt;- list( s1_df , s2_df , s3_df , s4_df , s5_df ) scf_list &lt;- lapply( scf_imp , merge , ext_df ) Replace all missing values in the replicate weights table with zeroes, multiply the replicate weights by the multiplication factor, then only keep the unique identifier and the final (combined) replicate weights: scf_rw_df[ is.na( scf_rw_df ) ] &lt;- 0 scf_rw_df[ , paste0( &#39;wgt&#39; , 1:999 ) ] &lt;- scf_rw_df[ , paste0( &#39;wt1b&#39; , 1:999 ) ] * scf_rw_df[ , paste0( &#39;mm&#39; , 1:999 ) ] scf_rw_df &lt;- scf_rw_df[ , c( &#39;yy1&#39; , paste0( &#39;wgt&#39; , 1:999 ) ) ] Sort both the five implicates and also the replicate weights table by the unique identifier: scf_list &lt;- lapply( scf_list , function( w ) w[ order( w[ , &#39;yy1&#39; ] ) , ] ) scf_rw_df &lt;- scf_rw_df[ order( scf_rw_df[ , &#39;yy1&#39; ] ) , ] Define the design: library(survey) library(mitools) scf_design &lt;- svrepdesign( weights = ~wgt , repweights = scf_rw_df[ , -1 ] , data = imputationList( scf_list ) , scale = 1 , rscales = rep( 1 / 998 , 999 ) , mse = FALSE , type = &quot;other&quot; , combined.weights = TRUE ) Variable Recoding Add new columns to the data set: scf_design &lt;- update( scf_design , hhsex = factor( hhsex , levels = 1:2 , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , married = as.numeric( married == 1 ) , edcl = factor( edcl , levels = 1:4 , labels = c( &quot;less than high school&quot; , &quot;high school or GED&quot; , &quot;some college&quot; , &quot;college degree&quot; ) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: scf_MIcombine( with( scf_design , svyby( ~ five , ~ five , unwtd.count ) ) ) scf_MIcombine( with( scf_design , svyby( ~ five , ~ hhsex , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: scf_MIcombine( with( scf_design , svytotal( ~ five ) ) ) scf_MIcombine( with( scf_design , svyby( ~ five , ~ hhsex , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: scf_MIcombine( with( scf_design , svymean( ~ networth ) ) ) scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: scf_MIcombine( with( scf_design , svymean( ~ edcl ) ) ) scf_MIcombine( with( scf_design , svyby( ~ edcl , ~ hhsex , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: scf_MIcombine( with( scf_design , svytotal( ~ networth ) ) ) scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: scf_MIcombine( with( scf_design , svytotal( ~ edcl ) ) ) scf_MIcombine( with( scf_design , svyby( ~ edcl , ~ hhsex , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: scf_MIcombine( with( scf_design , svyquantile( ~ networth , 0.5 , se = TRUE , interval.type = &#39;quantile&#39; ) ) ) scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svyquantile , 0.5 , se = TRUE , interval.type = &#39;quantile&#39; , ci = TRUE ) ) ) Estimate a ratio: scf_MIcombine( with( scf_design , svyratio( numerator = ~ income , denominator = ~ networth ) ) ) Subsetting Restrict the survey design to labor force participants: sub_scf_design &lt;- subset( scf_design , lf == 1 ) Calculate the mean (average) of this subset: scf_MIcombine( with( sub_scf_design , svymean( ~ networth ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- scf_MIcombine( with( scf_design , svymean( ~ networth ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( scf_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: scf_MIcombine( with( scf_design , svyvar( ~ networth ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement scf_MIcombine( with( scf_design , svymean( ~ networth , deff = TRUE ) ) ) # SRS with replacement scf_MIcombine( with( scf_design , svymean( ~ networth , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ married , scf_design , # method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( networth ~ married , scf_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ married + edcl , scf_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- scf_MIcombine( with( scf_design , svyglm( networth ~ married + edcl ) ) ) summary( glm_result ) Intermish Replication Example This example matches the “Table 4” tab’s cell Y6 of the Excel Based on Public Data: mean_net_worth &lt;- scf_MIcombine( with( scf_design , svymean( ~ networth ) ) ) stopifnot( round( coef( mean_net_worth ) / 1000 , 1 ) == 1059.5 ) This example comes within $500 of the standard error of mean net worth from Table 2 of the Federal Reserve Bulletin, displaying the minor differences between the Internal Data and Public Data: stopifnot( abs( 23.2 - round( SE( mean_net_worth ) / 1000 , 1 ) ) &lt; 0.5 ) This example matches the “Table 4” tab’s cells X6 of the Excel Based on Public Data: # compute quantile with all five implicates stacked (not the recommended technique) fake_design &lt;- svydesign( ~ 1 , data = ext_df[ c( &#39;networth&#39; , &#39;wgt&#39; ) ] , weights = ~ wgt ) median_net_worth_incorrect_errors &lt;- svyquantile( ~ networth , fake_design , 0.5 ) stopifnot( round( coef( median_net_worth_incorrect_errors ) / 1000 , 2 ) == 192.7 ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for SCF users, this code calculates the gini coefficient on complex sample survey data: library(convey) scf_design$designs &lt;- lapply( scf_design$designs , convey_prep ) scf_MIcombine( with( scf_design , svygini( ~ networth ) ) ) "],["survey-of-income-and-program-participation-sipp.html", "Survey of Income and Program Participation (SIPP) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Poverty and Inequality Estimation with convey   Analysis Examples with srvyr  ", " Survey of Income and Program Participation (SIPP) The primary longitudinal assessment of income fluctuation, labor force participation, social programs. Annual tables with one record per month per person per sampled household, time period weights. A complex sample generalizing to the U.S. civilian non-institutionalized across varying time periods. Multi-year panels since 1980s, its current and now permanent four year rotation beginning in 2018. Administered and financed by the US Census Bureau. Please skim before you begin: 2022 Survey of Income and Program Participation Users’ Guide 2022 Data User Notes This human-composed haiku or a bouquet of artificial intelligence-generated limericks # federal programs # poverty oversample # monthly dynamics Download, Import, Preparation Determine which variables from the main table to import: variables_to_keep &lt;- c( &#39;ssuid&#39; , &#39;pnum&#39; , &#39;monthcode&#39; , &#39;spanel&#39; , &#39;swave&#39; , &#39;erelrpe&#39; , &#39;tlivqtr&#39; , &#39;wpfinwgt&#39; , &#39;rmesr&#39; , &#39;thcyincpov&#39; , &#39;tfcyincpov&#39; , &#39;tehc_st&#39; , &#39;rhicovann&#39; , &#39;rfpov&#39; , &#39;thnetworth&#39; , &#39;tftotinc&#39; ) Download and import the latest main file: library(httr) library(data.table) main_tf &lt;- tempfile() main_url &lt;- paste0( &quot;https://www2.census.gov/programs-surveys/sipp/&quot; , &quot;data/datasets/2022/pu2022_csv.zip&quot; ) GET( main_url , write_disk( main_tf ) , progress() ) main_csv &lt;- unzip( main_tf , exdir = tempdir() ) sipp_main_dt &lt;- fread( main_csv , sep = &quot;|&quot; , select = toupper( variables_to_keep ) ) sipp_main_df &lt;- data.frame( sipp_main_dt ) names( sipp_main_df ) &lt;- tolower( names( sipp_main_df ) ) Download and import the appropriate replicate weights file: rw_tf &lt;- tempfile() rw_url &lt;- paste0( &quot;https://www2.census.gov/programs-surveys/sipp/&quot; , &quot;data/datasets/2022/rw2022_csv.zip&quot; ) GET( rw_url , write_disk( rw_tf ) , progress() ) rw_csv &lt;- unzip( rw_tf , exdir = tempdir() ) sipp_rw_dt &lt;- fread( rw_csv , sep = &quot;|&quot; ) sipp_rw_df &lt;- data.frame( sipp_rw_dt ) names( sipp_rw_df ) &lt;- tolower( names( sipp_rw_df ) ) Limit both files to December records for a point-in-time estimate, then merge: sipp_df &lt;- merge( sipp_main_df[ sipp_main_df[ , &#39;monthcode&#39; ] %in% 12 , ] , sipp_rw_df[ sipp_rw_df[ , &#39;monthcode&#39; ] %in% 12 , ] , by = c( &#39;ssuid&#39; , &#39;pnum&#39; , &#39;monthcode&#39; , &#39;spanel&#39; , &#39;swave&#39; ) ) stopifnot( nrow( sipp_df ) == sum( sipp_rw_df[ , &#39;monthcode&#39; ] %in% 12 ) ) Save locally   Save the object at any point: # sipp_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;SIPP&quot; , &quot;this_file.rds&quot; ) # saveRDS( sipp_df , file = sipp_fn , compress = FALSE ) Load the same object: # sipp_df &lt;- readRDS( sipp_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) sipp_design &lt;- svrepdesign( data = sipp_df , weights = ~ wpfinwgt , repweights = &quot;repwgt([1-9]+)&quot; , type = &quot;Fay&quot; , rho = 0.5 ) Variable Recoding Add new columns to the data set: rmesr_values &lt;- c( &quot;With a job entire month, worked all weeks&quot;, &quot;With a job all month, absent from work without pay 1+ weeks, absence not due to layoff&quot;, &quot;With a job all month, absent from work without pay 1+ weeks, absence due to layoff&quot;, &quot;With a job at least 1 but not all weeks, no time on layoff and no time looking for work&quot;, &quot;With a job at least 1 but not all weeks, some weeks on layoff or looking for work&quot;, &quot;No job all month, on layoff or looking for work all weeks&quot;, &quot;No job all month, at least one but not all weeks on layoff or looking for work&quot;, &quot;No job all month, no time on layoff and no time looking for work&quot; ) sipp_design &lt;- update( sipp_design , one = 1 , employment_status = factor( rmesr , levels = 1:8 , labels = rmesr_values ) , household_below_poverty = as.numeric( thcyincpov &lt; 1 ) , family_below_poverty = as.numeric( tfcyincpov &lt; 1 ) , state_name = factor( as.numeric( tehc_st ) , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L, 60L, 61L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;Puerto Rico&quot;, &quot;Foreign Country&quot;) ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( sipp_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , sipp_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , sipp_design ) svyby( ~ one , ~ state_name , sipp_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ tftotinc , sipp_design , na.rm = TRUE ) svyby( ~ tftotinc , ~ state_name , sipp_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ employment_status , sipp_design , na.rm = TRUE ) svyby( ~ employment_status , ~ state_name , sipp_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ tftotinc , sipp_design , na.rm = TRUE ) svyby( ~ tftotinc , ~ state_name , sipp_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ employment_status , sipp_design , na.rm = TRUE ) svyby( ~ employment_status , ~ state_name , sipp_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ tftotinc , sipp_design , 0.5 , na.rm = TRUE ) svyby( ~ tftotinc , ~ state_name , sipp_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ tftotinc , denominator = ~ rfpov , sipp_design , na.rm = TRUE ) Subsetting Restrict the survey design to individuals ever covered by health insurance during the year: sub_sipp_design &lt;- subset( sipp_design , rhicovann == 1 ) Calculate the mean (average) of this subset: svymean( ~ tftotinc , sub_sipp_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ tftotinc , sipp_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ tftotinc , ~ state_name , sipp_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( sipp_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ tftotinc , sipp_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ tftotinc , sipp_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ tftotinc , sipp_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ family_below_poverty , sipp_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( tftotinc ~ family_below_poverty , sipp_design ) Perform a chi-squared test of association for survey data: svychisq( ~ family_below_poverty + employment_status , sipp_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( tftotinc ~ family_below_poverty + employment_status , sipp_design ) summary( glm_result ) Intermish gustave doré’s les saltimbanques Replication Example This example matches statistics and standard errors from the Wealth and Asset Ownership for Households, by Type of Asset and Selected Characteristics: 2021: Restrict the design to permanent residence-based householders to match the count in Table 4: sipp_household_design &lt;- subset( sipp_design , erelrpe %in% 1:2 &amp; tlivqtr %in% 1:2 ) stopifnot( round( coef( svytotal( ~ one , sipp_household_design ) ) / 1000 , -2 ) == 132700 ) Compute Household Net Worth distribution and standard errors across the Total row of Tables 4 and 4A: sipp_household_design &lt;- update( sipp_household_design , thnetworth_category = factor( findInterval( thnetworth , c( 1 , 5000 , 10000 , 25000 , 50000 , 100000 , 250000 , 500000 ) ) , levels = 0:8 , labels = c( &quot;Zero or Negative&quot; , &quot;$1 to $4,999&quot; , &quot;$5,000 to $9,999&quot; , &quot;$10,000 to $24,999&quot; , &quot;$25,000 to $49,999&quot; , &quot;$50,000 to $99,999&quot; , &quot;$100,000 to $249,999&quot; , &quot;$250,000 to $499,999&quot; , &quot;$500,000 or over&quot; ) ) ) results &lt;- svymean( ~ thnetworth_category , sipp_household_design ) stopifnot( all.equal( as.numeric( round( coef( results ) * 100 , 1 ) ) , c( 11.3 , 6.9 , 3.5 , 5.9 , 6.4 , 8.2 , 15.3 , 13.9 , 28.7 ) ) ) stopifnot( all.equal( as.numeric( round( SE( results ) * 100 , 1 ) ) , c( 0.3 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.3 , 0.3 , 0.3 ) ) ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for SIPP users, this code calculates the gini coefficient on complex sample survey data: library(convey) sipp_design &lt;- convey_prep( sipp_design ) svygini( ~ tftotinc , sipp_design , na.rm = TRUE ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for SIPP users, this code replicates previously-presented examples: library(srvyr) sipp_srvyr_design &lt;- as_survey( sipp_design ) Calculate the mean (average) of a linear variable, overall and by groups: sipp_srvyr_design %&gt;% summarize( mean = survey_mean( tftotinc , na.rm = TRUE ) ) sipp_srvyr_design %&gt;% group_by( state_name ) %&gt;% summarize( mean = survey_mean( tftotinc , na.rm = TRUE ) ) "],["social-security-public-use-data-files-ssa.html", "Social Security Public-Use Data Files (SSA) Download, Import, Preparation Analysis Examples with base R   Intermish Replication Example Analysis Examples with dplyr   Analysis Examples with data.table   Analysis Examples with duckdb  ", " Social Security Public-Use Data Files (SSA) Microdata from administrative sources like the Master Beneficiary Record, Supplemental Security Record. Tables contain either one record per person or one record per person per year. A systematic sample of either social security number holders (most americans) or program recipients (current beneficiaries). Multiply 1% samples by 100 to get weighted statistics, 5% samples by 20. No expected release timeline. Released by the Office of Research, Evaluation, and Statistics, US Social Security Administration. Please skim before you begin: The 2006 Earnings Public-Use Microdata File: An Introduction Comparing Earnings Estimates from the 2006 Public-Use File and the Annual Statistical Supplement This human-composed haiku or a bouquet of artificial intelligence-generated limericks # annual earnings. # for pensioner payouts, see # the &#39;04 extract Download, Import, Preparation Download and import the 1951-2006 one percent files with one record per person and per person-year: library(haven) library(httr) tf &lt;- tempfile() ssa_url &lt;- &quot;https://www.ssa.gov/policy/docs/microdata/epuf/epuf2006_sas_files.zip&quot; GET( ssa_url , write_disk( tf ) ) ssa_files &lt;- unzip( tf , exdir = tempdir() ) ssa_fn &lt;- grep( &#39;demographic&#39; , ssa_files , value = TRUE ) annual_fn &lt;- grep( &#39;annual&#39; , ssa_files , value = TRUE ) ssa_tbl &lt;- read_sas( ssa_fn ) annual_tbl &lt;- read_sas( annual_fn ) ssa_df &lt;- data.frame( ssa_tbl ) annual_df &lt;- data.frame( annual_tbl ) names( ssa_df ) &lt;- tolower( names( ssa_df ) ) names( annual_df ) &lt;- tolower( names( annual_df ) ) Sum up 1951-1952 and 1953-2006 earnings, and also 1953-2006 credits, copying the naming convention: summed_earnings_5152 &lt;- with( subset( annual_df , year_earn %in% 1951:1952 ) , aggregate( annual_earnings , list( id ) , sum ) ) names( summed_earnings_5152 ) &lt;- c( &#39;id&#39; , &#39;tot_cov_earn5152&#39; ) summed_earnings_5306 &lt;- with( subset( annual_df , year_earn &gt; 1952 ) , aggregate( annual_earnings , list( id ) , sum ) ) names( summed_earnings_5306 ) &lt;- c( &#39;id&#39; , &#39;tot_cov_earn5306&#39; ) summed_quarters_5306 &lt;- with( subset( annual_df , year_earn &gt; 1952 ) , aggregate( annual_qtrs , list( id ) , sum ) ) names( summed_quarters_5306 ) &lt;- c( &#39;id&#39; , &#39;qc5306&#39; ) Isolate a single year of earnings: earnings_2006 &lt;- annual_df[ annual_df[ , &#39;year_earn&#39; ] == 2006 , c( &#39;id&#39; , &#39;annual_earnings&#39; ) ] names( earnings_2006 ) &lt;- c( &#39;id&#39; , &#39;tot_cov_earn06&#39; ) Merge each new column on to the person-level table, then add zeroes to person-years without earnings: stopifnot( all( !is.na( ssa_df ) ) ) before_nrow &lt;- nrow( ssa_df ) ssa_df &lt;- merge( ssa_df , summed_earnings_5152 , all.x = TRUE ) ssa_df &lt;- merge( ssa_df , summed_earnings_5306 , all.x = TRUE ) ssa_df &lt;- merge( ssa_df , summed_quarters_5306 , all.x = TRUE ) ssa_df &lt;- merge( ssa_df , earnings_2006 , all.x = TRUE ) ssa_df[ is.na( ssa_df ) ] &lt;- 0 stopifnot( nrow( ssa_df ) == before_nrow ) Save locally   Save the object at any point: # ssa_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;SSA&quot; , &quot;this_file.rds&quot; ) # saveRDS( ssa_df , file = ssa_fn , compress = FALSE ) Load the same object: # ssa_df &lt;- readRDS( ssa_fn ) Variable Recoding Add new columns to the data set: ssa_df &lt;- transform( ssa_df , decade_of_birth = floor( yob / 10 ) * 10 , sex = factor( sex , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , tot_cov_earn3706 = ( tot_cov_earn3750 + tot_cov_earn5152 + tot_cov_earn5306 ) , qc3706 = ( qc3750 + qc5152 + qc5306 ) , any_earnings_2006 = ( tot_cov_earn06 &gt; 0 ) , earnings_periods = factor( ifelse( ( tot_cov_earn5152 + tot_cov_earn5306 &gt; 0 ) &amp; tot_cov_earn3750 &gt; 0 , 1 , ifelse( tot_cov_earn5152 &gt; 0 | tot_cov_earn5306 &gt; 0 , 2 , ifelse( tot_cov_earn3750 &gt; 0 , 3 , 4 ) ) ) , levels = 1:4 , labels = c( &#39;Earnings in both periods&#39; , &#39;Earnings during 1951-2006 only&#39; , &#39;Earnings during 1937-1950 only&#39; , &#39;No earnings&#39; ) ) ) Analysis Examples with base R   Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( ssa_df ) table( ssa_df[ , &quot;sex&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( ssa_df[ , &quot;decade_of_birth&quot; ] ) ) prop.table( table( ssa_df[ , c( &quot;decade_of_birth&quot; , &quot;sex&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , 0.5 ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to individuals with at least forty lifetime credits: sub_ssa_df &lt;- subset( ssa_df , qc3706 &gt;= 40 ) Calculate the mean (average) of this subset: mean( sub_ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( tot_cov_earn3706 ~ any_earnings_2006 , ssa_df ) Perform a chi-squared test of association: this_table &lt;- table( ssa_df[ , c( &quot;any_earnings_2006&quot; , &quot;decade_of_birth&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( tot_cov_earn3706 ~ any_earnings_2006 + decade_of_birth , data = ssa_df ) summary( glm_result ) Intermish xerox buttocks with peach emoji Replication Example This example matches statistics in The 2006 Earnings Public-Use Microdata File: An Introduction: Chart 5. Percentage distribution of individuals in EPUF, by capped Social Security taxable earnings status: chart_five_results &lt;- prop.table( table( ssa_df[ , &#39;earnings_periods&#39; ] ) ) chart_five_results &lt;- round( 100 * chart_five_results ) stopifnot( chart_five_results[ &#39;Earnings in both periods&#39; ] == 16 ) stopifnot( chart_five_results[ &#39;Earnings during 1951-2006 only&#39; ] == 55 ) stopifnot( chart_five_results[ &#39;Earnings during 1937-1950 only&#39; ] == 4 ) stopifnot( chart_five_results[ &#39;No earnings&#39; ] == 25 ) Table 4. Average and median Social Security taxable earnings in EPUF, by sex, 1951–2006 (in dollars): nonzero_2006_earners &lt;- ssa_df[ ssa_df[ , &#39;tot_cov_earn06&#39; ] &gt; 0 , &#39;tot_cov_earn06&#39; ] stopifnot( round( mean( nonzero_2006_earners ) , 0 ) == 30953 ) stopifnot( round( quantile( nonzero_2006_earners )[ 3 ] , 0 ) == 24000 ) Table A1. Number and percentage distribution of individuals with Social Security taxable earnings records in EPUF, by sex, 1951–2006: nonzero_2006_earners &lt;- ssa_df[ ssa_df[ , &#39;tot_cov_earn06&#39; ] &gt; 0 , ] stopifnot( round( mean( nonzero_2006_earners[ , &#39;tot_cov_earn06&#39; ] ) , 0 ) == 30953 ) stopifnot( round( quantile( nonzero_2006_earners[ , &#39;tot_cov_earn06&#39; ] )[ 3 ] , 0 ) == 24000 ) This example matches statistics in Comparing Earnings Estimates from the 2006 Earnings Public-Use File and the Annual Statistical Supplement: Table 4. Comparing Supplement and EPUF estimates: Number of all, male, and female workers with any earnings during the year, 1951–2006: stopifnot( round( nrow( nonzero_2006_earners ) * 100 , -3 ) == 156280000 ) earners_in_2006_by_sex &lt;- table( nonzero_2006_earners[ , &#39;sex&#39; ] ) * 100 stopifnot( round( earners_in_2006_by_sex[ &#39;male&#39; ] , -3 ) == 81576000 ) stopifnot( round( earners_in_2006_by_sex[ &#39;female&#39; ] , -3 ) == 74681000 ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for SSA users, this code replicates previously-presented examples: library(dplyr) ssa_tbl &lt;- as_tibble( ssa_df ) Calculate the mean (average) of a linear variable, overall and by groups: ssa_tbl %&gt;% summarize( mean = mean( tot_cov_earn3706 ) ) ssa_tbl %&gt;% group_by( sex ) %&gt;% summarize( mean = mean( tot_cov_earn3706 ) ) Analysis Examples with data.table   The R data.table library provides a high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed. data.table offers concise syntax: fast to type, fast to read, fast speed, memory efficiency, a careful API lifecycle management, an active community, and a rich set of features. This vignette details the available features. As a starting point for SSA users, this code replicates previously-presented examples: library(data.table) ssa_dt &lt;- data.table( ssa_df ) Calculate the mean (average) of a linear variable, overall and by groups: ssa_dt[ , mean( tot_cov_earn3706 ) ] ssa_dt[ , mean( tot_cov_earn3706 ) , by = sex ] Analysis Examples with duckdb   The R duckdb library provides an embedded analytical data management system with support for the Structured Query Language (SQL). duckdb offers a simple, feature-rich, fast, and free SQL OLAP management system. This vignette details the available features. As a starting point for SSA users, this code replicates previously-presented examples: library(duckdb) con &lt;- dbConnect( duckdb::duckdb() , dbdir = &#39;my-db.duckdb&#39; ) dbWriteTable( con , &#39;ssa&#39; , ssa_df ) Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( con , &#39;SELECT AVG( tot_cov_earn3706 ) FROM ssa&#39; ) dbGetQuery( con , &#39;SELECT sex , AVG( tot_cov_earn3706 ) FROM ssa GROUP BY sex&#39; ) "],["trends-in-international-mathematics-and-science-study-timss.html", "Trends in International Mathematics and Science Study (TIMSS) Function Definitions Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " Trends in International Mathematics and Science Study (TIMSS) A comparative study of student achievement in math and science across more than 50 nations. Grade-specific tables with one record per school, student, teacher, plus files containing student achievement, home background, student-teacher linkage, and within-country scoring reliability. A complex survey generalizing to fourth- and eighth-grade populations of participating countries. Released quadrennially since 1995. Funded by the International Association for the Evaluation of Educational Achievement, run at BC. Please skim before you begin: TIMSS 2019 User Guide for the International Database, 2nd Edition Methods and Procedures: TIMSS 2019 Technical Report This human-composed haiku or a bouquet of artificial intelligence-generated limericks # brando for stella, # gump&#39;s jenny, rock&#39;s adrian, # students toward math test Function Definitions This survey uses a multiply-imputed variance estimation technique described in Methods Chapter 14. Most users do not need to study this function carefully. Define a function specific to only this dataset: timss_MIcombine &lt;- function (results, variances, call = sys.call(), df.complete = Inf, ...) { m &lt;- length(results) oldcall &lt;- attr(results, &quot;call&quot;) if (missing(variances)) { variances &lt;- suppressWarnings(lapply(results, vcov)) results &lt;- lapply(results, coef) } vbar &lt;- variances[[1]] cbar &lt;- results[[1]] for (i in 2:m) { cbar &lt;- cbar + results[[i]] vbar &lt;- vbar + variances[[i]] } cbar &lt;- cbar/m vbar &lt;- vbar/m # MODIFICATION # evar &lt;- var(do.call(&quot;rbind&quot;, results)) evar &lt;- sum( ( unlist( results ) - cbar )^2 / 4 ) r &lt;- (1 + 1/m) * evar/vbar df &lt;- (m - 1) * (1 + 1/r)^2 if (is.matrix(df)) df &lt;- diag(df) if (is.finite(df.complete)) { dfobs &lt;- ((df.complete + 1)/(df.complete + 3)) * df.complete * vbar/(vbar + evar) if (is.matrix(dfobs)) dfobs &lt;- diag(dfobs) df &lt;- 1/(1/dfobs + 1/df) } if (is.matrix(r)) r &lt;- diag(r) rval &lt;- list(coefficients = cbar, variance = vbar + evar * (m + 1)/m, call = c(oldcall, call), nimp = m, df = df, missinfo = (r + 2/(df + 3))/(r + 1)) class(rval) &lt;- &quot;MIresult&quot; rval } Download, Import, Preparation Download and unzip the 2019 fourth grade international database: library(httr) tf &lt;- tempfile() this_url &lt;- &quot;https://timss2019.org/international-database/downloads/T19_G4_SPSS%20Data.zip&quot; GET( this_url , write_disk( tf ) , progress() ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) Import and stack each of the student context data files for Albania through Canada: library(haven) # limit unzipped files to those starting with `asg` followed by three letters followed by `m7` asg_fns &lt;- unzipped_files[ grepl( &#39;^asg[a-z][a-z][a-z]m7&#39; , basename( unzipped_files ) ) ] # further limit asg files to the first ten countries countries_thru_canada &lt;- c(&quot;alb&quot;, &quot;arm&quot;, &quot;aus&quot;, &quot;aut&quot;, &quot;aze&quot;, &quot;bhr&quot;, &quot;bfl&quot;, &quot;bih&quot;, &quot;bgr&quot;, &quot;can&quot;) fns_thru_canada &lt;- paste0( paste0( &#39;^asg&#39; , countries_thru_canada , &#39;m7&#39; ) , collapse = &quot;|&quot; ) asg_alb_can_fns &lt;- asg_fns[ grepl( fns_thru_canada , basename( asg_fns ) ) ] timss_df &lt;- NULL for( spss_fn in asg_alb_can_fns ){ this_tbl &lt;- read_spss( spss_fn ) this_tbl &lt;- zap_labels( this_tbl ) this_df &lt;- data.frame( this_tbl ) names( this_df ) &lt;- tolower( names( this_df ) ) timss_df &lt;- rbind( timss_df , this_df ) } # order the data.frame by unique student id timss_df &lt;- timss_df[ with( timss_df , order( idcntry , idstud ) ) , ] Save locally   Save the object at any point: # timss_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;TIMSS&quot; , &quot;this_file.rds&quot; ) # saveRDS( timss_df , file = timss_fn , compress = FALSE ) Load the same object: # timss_df &lt;- readRDS( timss_fn ) Survey Design Definition Construct a multiply-imputed, complex sample survey design: From among possibly plausible values, determine all columns that are multiply-imputed plausible values: # identify all columns ending with `01` thru `05` ppv &lt;- grep( &quot;(.*)0[1-5]$&quot; , names( timss_df ) , value = TRUE ) # remove those ending digits ppv_prefix &lt;- gsub( &quot;0[1-5]$&quot; , &quot;&quot; , ppv ) # identify each of the possibilities with exactly five matches (five implicates) pv &lt;- names( table( ppv_prefix )[ table( ppv_prefix ) == 5 ] ) # identify each of the `01` thru `05` plausible value columns pv_columns &lt;- grep( paste0( &quot;^&quot; , pv , &quot;0[1-5]$&quot; , collapse = &quot;|&quot; ) , names( timss_df ) , value = TRUE ) Extract those multiply-imputed columns into a separate data.frame, then remove them from the source: pv_wide_df &lt;- timss_df[ c( &#39;idcntry&#39; , &#39;idstud&#39; , pv_columns ) ] timss_df[ pv_columns ] &lt;- NULL Reshape these columns from one record per student to one record per student per implicate: pv_long_df &lt;- reshape( pv_wide_df , varying = lapply( paste0( pv , &#39;0&#39; ) , paste0 , 1:5 ) , direction = &#39;long&#39; , timevar = &#39;implicate&#39; , idvar = c( &#39;idcntry&#39; , &#39;idstud&#39; ) ) names( pv_long_df ) &lt;- gsub( &quot;01$&quot; , &quot;&quot; , names( pv_long_df ) ) Merge the columns from the source data.frame onto the one record per student per implicate data.frame: timss_long_df &lt;- merge( timss_df , pv_long_df ) timss_long_df &lt;- timss_long_df[ with( timss_long_df , order( idcntry , idstud ) ) , ] stopifnot( nrow( timss_long_df ) == nrow( pv_long_df ) ) stopifnot( nrow( timss_long_df ) / 5 == nrow( timss_df ) ) Divide the five plausible value implicates into a list with five data.frames based on the implicate number: timss_list &lt;- split( timss_long_df , timss_long_df[ , &#39;implicate&#39; ] ) Construct a replicate weights table following the estimation technique described in Methods Chapter 14: weights_df &lt;- timss_df[ c( &#39;jkrep&#39; , &#39;jkzone&#39; ) ] for( j in 1:75 ){ for( i in 0:1 ){ weights_df[ weights_df[ , &#39;jkzone&#39; ] != j , paste0( &#39;rw&#39; , i , j ) ] &lt;- 1 weights_df[ weights_df[ , &#39;jkzone&#39; ] == j , paste0( &#39;rw&#39; , i , j ) ] &lt;- 2 * ( weights_df[ weights_df[ , &#39;jkzone&#39; ] == j , &#39;jkrep&#39; ] == i ) } } weights_df[ c( &#39;jkrep&#39; , &#39;jkzone&#39; ) ] &lt;- NULL Define the design: library(survey) library(mitools) timss_design &lt;- svrepdesign( weights = ~totwgt , repweights = weights_df , data = imputationList( timss_list ) , type = &quot;other&quot; , scale = 0.5 , rscales = rep( 1 , 150 ) , combined.weights = FALSE , mse = TRUE ) Variable Recoding Add new columns to the data set: timss_design &lt;- update( timss_design , one = 1 , countries_thru_canada = factor( as.numeric( idcntry ) , levels = c(8L, 51L, 36L, 40L, 31L, 48L, 956L, 70L, 100L, 124L) , labels = c(&quot;Albania&quot;, &quot;Armenia&quot;, &quot;Australia&quot;, &quot;Austria&quot;, &quot;Azerbaijan&quot;, &quot;Bahrain&quot;, &quot;Belgium (Flemish)&quot;, &quot;Bosnia and Herzegovina&quot;, &quot;Bulgaria&quot;, &quot;Canada&quot;) ) , sex = factor( asbg01 , levels = 1:2 , labels = c( &quot;female&quot; , &quot;male&quot; ) ) , born_in_country = ifelse( asbg07 %in% 1:2 , as.numeric( asbg07 == 1 ) , NA ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: timss_MIcombine( with( timss_design , svyby( ~ one , ~ one , unwtd.count ) ) ) timss_MIcombine( with( timss_design , svyby( ~ one , ~ sex , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: timss_MIcombine( with( timss_design , svytotal( ~ one ) ) ) timss_MIcombine( with( timss_design , svyby( ~ one , ~ sex , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: timss_MIcombine( with( timss_design , svymean( ~ asmmat , na.rm = TRUE ) ) ) timss_MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svymean , na.rm = TRUE ) ) ) Calculate the distribution of a categorical variable, overall and by groups: timss_MIcombine( with( timss_design , svymean( ~ countries_thru_canada ) ) ) timss_MIcombine( with( timss_design , svyby( ~ countries_thru_canada , ~ sex , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: timss_MIcombine( with( timss_design , svytotal( ~ asmmat , na.rm = TRUE ) ) ) timss_MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svytotal , na.rm = TRUE ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: timss_MIcombine( with( timss_design , svytotal( ~ countries_thru_canada ) ) ) timss_MIcombine( with( timss_design , svyby( ~ countries_thru_canada , ~ sex , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: timss_MIcombine( with( timss_design , svyquantile( ~ asmmat , 0.5 , se = TRUE , na.rm = TRUE ) ) ) timss_MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svyquantile , 0.5 , se = TRUE , ci = TRUE , na.rm = TRUE ) ) ) Estimate a ratio: timss_MIcombine( with( timss_design , svyratio( numerator = ~ asssci , denominator = ~ asmmat ) ) ) Subsetting Restrict the survey design to Australia, Austria, Azerbaijan, Belgium (French): sub_timss_design &lt;- subset( timss_design , idcntry %in% c( 36 , 40 , 31 , 956 ) ) Calculate the mean (average) of this subset: timss_MIcombine( with( sub_timss_design , svymean( ~ asmmat , na.rm = TRUE ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- timss_MIcombine( with( timss_design , svymean( ~ asmmat , na.rm = TRUE ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- timss_MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svymean , na.rm = TRUE ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( timss_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: timss_MIcombine( with( timss_design , svyvar( ~ asmmat , na.rm = TRUE ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement timss_MIcombine( with( timss_design , svymean( ~ asmmat , na.rm = TRUE , deff = TRUE ) ) ) # SRS with replacement timss_MIcombine( with( timss_design , svymean( ~ asmmat , na.rm = TRUE , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ born_in_country , timss_design , # method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( asmmat ~ born_in_country , timss_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ born_in_country + countries_thru_canada , timss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- timss_MIcombine( with( timss_design , svyglm( asmmat ~ born_in_country + countries_thru_canada ) ) ) summary( glm_result ) Intermish the author as silverback gorilla attempting selfie Replication Example This example matches the mean proficiency and standard error of the Australia row of the Summary Statistics and Standard Errors for Proficiency in Overall Mathematics-Grade 4 table from the Appendix 14A: Summary Statistics and Standard Errors for Proficiency in Grade 4 Mathematics: australia_design &lt;- subset( timss_design , countries_thru_canada %in% &quot;Australia&quot; ) stopifnot( nrow( australia_design ) == 5890 ) result &lt;- timss_MIcombine( with( australia_design , svymean( ~ asmmat ) ) ) stopifnot( round( coef( result ) , 3 ) == 515.880 ) stopifnot( round( SE( result ) , 3 ) == 2.776 ) This example matches the jackknife sampling, imputation, and total variances of the same row: australia_fn &lt;- unzipped_files[ grepl( &#39;asgaus&#39; , basename( unzipped_files ) ) ] australia_tbl &lt;- read_spss( australia_fn ) australia_tbl &lt;- zap_labels( australia_tbl ) australia_df &lt;- data.frame( australia_tbl ) names( australia_df ) &lt;- tolower( names( australia_df ) ) estimate &lt;- mean( c( with( australia_df , weighted.mean( asmmat01 , totwgt ) ) , with( australia_df , weighted.mean( asmmat02 , totwgt ) ) , with( australia_df , weighted.mean( asmmat03 , totwgt ) ) , with( australia_df , weighted.mean( asmmat04 , totwgt ) ) , with( australia_df , weighted.mean( asmmat05 , totwgt ) ) ) ) stopifnot( round( estimate , 3 ) == 515.880 ) for( k in 1:5 ){ this_variance &lt;- 0 for( j in 1:75 ){ for( i in 0:1 ){ this_variance &lt;- this_variance + ( weighted.mean( australia_df[ , paste0( &#39;asmmat0&#39; , k ) ] , ifelse( j == australia_df[ , &#39;jkzone&#39; ] , australia_df[ , &#39;totwgt&#39; ] * 2 * ( australia_df[ , &#39;jkrep&#39; ] == i ) , australia_df[ , &#39;totwgt&#39; ] ) ) - weighted.mean( australia_df[ , paste0( &#39;asmmat0&#39; , k ) ] , australia_df[ , &#39;totwgt&#39; ] ) )^2 } } assign( paste0( &#39;v&#39; , k ) , this_variance * 0.5 ) } sampling_variance &lt;- mean( c( v1 , v2 , v3 , v4 , v5 ) ) stopifnot( round( sampling_variance , 3 ) == 7.397 ) imputation_variance &lt;- ( 6 / 5 ) * ( ( ( with( australia_df , weighted.mean( asmmat01 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asmmat02 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asmmat03 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asmmat04 , totwgt ) ) - estimate )^2 / 4 ) + ( ( with( australia_df , weighted.mean( asmmat05 , totwgt ) ) - estimate )^2 / 4 ) ) stopifnot( round( imputation_variance , 3 ) == 0.309 ) stopifnot( round( sampling_variance + imputation_variance , 3 ) == 7.706 ) "],["youth-risk-behavior-surveillance-system-yrbss.html", "Youth Risk Behavior Surveillance System (YRBSS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example Analysis Examples with srvyr  ", " Youth Risk Behavior Surveillance System (YRBSS) The high school edition of the Behavioral Risk Factor Surveillance System (BRFSS). One table with one row per sampled youth respondent. A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States. Released biennially since 1993. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: Methodology of the Youth Risk Behavior Surveillance System Wikipedia Entry This human-composed haiku or a bouquet of artificial intelligence-generated limericks # maladolescence # epidemiology # sex, drugs, rock and roll Download, Import, Preparation Load the SAScii library to interpret a SAS input program, and also re-arrange the SAS input program: library(SAScii) sas_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/2019XXH-SAS-Input-Program.sas&quot; sas_text &lt;- tolower( readLines( sas_url ) ) # find the (out of numerical order) # `site` location variable&#39;s position # within the SAS input program site_location &lt;- which( sas_text == &#39;@1 site $3.&#39; ) # find the start field&#39;s position # within the SAS input program input_location &lt;- which( sas_text == &quot;input&quot; ) # create a vector from 1 to the length of the text file sas_length &lt;- seq( length( sas_text ) ) # remove the site_location sas_length &lt;- sas_length[ -site_location ] # re-insert the site variable&#39;s location # immediately after the starting position sas_reorder &lt;- c( sas_length[ seq( input_location ) ] , site_location , sas_length[ seq( input_location + 1 , length( sas_length ) ) ] ) # re-order the sas text file sas_text &lt;- sas_text[ sas_reorder ] sas_tf &lt;- tempfile() writeLines( sas_text , sas_tf ) Download and import the national file: dat_tf &lt;- tempfile() dat_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/XXH2019_YRBS_Data.dat&quot; download.file( dat_url , dat_tf , mode = &#39;wb&#39; ) yrbss_df &lt;- read.SAScii( dat_tf , sas_tf ) names( yrbss_df ) &lt;- tolower( names( yrbss_df ) ) yrbss_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # yrbss_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; , &quot;this_file.rds&quot; ) # saveRDS( yrbss_df , file = yrbss_fn , compress = FALSE ) Load the same object: # yrbss_df &lt;- readRDS( yrbss_fn ) Survey Design Definition Construct a complex sample survey design: library(survey) yrbss_design &lt;- svydesign( ~ psu , strata = ~ stratum , data = yrbss_df , weights = ~ weight , nest = TRUE ) Variable Recoding Add new columns to the data set: yrbss_design &lt;- update( yrbss_design , q2 = q2 , never_rarely_wore_seat_belt = as.numeric( qn8 == 1 ) , ever_used_marijuana = as.numeric( qn45 == 1 ) , tried_to_quit_tobacco_past_year = as.numeric( q39 == 2 ) , used_tobacco_past_year = as.numeric( q39 &gt; 1 ) ) Analysis Examples with the survey library   Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( yrbss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , yrbss_design ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ bmipct , yrbss_design , 0.5 , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ tried_to_quit_tobacco_past_year , denominator = ~ used_tobacco_past_year , yrbss_design , na.rm = TRUE ) Subsetting Restrict the survey design to youths who ever drank alcohol: sub_yrbss_design &lt;- subset( yrbss_design , qn40 &gt; 1 ) Calculate the mean (average) of this subset: svymean( ~ bmipct , sub_yrbss_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( yrbss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ bmipct , yrbss_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( bmipct ~ never_rarely_wore_seat_belt , yrbss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( bmipct ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) summary( glm_result ) Intermish the death of piggy and other plot points as ancient art Replication Example This example matches statistics, standard errors, and confidence intervals from the “never/rarely wore seat belt” row of PDF page 29 of this CDC analysis software document: unwtd_count_result &lt;- unwtd.count( ~ never_rarely_wore_seat_belt , yrbss_design ) stopifnot( coef( unwtd_count_result ) == 11149 ) wtd_n_result &lt;- svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_seat_belt ) ) ) stopifnot( round( coef( wtd_n_result ) , 0 ) == 12132 ) share_result &lt;- svymean( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE ) stopifnot( round( coef( share_result ) , 4 ) == .0654 ) stopifnot( round( SE( share_result ) , 4 ) == .0065 ) ci_result &lt;- svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE , method = &quot;beta&quot; ) stopifnot( round( confint( ci_result )[1] , 4 ) == 0.0529 ) stopifnot( round( confint( ci_result )[2] , 2 ) == 0.08 ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for YRBSS users, this code replicates previously-presented examples: library(srvyr) yrbss_srvyr_design &lt;- as_survey( yrbss_design ) Calculate the mean (average) of a linear variable, overall and by groups: yrbss_srvyr_design %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) yrbss_srvyr_design %&gt;% group_by( ever_used_marijuana ) %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
