[["index.html", "Analyze Survey Data for Free Public Microdata, Easy to Type Website", " Analyze Survey Data for Free Public Microdata, Easy to Type Website Please ask questions about this book at stackoverflow.com with the R and survey tags. This book replaces my archived blog, prior code, and the no longer maintained lodown package. A work of R is never finished, only abandoned. - Anthony Damico "],["american-community-survey-acs.html", "American Community Survey (ACS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Poverty and Inequality Estimation with convey   Analysis Examples with srvyr   Replication Example", " American Community Survey (ACS) The US Census Bureau’s annual replacement for the long-form decennial census. Two tables per state, the first with one row per household and the second with one row per individual. The civilian population of the United States. Released annually since 2005. Administered and financed by the US Census Bureau. Please skim before you begin: Guidance for Data Users Wikipedia Entry This poem # one percent sample # the decennial census # in miniature Download, Import, Preparation Download and import the Alabama household file: (switch sas_hal to sas_hak for Alaska or sas_hus for the entire country) library(haven) tf_household &lt;- tempfile() this_url_household &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_hal.zip&quot; download.file( this_url_household , tf_household , mode = &#39;wb&#39; ) unzipped_files_household &lt;- unzip( tf_household , exdir = tempdir() ) acs_sas_household &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_household , value = TRUE ) acs_df_household &lt;- read_sas( acs_sas_household ) names( acs_df_household ) &lt;- tolower( names( acs_df_household ) ) Download and import the Alabama person file: (switch sas_pal to sas_pak for Alaska or sas_pus for the entire country) tf_person &lt;- tempfile() this_url_person &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_pal.zip&quot; download.file( this_url_person , tf_person , mode = &#39;wb&#39; ) unzipped_files_person &lt;- unzip( tf_person , exdir = tempdir() ) acs_sas_person &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_person , value = TRUE ) acs_df_person &lt;- read_sas( acs_sas_person ) names( acs_df_person ) &lt;- tolower( names( acs_df_person ) ) Remove overlapping column and merge household + person files: acs_df_household[ , &#39;rt&#39; ] &lt;- NULL acs_df_person[ , &#39;rt&#39; ] &lt;- NULL acs_df &lt;- merge( acs_df_household , acs_df_person ) stopifnot( nrow( acs_df ) == nrow( acs_df_person ) ) acs_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # acs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ACS&quot; , &quot;this_file.rds&quot; ) # saveRDS( acs_df , file = acs_fn , compress = FALSE ) Load the same object: # acs_df &lt;- readRDS( acs_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) acs_design &lt;- svrepdesign( weight = ~pwgtp , repweights = &#39;pwgtp[0-9]+&#39; , scale = 4 / 80 , rscales = rep( 1 , 80 ) , mse = TRUE , type = &#39;JK1&#39; , data = acs_df ) Variable Recoding Add new columns to the data set: acs_design &lt;- update( acs_design , state_name = factor( as.numeric( st ) , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L, 72L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;Puerto Rico&quot;) ) , cit = factor( cit , levels = 1:5 , labels = c( &#39;born in the u.s.&#39; , &#39;born in the territories&#39; , &#39;born abroad to american parents&#39; , &#39;naturalized citizen&#39; , &#39;non-citizen&#39; ) ) , poverty_level = as.numeric( povpip ) , married = as.numeric( mar %in% 1 ) , sex = factor( sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( acs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cit , acs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , acs_design ) svyby( ~ one , ~ cit , acs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ poverty_level , acs_design , 0.5 , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ssip , denominator = ~ pincp , acs_design , na.rm = TRUE ) Subsetting Restrict the survey design to senior citizens: sub_acs_design &lt;- subset( acs_design , agep &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ poverty_level , sub_acs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ poverty_level , acs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( acs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ poverty_level , acs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ married , acs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( poverty_level ~ married , acs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ married + sex , acs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( poverty_level ~ married + sex , acs_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for ACS users, this code calculates the gini coefficient on complex sample survey data: library(convey) acs_design &lt;- convey_prep( acs_design ) svygini( ~ hincp , acs_design , na.rm = TRUE ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ACS users, this code replicates previously-presented examples: library(srvyr) acs_srvyr_design &lt;- as_survey( acs_design ) Calculate the mean (average) of a linear variable, overall and by groups: acs_srvyr_design %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) acs_srvyr_design %&gt;% group_by( cit ) %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) Replication Example This example matches statistics, standard errors, and margin of errors from the 2021 PUMS tallies: Match the sum of the weights: stopifnot( round( coef( svytotal( ~ one , acs_design ) ) , 0 ) == 5039877 ) Compute the population by age: pums_estimate &lt;- c(288139L, 299245L, 336727L, 334606L, 327102L, 635004L, 641405L, 615709L, 335431L, 341926L, 538367L, 265742L, 80474L) pums_standard_error &lt;- c(2727L, 5368L, 6067L, 4082L, 4485L, 5716L, 4420L, 3706L, 4836L, 5100L, 2158L, 3363L, 3186L) pums_margin_of_error &lt;- c(4486L, 8830L, 9981L, 6715L, 7378L, 9402L, 7271L, 6096L, 7956L, 8389L, 3550L, 5532L, 5240L) results &lt;- svytotal( ~ as.numeric( agep %in% 0:4 ) + as.numeric( agep %in% 5:9 ) + as.numeric( agep %in% 10:14 ) + as.numeric( agep %in% 15:19 ) + as.numeric( agep %in% 20:24 ) + as.numeric( agep %in% 25:34 ) + as.numeric( agep %in% 35:44 ) + as.numeric( agep %in% 45:54 ) + as.numeric( agep %in% 55:59 ) + as.numeric( agep %in% 60:64 ) + as.numeric( agep %in% 65:74 ) + as.numeric( agep %in% 75:84 ) + as.numeric( agep %in% 85:100 ) , acs_design ) stopifnot( all( round( coef( results ) , 0 ) == pums_estimate ) ) stopifnot( all( round( SE( results ) , 0 ) == pums_standard_error ) ) stopifnot( all( round( SE( results ) * 1.645 , 0 ) == pums_margin_of_error ) ) "],["american-time-use-survey-atus.html", "American Time Use Survey (ATUS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " American Time Use Survey (ATUS) Sampled individuals write down everything they do for a single twenty-four hour period, in ten minute intervals. Time use data allows for the study of uncompensated work like cooking, chores, childcare. Many tables with structures described in the user guide, linkable to the Current Population Survey. A complex survey generalizing to person-hours among civilian non-institutional americans aged 15+. Released annually since 2003. Administered by the Bureau of Labor Statistics. Please skim before you begin: American Time Use Survey User’s Guide Wikipedia Entry This poem # don&#39;t judge me bruno # eat one hour, sleep the rest # it&#39;s my lazy day Download, Import, Preparation Define a function to download, unzip, and import each comma-separated value dat file: atus_csv_import &lt;- function( this_url ){ this_tf &lt;- tempfile() download.file( this_url , this_tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( this_tf , exdir = tempdir() ) this_dat &lt;- grep( &#39;\\\\.dat$&#39; , unzipped_files , value = TRUE ) this_df &lt;- read.csv( this_dat ) file.remove( c( this_tf , unzipped_files ) ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } Download and import the activity, respondent, roster, and weights tables: act_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusact-2021.zip&quot; ) resp_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusresp-2021.zip&quot; ) rost_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusrost-2021.zip&quot; ) wgts_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atuswgts-2021.zip&quot; ) Specify which variables to keep in each of the data.frame objects: act_df &lt;- act_df[ c( &#39;tucaseid&#39; , &#39;tutier1code&#39; , &#39;tutier2code&#39; , &#39;tuactdur24&#39; ) ] resp_df &lt;- resp_df[ c( &#39;tucaseid&#39; , &#39;tufinlwgt&#39; , &#39;tulineno&#39; ) ] rost_df &lt;- rost_df[ , c( &#39;tucaseid&#39; , &#39;tulineno&#39; , &#39;teage&#39; , &#39;tesex&#39; ) ] Distribute travel-related activities (tutier1code == 18 from the lexicon) based on their second tier code: act_df[ act_df[ , &#39;tutier1code&#39; ] == 18 &amp; act_df[ , &#39;tutier2code&#39; ] == 99 , &#39;tutier1code&#39; ] &lt;- 50 act_df[ act_df[ , &#39;tutier1code&#39; ] == 18 , &#39;tutier1code&#39; ] &lt;- act_df[ act_df[ , &#39;tutier1code&#39; ] == 18 , &#39;tutier2code&#39; ] Sum up all durations at the (respondent x major activity category)-level: act_long_df &lt;- aggregate( tuactdur24 ~ tucaseid + tutier1code , data = act_df , sum ) act_wide_df &lt;- reshape( act_long_df , idvar = &#39;tucaseid&#39; , timevar = &#39;tutier1code&#39; , direction = &#39;wide&#39; ) # for individuals not engaging in an activity category, replace missings with zero minutes act_wide_df[ is.na( act_wide_df ) ] &lt;- 0 # for all columns except the respondent identifier, convert minutes to hours act_wide_df[ , -1 ] &lt;- act_wide_df[ , -1 ] / 60 Merge the respondent and summed activity tables, then the roster table, and finally the replicate weights: resp_act_df &lt;- merge( resp_df , act_wide_df ) stopifnot( nrow( resp_act_df ) == nrow( resp_df ) ) resp_act_rost_df &lt;- merge( resp_act_df , rost_df ) stopifnot( nrow( resp_act_rost_df ) == nrow( resp_df ) ) atus_df &lt;- merge( resp_act_rost_df , wgts_df ) stopifnot( nrow( atus_df ) == nrow( resp_df ) ) # remove dots from column names names( atus_df ) &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , names( atus_df ) ) atus_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # atus_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; , &quot;this_file.rds&quot; ) # saveRDS( atus_df , file = atus_fn , compress = FALSE ) Load the same object: # atus_df &lt;- readRDS( atus_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) atus_design &lt;- svrepdesign( weights = ~ tufinlwgt , repweights = &quot;finlwgt[0-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , mse = TRUE , data = atus_df ) Variable Recoding Add new columns to the data set: # caring for and helping household members is top level 03 from the lexicon # https://www.bls.gov/tus/lexicons/lexiconnoex2021.pdf atus_design &lt;- update( atus_design , any_care = as.numeric( tuactdur24_3 &gt; 0 ) , tesex = factor( tesex , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , age_category = factor( 1 + findInterval( teage , c( 18 , 35 , 65 ) ) , labels = c( &quot;under 18&quot; , &quot;18 - 34&quot; , &quot;35 - 64&quot; , &quot;65 or older&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( atus_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_category , atus_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , atus_design ) svyby( ~ one , ~ age_category , atus_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ tuactdur24_1 , atus_design ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ tesex , atus_design ) svyby( ~ tesex , ~ age_category , atus_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ tuactdur24_1 , atus_design ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ tesex , atus_design ) svyby( ~ tesex , ~ age_category , atus_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ tuactdur24_1 , atus_design , 0.5 ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ tuactdur24_5 , denominator = ~ tuactdur24_12 , atus_design ) Subsetting Restrict the survey design to any time volunteering: sub_atus_design &lt;- subset( atus_design , tuactdur24_15 &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ tuactdur24_1 , sub_atus_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ tuactdur24_1 , atus_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( atus_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ tuactdur24_1 , atus_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ tuactdur24_1 , atus_design , deff = TRUE ) # SRS with replacement svymean( ~ tuactdur24_1 , atus_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ any_care , atus_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( tuactdur24_1 ~ any_care , atus_design ) Perform a chi-squared test of association for survey data: svychisq( ~ any_care + tesex , atus_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( tuactdur24_1 ~ any_care + tesex , atus_design ) summary( glm_result ) Intermish cuckoo clock wristwatch or henna wristwatch Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ATUS users, this code replicates previously-presented examples: library(srvyr) atus_srvyr_design &lt;- as_survey( atus_design ) Calculate the mean (average) of a linear variable, overall and by groups: atus_srvyr_design %&gt;% summarize( mean = survey_mean( tuactdur24_1 ) ) atus_srvyr_design %&gt;% group_by( age_category ) %&gt;% summarize( mean = survey_mean( tuactdur24_1 ) ) Replication Example This example matches the “Caring for and helping household members” row of Table A-1: hours_per_day_civilian_population &lt;- svymean( ~ tuactdur24_3 , atus_design ) stopifnot( round( coef( hours_per_day_civilian_population ) , 2 ) == 0.47 ) percent_engaged_per_day &lt;- svymean( ~ any_care , atus_design ) stopifnot( round( coef( percent_engaged_per_day ) , 3 ) == 0.217 ) hours_per_day_among_engaged &lt;- svymean( ~ tuactdur24_3 , subset( atus_design , any_care ) ) stopifnot( round( coef( hours_per_day_among_engaged ) , 2 ) == 2.17 ) This example matches the average hours and SE from Section 7.5 of the User’s Guide: Download and import the activity, activity summary, respondent, and weights tables: actsum07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atussum_2007.zip&quot; ) resp07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusresp_2007.zip&quot; ) act07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atusact_2007.zip&quot; ) wgts07_df &lt;- atus_csv_import( &quot;https://www.bls.gov/tus/datafiles/atuswgts_2007.zip&quot; ) Option 1. Sum the two television fields from the activity summary file, removing zeroes: television_per_person &lt;- data.frame( tucaseid = actsum07_df[ , &#39;tucaseid&#39; ] , tuactdur24 = rowSums( actsum07_df[ , c( &#39;t120303&#39; , &#39;t120304&#39; ) ] ) ) television_per_person &lt;- television_per_person[ television_per_person[ , &#39;tuactdur24&#39; ] &gt; 0 , ] Option 2. Limit the activity file to television watching records according to the 2007 Lexicon: television_activity &lt;- subset( act07_df , tutier1code == 12 &amp; tutier2code == 3 &amp; tutier3code %in% 3:4 ) television_activity_summed &lt;- aggregate( tuactdur24 ~ tucaseid , data = television_activity , sum ) Confirm both aggregation options yield the same results: stopifnot( all( television_per_person[ , &#39;tucaseid&#39; ] == television_activity_summed[ , &#39;tucaseid&#39; ] ) ) stopifnot( all( television_per_person[ , &#39;tuactdur24&#39; ] == television_activity_summed[ , &#39;tuactdur24&#39; ] ) ) Merge the respondent and summed activity tables, then the replicate weights: resp07_tpp_df &lt;- merge( resp07_df[ , c( &#39;tucaseid&#39; , &#39;tufinlwgt&#39; ) ] , television_per_person , all.x = TRUE ) stopifnot( nrow( resp07_tpp_df ) == nrow( resp07_df ) ) # for individuals without television time, replace missings with zero minutes resp07_tpp_df[ is.na( resp07_tpp_df[ , &#39;tuactdur24&#39; ] ) , &#39;tuactdur24&#39; ] &lt;- 0 # convert minutes to hours resp07_tpp_df[ , &#39;tuactdur24_hour&#39; ] &lt;- resp07_tpp_df[ , &#39;tuactdur24&#39; ] / 60 atus07_df &lt;- merge( resp07_tpp_df , wgts07_df ) stopifnot( nrow( atus07_df ) == nrow( resp07_df ) ) Construct a complex sample survey design: atus07_design &lt;- svrepdesign( weights = ~ tufinlwgt , repweights = &quot;finlwgt[0-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , data = atus07_df ) Match the statistic and SE of the number of hours daily that americans older than 14 watch tv: result &lt;- svymean( ~ tuactdur24_hour , atus07_design ) stopifnot( round( coef( result ) , 2 ) == 2.62 ) stopifnot( round( SE( result ) , 4 ) == 0.0293 ) "],["behavioral-risk-factor-surveillance-system-brfss.html", "Behavioral Risk Factor Surveillance System (BRFSS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " Behavioral Risk Factor Surveillance System (BRFSS) A health behavior telephone interview survey with enough sample size to examine all fifty states. One table with one row per telephone respondent. A complex survey designed to generalize to the civilian non-institutional adult population of the U.S. Released annually since 1984 but all states did not participate until 1994. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: BRFSS Data User Guide Wikipedia Entry This poem # a cellphone vibrates # it&#39;s the cdc! asking # if you ate veggies Download, Import, Preparation Download and import the national file: library(haven) zip_tf &lt;- tempfile() zip_url &lt;- &quot;https://www.cdc.gov/brfss/annual_data/2021/files/LLCP2021XPT.zip&quot; download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) brfss_tbl &lt;- read_xpt( zip_tf ) brfss_df &lt;- data.frame( brfss_tbl ) names( brfss_df ) &lt;- tolower( names( brfss_df ) ) brfss_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # brfss_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;BRFSS&quot; , &quot;this_file.rds&quot; ) # saveRDS( brfss_df , file = brfss_fn , compress = FALSE ) Load the same object: # brfss_df &lt;- readRDS( brfss_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) variables_to_keep &lt;- c( &#39;one&#39; , &#39;x_psu&#39; , &#39;x_ststr&#39; , &#39;x_llcpwt&#39; , &#39;genhlth&#39; , &#39;medcost1&#39; , &#39;x_state&#39; , &#39;x_age80&#39; , &#39;nummen&#39; , &#39;numadult&#39; , &#39;x_hlthpln&#39; ) brfss_df &lt;- brfss_df[ variables_to_keep ] brfss_national_design &lt;- svydesign( id = ~ x_psu , strata = ~ x_ststr , data = brfss_df , weight = ~ x_llcpwt , nest = TRUE ) Since large linearized survey designs execute slowly, a replication design might be preferrable for exploratory analysis. Coefficients (such as means and medians) do not change, standard errors and confidence intervals differ slightly. The initial conversion with as.svrepdesign requires an extended period of processing time (perhaps run once overnight), subsequent analyses will finish much faster: # brfss_replication_design &lt;- # as.svrepdesign( # brfss_national_design , # type = &#39;bootstrap&#39; # ) # system.time( print( svymean( ~ x_age80 , brfss_national_design ) ) ) # system.time( print( svymean( ~ x_age80 , brfss_replication_design ) ) ) In this example, limit the national design to only Alaska for quicker processing: brfss_design &lt;- subset( brfss_national_design , x_state %in% 2 ) Variable Recoding Add new columns to the data set: brfss_design &lt;- update( brfss_design , fair_or_poor_health = ifelse( genhlth %in% 1:5 , as.numeric( genhlth &gt; 3 ) , NA ) , no_doc_visit_due_to_cost = factor( medcost1 , levels = c( 1 , 2 , 7 , 9 ) , labels = c( &quot;yes&quot; , &quot;no&quot; , &quot;dk&quot; , &quot;rf&quot; ) ) , state_name = factor( x_state , levels = c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 66, 72, 78) , labels = c(&quot;ALABAMA&quot;, &quot;ALASKA&quot;, &quot;ARIZONA&quot;, &quot;ARKANSAS&quot;, &quot;CALIFORNIA&quot;, &quot;COLORADO&quot;, &quot;CONNECTICUT&quot;, &quot;DELAWARE&quot;, &quot;DISTRICT OF COLUMBIA&quot;, &quot;FLORIDA&quot;, &quot;GEORGIA&quot;, &quot;HAWAII&quot;, &quot;IDAHO&quot;, &quot;ILLINOIS&quot;, &quot;INDIANA&quot;, &quot;IOWA&quot;, &quot;KANSAS&quot;, &quot;KENTUCKY&quot;, &quot;LOUISIANA&quot;, &quot;MAINE&quot;, &quot;MARYLAND&quot;, &quot;MASSACHUSETTS&quot;, &quot;MICHIGAN&quot;, &quot;MINNESOTA&quot;, &quot;MISSISSIPPI&quot;, &quot;MISSOURI&quot;, &quot;MONTANA&quot;, &quot;NEBRASKA&quot;, &quot;NEVADA&quot;, &quot;NEW HAMPSHIRE&quot;, &quot;NEW JERSEY&quot;, &quot;NEW MEXICO&quot;, &quot;NEW YORK&quot;, &quot;NORTH CAROLINA&quot;, &quot;NORTH DAKOTA&quot;, &quot;OHIO&quot;, &quot;OKLAHOMA&quot;, &quot;OREGON&quot;, &quot;PENNSYLVANIA&quot;, &quot;RHODE ISLAND&quot;, &quot;SOUTH CAROLINA&quot;, &quot;SOUTH DAKOTA&quot;, &quot;TENNESSEE&quot;, &quot;TEXAS&quot;, &quot;UTAH&quot;, &quot;VERMONT&quot;, &quot;VIRGINIA&quot;, &quot;WASHINGTON&quot;, &quot;WEST VIRGINIA&quot;, &quot;WISCONSIN&quot;, &quot;WYOMING&quot;, &quot;GUAM&quot;, &quot;PUERTO RICO&quot;, &quot;U.S. VIRGIN ISLANDS&quot;) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( brfss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , brfss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , brfss_design ) svyby( ~ one , ~ state_name , brfss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ x_age80 , brfss_design ) svyby( ~ x_age80 , ~ state_name , brfss_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ no_doc_visit_due_to_cost , brfss_design , na.rm = TRUE ) svyby( ~ no_doc_visit_due_to_cost , ~ state_name , brfss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ x_age80 , brfss_design ) svyby( ~ x_age80 , ~ state_name , brfss_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ no_doc_visit_due_to_cost , brfss_design , na.rm = TRUE ) svyby( ~ no_doc_visit_due_to_cost , ~ state_name , brfss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ x_age80 , brfss_design , 0.5 ) svyby( ~ x_age80 , ~ state_name , brfss_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ nummen , denominator = ~ numadult , brfss_design , na.rm = TRUE ) Subsetting Restrict the survey design to persons without health insurance: sub_brfss_design &lt;- subset( brfss_design , x_hlthpln == 2 ) Calculate the mean (average) of this subset: svymean( ~ x_age80 , sub_brfss_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ x_age80 , brfss_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ x_age80 , ~ state_name , brfss_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( brfss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ x_age80 , brfss_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ x_age80 , brfss_design , deff = TRUE ) # SRS with replacement svymean( ~ x_age80 , brfss_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ fair_or_poor_health , brfss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( x_age80 ~ fair_or_poor_health , brfss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ fair_or_poor_health + no_doc_visit_due_to_cost , brfss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( x_age80 ~ fair_or_poor_health + no_doc_visit_due_to_cost , brfss_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for BRFSS users, this code replicates previously-presented examples: library(srvyr) brfss_srvyr_design &lt;- as_survey( brfss_design ) Calculate the mean (average) of a linear variable, overall and by groups: brfss_srvyr_design %&gt;% summarize( mean = survey_mean( x_age80 ) ) brfss_srvyr_design %&gt;% group_by( state_name ) %&gt;% summarize( mean = survey_mean( x_age80 ) ) Replication Example This example matches Alaska’s confidence intervals from the BRFSS Prevalence &amp; Trends Data: result &lt;- svymean( ~ no_doc_visit_due_to_cost , subset( brfss_design , no_doc_visit_due_to_cost %in% c( &#39;yes&#39; , &#39;no&#39; ) ) , na.rm = TRUE ) stopifnot( round( confint( result )[ 1 , 1 ] , 3 ) == 0.094 ) stopifnot( round( confint( result )[ 1 , 2 ] , 3 ) == 0.122 ) stopifnot( round( confint( result )[ 2 , 1 ] , 3 ) == 0.878 ) stopifnot( round( confint( result )[ 2 , 2 ] , 3 ) == 0.906 ) "],["consumer-expenditure-survey-ces.html", "Consumer Expenditure Survey (CES) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " Consumer Expenditure Survey (CES) A household budget survey designed to guide major economic indicators like the Consumer Price Index. One table of survey responses per quarter with one row per sampled household (consumer unit). Additional tables containing one record per expenditure. A complex sample survey designed to generalize to the civilian non-institutional U.S. population. Released annually since 1996. Administered by the Bureau of Labor Statistics. Please skim before you begin: Consumer Expenditure Surveys Public Use Microdata Getting Started Guide Wikipedia Entry This poem # price indices and # you spent how much on beans, jack? # pocketbook issues Download, Import, Preparation Download both the prior and current year of interview microdata: library(haven) tf_prior_year &lt;- tempfile() this_url_prior_year &lt;- &quot;https://www.bls.gov/cex/pumd/data/stata/intrvw20.zip&quot; download.file( this_url_prior_year , tf_prior_year , mode = &#39;wb&#39; ) unzipped_files_prior_year &lt;- unzip( tf_prior_year , exdir = tempdir() ) tf_current_year &lt;- tempfile() this_url_current_year &lt;- &quot;https://www.bls.gov/cex/pumd/data/stata/intrvw21.zip&quot; download.file( this_url_current_year , tf_current_year , mode = &#39;wb&#39; ) unzipped_files_current_year &lt;- unzip( tf_current_year , exdir = tempdir() ) unzipped_files &lt;- c( unzipped_files_current_year , unzipped_files_prior_year ) Import and stack all 2021 quarterly files plus 2022’s first quarter: fmli_files &lt;- grep( &quot;fmli2[1-2]&quot; , unzipped_files , value = TRUE ) fmli_tbls &lt;- lapply( fmli_files , read_dta ) fmli_dfs &lt;- lapply( fmli_tbls , data.frame ) fmli_dfs &lt;- lapply( fmli_dfs , function( w ){ names( w ) &lt;- tolower( names( w ) ) ; w } ) fmli_cols &lt;- lapply( fmli_dfs , names ) intersecting_cols &lt;- Reduce( intersect , fmli_cols ) fmli_dfs &lt;- lapply( fmli_dfs , function( w ) w[ intersecting_cols ] ) ces_df &lt;- do.call( rbind , fmli_dfs ) Scale the weight columns based on the number of months in 2021: ces_df[ , c( &#39;qintrvyr&#39; , &#39;qintrvmo&#39; ) ] &lt;- sapply( ces_df[ , c( &#39;qintrvyr&#39; , &#39;qintrvmo&#39; ) ] , as.numeric ) weight_columns &lt;- grep( &#39;wt&#39; , names( ces_df ) , value = TRUE ) ces_df &lt;- transform( ces_df , mo_scope = ifelse( qintrvyr %in% 2021 &amp; qintrvmo %in% 1:3 , qintrvmo - 1 , ifelse( qintrvyr %in% 2022 , 4 - qintrvmo , 3 ) ) ) for ( this_column in weight_columns ){ ces_df[ is.na( ces_df[ , this_column ] ) , this_column ] &lt;- 0 ces_df[ , paste0( &#39;popwt_&#39; , this_column ) ] &lt;- ( ces_df[ , this_column ] * ces_df[ , &#39;mo_scope&#39; ] / 12 ) } Combine previous quarter and current quarter variables into a single variable: expenditure_variables &lt;- gsub( &quot;pq$&quot; , &quot;&quot; , grep( &quot;pq$&quot; , names( ces_df ) , value = TRUE ) ) # confirm that for every variable ending in pq, # there&#39;s the same variable ending in cq stopifnot( all( paste0( expenditure_variables , &#39;cq&#39; ) %in% names( ces_df ) ) ) # confirm none of the variables without the pq or cq suffix exist if( any( expenditure_variables %in% names( ces_df ) ) ) stop( &quot;variable conflict&quot; ) for( this_column in expenditure_variables ){ ces_df[ , this_column ] &lt;- rowSums( ces_df[ , paste0( this_column , c( &#39;pq&#39; , &#39;cq&#39; ) ) ] , na.rm = TRUE ) # annualize the quarterly spending ces_df[ , this_column ] &lt;- 4 * ces_df[ , this_column ] ces_df[ is.na( ces_df[ , this_column ] ) , this_column ] &lt;- 0 } Append any interview survey UCC found at https://www.bls.gov/cex/ce_source_integrate.xlsx: ucc_exp &lt;- c( &quot;450110&quot; , &quot;450210&quot; ) mtbi_files &lt;- grep( &quot;mtbi2[1-2]&quot; , unzipped_files , value = TRUE ) mtbi_tbls &lt;- lapply( mtbi_files , read_dta ) mtbi_dfs &lt;- lapply( mtbi_tbls , data.frame ) mtbi_dfs &lt;- lapply( mtbi_dfs , function( w ){ names( w ) &lt;- tolower( names( w ) ) ; w } ) mtbi_dfs &lt;- lapply( mtbi_dfs , function( w ) w[ c( &#39;newid&#39; , &#39;cost&#39; , &#39;ucc&#39; , &#39;ref_yr&#39; ) ] ) mtbi_df &lt;- do.call( rbind , mtbi_dfs ) mtbi_df &lt;- subset( mtbi_df , ( ref_yr %in% 2021 ) &amp; ( ucc %in% ucc_exp ) ) mtbi_agg &lt;- aggregate( cost ~ newid , data = mtbi_df , sum ) names( mtbi_agg ) &lt;- c( &#39;newid&#39; , &#39;new_car_truck_exp&#39; ) before_nrow &lt;- nrow( ces_df ) ces_df &lt;- merge( ces_df , mtbi_agg , all.x = TRUE ) stopifnot( nrow( ces_df ) == before_nrow ) ces_df[ is.na( ces_df[ , &#39;new_car_truck_exp&#39; ] ) , &#39;new_car_truck_exp&#39; ] &lt;- 0 Save locally   Save the object at any point: # ces_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; , &quot;this_file.rds&quot; ) # saveRDS( ces_df , file = ces_fn , compress = FALSE ) Load the same object: # ces_df &lt;- readRDS( ces_fn ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: Separate the ces_df data.frame into five implicates, each differing from the others only in the multiply-imputed variables: library(survey) library(mitools) # create a vector containing all of the multiply-imputed variables # (leaving the numbers off the end) mi_vars &lt;- gsub( &quot;5$&quot; , &quot;&quot; , grep( &quot;[a-z]5$&quot; , names( ces_df ) , value = TRUE ) ) # loop through each of the five variables.. for ( i in 1:5 ){ # copy the &#39;ces_df&#39; table over to a new temporary data frame &#39;x&#39; x &lt;- ces_df # loop through each of the multiply-imputed variables.. for ( j in mi_vars ){ # copy the contents of the current column (for example &#39;welfare1&#39;) # over to a new column ending in &#39;mi&#39; (for example &#39;welfaremi&#39;) x[ , paste0( j , &#39;mi&#39; ) ] &lt;- x[ , paste0( j , i ) ] # delete the all five of the imputed variable columns x &lt;- x[ , !( names( x ) %in% paste0( j , 1:5 ) ) ] } assign( paste0( &#39;imp&#39; , i ) , x ) } ces_design &lt;- svrepdesign( weights = ~ finlwt21 , repweights = &quot;^wtrep[0-9][0-9]$&quot; , data = imputationList( list( imp1 , imp2 , imp3 , imp4 , imp5 ) ) , type = &quot;BRR&quot; , combined.weights = TRUE , mse = TRUE ) Variable Recoding Add new columns to the data set: ces_design &lt;- update( ces_design , one = 1 , any_food_stamp = as.numeric( jfs_amtmi &gt; 0 ) , bls_urbn = factor( bls_urbn , levels = 1:2 , labels = c( &#39;urban&#39; , &#39;rural&#39; ) ) , sex_ref = factor( sex_ref , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( ces_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( ces_design , svyby( ~ one , ~ bls_urbn , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( ces_design , svytotal( ~ one ) ) ) MIcombine( with( ces_design , svyby( ~ one , ~ bls_urbn , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( ces_design , svymean( ~ totexp ) ) ) MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( ces_design , svymean( ~ sex_ref ) ) ) MIcombine( with( ces_design , svyby( ~ sex_ref , ~ bls_urbn , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( ces_design , svytotal( ~ totexp ) ) ) MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( ces_design , svytotal( ~ sex_ref ) ) ) MIcombine( with( ces_design , svyby( ~ sex_ref , ~ bls_urbn , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( ces_design , svyquantile( ~ totexp , 0.5 , se = TRUE ) ) ) MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svyquantile , 0.5 , se = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( ces_design , svyratio( numerator = ~ totexp , denominator = ~ fincbtxmi ) ) ) Subsetting Restrict the survey design to california residents: sub_ces_design &lt;- subset( ces_design , state == &#39;06&#39; ) Calculate the mean (average) of this subset: MIcombine( with( sub_ces_design , svymean( ~ totexp ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( ces_design , svymean( ~ totexp ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( ces_design , svyby( ~ totexp , ~ bls_urbn , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ces_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( ces_design , svyvar( ~ totexp ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( ces_design , svymean( ~ totexp , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( ces_design , svymean( ~ totexp , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ any_food_stamp , ces_design , # method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( totexp ~ any_food_stamp , ces_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ any_food_stamp + sex_ref , ces_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( ces_design , svyglm( totexp ~ any_food_stamp + sex_ref ) ) ) summary( glm_result ) Intermish penny for your thoughts Replication Example This example matches the number of consumer units and the Cars and trucks, new rows of Table R-1: result &lt;- MIcombine( with( ces_design , svytotal( ~ as.numeric( popwt_finlwt21 / finlwt21 ) ) ) ) stopifnot( round( coef( result ) , -3 ) == 133595000 ) results &lt;- sapply( weight_columns , function( this_column ){ sum( ces_df[ , &#39;new_car_truck_exp&#39; ] * ces_df[ , this_column ] ) / sum( ces_df[ , paste0( &#39;popwt_&#39; , this_column ) ] ) } ) stopifnot( round( results[1] , 2 ) == 2210.45 ) standard_error &lt;- sqrt( ( 1 / 44 ) * sum( ( results[-1] - results[1] )^2 ) ) stopifnot( round( standard_error , 2 ) == 184.47 ) # note the minor differences MIcombine( with( ces_design , svymean( ~ cartkn ) ) ) "],["exame-nacional-de-desempenho-de-estudantes-enade.html", "Exame Nacional de Desempenho de Estudantes (ENADE) Download, Import, Preparation Analysis Examples with base R   Intermish Analysis Examples with dplyr   Replication Example", " Exame Nacional de Desempenho de Estudantes (ENADE) The nationwide mandatory examination of college graduates. One table with one row per individual undergraduate student in Brazil. An enumeration of undergraduate students in Brazil. Released annually since 2004. Compiled by the Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP). Please skim before you begin: Cálculo da nota final do Exame Nacional de Desempenho dos Estudiantes Wikipedia Entry This poem # undergraduates # sit for standardized testing # exit interview Download, Import, Preparation Download, import, and merge two of the 2021 files: library(httr) library(archive) tf &lt;- tempfile() this_url &lt;- &quot;https://download.inep.gov.br/microdados/microdados_enade_2021.zip&quot; GET( this_url , write_disk( tf ) ) archive_extract( tf , dir = tempdir() ) read_enade_archive &lt;- function( this_regular_expression , this_directory ){ this_filename &lt;- grep( this_regular_expression , list.files( this_directory , recursive = TRUE , full.names = TRUE ) , value = TRUE ) this_df &lt;- read.table( this_filename , header = TRUE , sep = &quot;;&quot; , na.strings = &quot;&quot; ) names( this_df ) &lt;- tolower( names( this_df ) ) this_df } arq1_df &lt;- read_enade_archive( &#39;arq1\\\\.txt$&#39; , tempdir() ) arq1_df &lt;- unique( arq1_df[ c( &#39;co_curso&#39; , &#39;co_uf_curso&#39; , &#39;co_categad&#39; , &#39;co_grupo&#39; ) ] ) arq3_df &lt;- read_enade_archive( &#39;arq3\\\\.txt$&#39; , tempdir() ) enade_df &lt;- merge( arq3_df , arq1_df ) stopifnot( nrow( enade_df ) == nrow( arq3_df ) ) Save locally   Save the object at any point: # enade_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ENADE&quot; , &quot;this_file.rds&quot; ) # saveRDS( enade_df , file = enade_fn , compress = FALSE ) Load the same object: # enade_df &lt;- readRDS( enade_fn ) Analysis Examples with base R   Variable Recoding Add new columns to the data set: enade_df &lt;- transform( enade_df , # qual foi o tempo gasto por voce para concluir a prova? less_than_two_hours = as.numeric( co_rs_i9 %in% c( &#39;A&#39; , &#39;B&#39; ) ) , administrative_category = factor( co_categad , levels = c( 1:5 , 7 ) , labels = c( &#39;1. Pública Federal&#39; , &#39;2. Pública Estadual&#39; , &#39;3. Pública Municipal&#39; , &#39;4. Privada com fins lucrativos&#39; , &#39;5. Privada sem fins lucrativos&#39; , &#39;7. Especial&#39; ) ) , state_name = factor( co_uf_curso , levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) , labels = c( &quot;Rondonia&quot; , &quot;Acre&quot; , &quot;Amazonas&quot; , &quot;Roraima&quot; , &quot;Para&quot; , &quot;Amapa&quot; , &quot;Tocantins&quot; , &quot;Maranhao&quot; , &quot;Piaui&quot; , &quot;Ceara&quot; , &quot;Rio Grande do Norte&quot; , &quot;Paraiba&quot; , &quot;Pernambuco&quot; , &quot;Alagoas&quot; , &quot;Sergipe&quot; , &quot;Bahia&quot; , &quot;Minas Gerais&quot; , &quot;Espirito Santo&quot; , &quot;Rio de Janeiro&quot; , &quot;Sao Paulo&quot; , &quot;Parana&quot; , &quot;Santa Catarina&quot; , &quot;Rio Grande do Sul&quot; , &quot;Mato Grosso do Sul&quot; , &quot;Mato Grosso&quot; , &quot;Goias&quot; , &quot;Distrito Federal&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( enade_df ) table( enade_df[ , &quot;administrative_category&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( enade_df[ , &quot;state_name&quot; ] ) ) prop.table( table( enade_df[ , c( &quot;state_name&quot; , &quot;administrative_category&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( enade_df[ , &quot;nt_obj_fg&quot; ] , 0.5 , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to students reporting that the general training section was easy or very easy: sub_enade_df &lt;- subset( enade_df , co_rs_i1 %in% c( &quot;A&quot; , &quot;B&quot; ) ) Calculate the mean (average) of this subset: mean( sub_enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;administrative_category&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( nt_obj_fg ~ less_than_two_hours , enade_df ) Perform a chi-squared test of association: this_table &lt;- table( enade_df[ , c( &quot;less_than_two_hours&quot; , &quot;state_name&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( nt_obj_fg ~ less_than_two_hours + state_name , data = enade_df ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for ENADE users, this code replicates previously-presented examples: library(dplyr) enade_tbl &lt;- as_tibble( enade_df ) Calculate the mean (average) of a linear variable, overall and by groups: enade_tbl %&gt;% summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) ) enade_tbl %&gt;% group_by( administrative_category ) %&gt;% summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) ) Replication Example This example matches the tecnologia em gestão da tecnologia da informação test scores on PDF page 48 of the 2021 final results document: it_students &lt;- subset( enade_df , co_grupo %in% 6409 ) results &lt;- sapply( it_students[ c( &#39;nt_fg&#39; , &#39;nt_ce&#39; , &#39;nt_ger&#39; ) ] , mean , na.rm = TRUE ) stopifnot( round( results[ &#39;nt_fg&#39; ] , 1 ) == 30.4 ) stopifnot( round( results[ &#39;nt_ce&#39; ] , 1 ) == 38.2 ) stopifnot( round( results[ &#39;nt_ger&#39; ] , 1 ) == 36.3 ) "],["european-social-survey-ess.html", "European Social Survey (ESS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " European Social Survey (ESS) The barometer of political opinion and behavior across the continent. One table per country with one row per sampled respondent. A complex sample designed to generalize to residents aged 15 and older in participating nations. Released biennially since 2002. Headquartered at City, University of London and governed by a scientific team across Europe. Please skim before you begin: Findings from the European Social Survey Wikipedia Entry This poem # pent up belief gauge # open border monarchists # survey for your thoughts Download, Import, Preparation Register at the ESS Data Portal at https://ess-search.nsd.no/. Choose ESS round 8 - 2016. Welfare attitudes, Attitudes to climate change. Download the integrated file and also the sample design (SDDF) files as SAV (SPSS) files: library(foreign) ess_int_df &lt;- read.spss( file.path( path.expand( &quot;~&quot; ) , &quot;ESS8e02_2.sav&quot; ) , to.data.frame = TRUE , use.value.labels = FALSE ) ess_sddf_df &lt;- read.spss( file.path( path.expand( &quot;~&quot; ) , &quot;ESS8SDDFe01_1.sav&quot; ) , to.data.frame = TRUE , use.value.labels = FALSE ) ess_df &lt;- merge( ess_int_df , ess_sddf_df , by = c( &#39;cntry&#39; , &#39;idno&#39; ) ) stopifnot( nrow( ess_df ) == nrow( ess_int_df ) ) Save locally   Save the object at any point: # ess_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;this_file.rds&quot; ) # saveRDS( ess_df , file = ess_fn , compress = FALSE ) Load the same object: # ess_df &lt;- readRDS( ess_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) ## Loading required package: grid ## Loading required package: Matrix ## Loading required package: survival ## ## Attaching package: &#39;survey&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## dotchart options( survey.lonely.psu = &quot;adjust&quot; ) ess_df[ , &#39;anweight&#39; ] &lt;- ess_df[ , &#39;pspwght&#39; ] * ess_df[ , &#39;pweight&#39; ] * 10000 ess_design &lt;- svydesign( ids = ~psu , strata = ~stratum , weights = ~anweight , data = ess_df , nest = TRUE ) Variable Recoding Add new columns to the data set: ess_design &lt;- update( ess_design , one = 1 , gndr = factor( gndr , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , netusoft = factor( netusoft , levels = 1:5 , labels = c( &#39;Never&#39; , &#39;Only occasionally&#39; , &#39;A few times a week&#39; , &#39;Most days&#39; , &#39;Every day&#39; ) ) , belonging_to_particular_religion = as.numeric( rlgblg == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( ess_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cntry , ess_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , ess_design ) svyby( ~ one , ~ cntry , ess_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ ppltrst , ess_design , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ gndr , ess_design , na.rm = TRUE ) svyby( ~ gndr , ~ cntry , ess_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ ppltrst , ess_design , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ gndr , ess_design , na.rm = TRUE ) svyby( ~ gndr , ~ cntry , ess_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ ppltrst , ess_design , 0.5 , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ppltrst , denominator = ~ pplfair , ess_design , na.rm = TRUE ) Subsetting Restrict the survey design to voters: sub_ess_design &lt;- subset( ess_design , vote == 1 ) Calculate the mean (average) of this subset: svymean( ~ ppltrst , sub_ess_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ ppltrst , ess_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ ppltrst , ~ cntry , ess_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ess_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ ppltrst , ess_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ ppltrst , ess_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ ppltrst , ess_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ belonging_to_particular_religion , ess_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( ppltrst ~ belonging_to_particular_religion , ess_design ) Perform a chi-squared test of association for survey data: svychisq( ~ belonging_to_particular_religion + gndr , ess_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( ppltrst ~ belonging_to_particular_religion + gndr , ess_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ESS users, this code replicates previously-presented examples: library(srvyr) ## ## Attaching package: &#39;srvyr&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter ess_srvyr_design &lt;- as_survey( ess_design ) Calculate the mean (average) of a linear variable, overall and by groups: ess_srvyr_design %&gt;% summarize( mean = survey_mean( ppltrst , na.rm = TRUE ) ) ess_srvyr_design %&gt;% group_by( cntry ) %&gt;% summarize( mean = survey_mean( ppltrst , na.rm = TRUE ) ) Replication Example This example below matches statistics and confidence intervals within 0.1% from the Guide to Using Weights and Sample Design Indicators with ESS Data published_proportions &lt;- c( 0.166 , 0.055 , 0.085 , 0.115 , 0.578 ) published_lb &lt;- c( 0.146 , 0.045 , 0.072 , 0.099 , 0.550 ) published_ub &lt;- c( 0.188 , 0.068 , 0.100 , 0.134 , 0.605 ) austrians &lt;- subset( ess_design , cntry == &#39;AT&#39; ) ( results &lt;- svymean( ~ netusoft , austrians , na.rm = TRUE ) ) stopifnot( all( round( coef( results ) , 3 ) == published_proportions ) ) ( ci_results &lt;- confint( results ) ) stopifnot( all( abs( ci_results[ , 1 ] - published_lb ) &lt; 0.0015 ) ) stopifnot( all( abs( ci_results[ , 2 ] - published_ub ) &lt; 0.0015 ) ) "],["medicare-current-beneficiary-survey-mcbs.html", "Medicare Current Beneficiary Survey (MCBS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " Medicare Current Beneficiary Survey (MCBS) The monitoring system for Medicare enrollees in the United States on topics not available in the program’s administrative data, such as out of pocket expenditure and beneficiary satisfaction. Survey and supplemental tables with one row per sampled individual, although downloadable datasets not linkable. A complex sample survey designed to generalize to all elderly and disabled individuals with at least one month of program enrollment during the calendar year. Released annually as a public use file since 2015. Conducted by the Office of Enterprise Data and Analytics (OEDA) of the Centers for Medicare &amp; Medicaid Services (CMS) through a contract with NORC at the University of Chicago. Please skim before you begin: MCBS Methodology Report MCBS Advanced Tutorial on Weighting and Variance Estimation This poem # old, or disabled # access to medical care, # utilization Download, Import, Preparation tf &lt;- tempfile() this_url &lt;- &quot;https://www.cms.gov/files/zip/cspuf2019.zip&quot; download.file( this_url , tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) mcbs_csv &lt;- grep( &#39;\\\\.csv$&#39; , unzipped_files , value = TRUE ) mcbs_df &lt;- read.csv( mcbs_csv ) names( mcbs_df ) &lt;- tolower( names( mcbs_df ) ) Save locally   Save the object at any point: # mcbs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;MCBS&quot; , &quot;this_file.rds&quot; ) # saveRDS( mcbs_df , file = mcbs_fn , compress = FALSE ) Load the same object: # mcbs_df &lt;- readRDS( mcbs_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) mcbs_design &lt;- svrepdesign( weight = ~cspufwgt , repweights = &#39;cspuf[0-9]+&#39; , mse = TRUE , type = &#39;Fay&#39; , rho = 0.3 , data = mcbs_df ) Variable Recoding Add new columns to the data set: mcbs_design &lt;- update( mcbs_design , one = 1 , csp_age = factor( csp_age , levels = 1:3 , labels = c( &#39;01: younger than 65&#39; , &#39;02: 65 to 74&#39; , &#39;03: 75 or older&#39; ) ) , two_or_more_chronic_conditions = as.numeric( csp_nchrncnd &gt; 1 ) , csp_sex = factor( csp_sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( mcbs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ csp_age , mcbs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , mcbs_design ) svyby( ~ one , ~ csp_age , mcbs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ pamtoop , mcbs_design ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ csp_sex , mcbs_design ) svyby( ~ csp_sex , ~ csp_age , mcbs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ pamtoop , mcbs_design ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ csp_sex , mcbs_design ) svyby( ~ csp_sex , ~ csp_age , mcbs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ pamtoop , mcbs_design , 0.5 ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ pamtoop , denominator = ~ pamttot , mcbs_design ) Subsetting Restrict the survey design to household income below $25,000: sub_mcbs_design &lt;- subset( mcbs_design , csp_income == 1 ) Calculate the mean (average) of this subset: svymean( ~ pamtoop , sub_mcbs_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ pamtoop , mcbs_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ pamtoop , ~ csp_age , mcbs_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( mcbs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ pamtoop , mcbs_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ pamtoop , mcbs_design , deff = TRUE ) # SRS with replacement svymean( ~ pamtoop , mcbs_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ two_or_more_chronic_conditions , mcbs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( pamtoop ~ two_or_more_chronic_conditions , mcbs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ two_or_more_chronic_conditions + csp_sex , mcbs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( pamtoop ~ two_or_more_chronic_conditions + csp_sex , mcbs_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for MCBS users, this code replicates previously-presented examples: library(srvyr) mcbs_srvyr_design &lt;- as_survey( mcbs_design ) Calculate the mean (average) of a linear variable, overall and by groups: mcbs_srvyr_design %&gt;% summarize( mean = survey_mean( pamtoop ) ) mcbs_srvyr_design %&gt;% group_by( csp_age ) %&gt;% summarize( mean = survey_mean( pamtoop ) ) Replication Example This example matches the weighted total from the 2019 Data User’s Guide: Cost Supplement File Public Use File: stopifnot( round( coef( svytotal( ~ one , mcbs_design ) ) , 0 ) == 56307461 ) "],["medical-large-claims-experience-study-mlces.html", "Medical Large Claims Experience Study (MLCES) Download, Import, Preparation Analysis Examples with base R   Intermish Analysis Examples with dplyr   Replication Example", " Medical Large Claims Experience Study (MLCES) A high quality dataset of medical claims from seven private health insurance companies. One table with one row per individual with nonzero total paid charges. A convenience sample of group (employer-sponsored) health insurers in the United States. 1997 thru 1999 with no expected updates in the future. Provided by the Society of Actuaries (SOA). Please skim before you begin: Group Medical Insurance Claims Database Collection and Analysis Report Claim Severities, Claim Relativities, and Age: Evidence from SOA Group Health Data This poem # skewed by black swan tails # means, medians sing adieu # claims distribution Download, Import, Preparation Download and import the 1999 medical claims file: tf &lt;- tempfile() this_url &lt;- &quot;https://www.soa.org/Files/Research/1999.zip&quot; download.file( this_url , tf , mode = &#39;wb&#39; ) unzipped_file &lt;- unzip( tf , exdir = tempdir() ) mlces_df &lt;- read.csv( unzipped_file ) names( mlces_df ) &lt;- tolower( names( mlces_df ) ) Save locally   Save the object at any point: # mlces_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;MLCES&quot; , &quot;this_file.rds&quot; ) # saveRDS( mlces_df , file = mlces_fn , compress = FALSE ) Load the same object: # mlces_df &lt;- readRDS( mlces_fn ) Analysis Examples with base R   Variable Recoding Add new columns to the data set: mlces_df &lt;- transform( mlces_df , one = 1 , claimant_relationship_to_policyholder = ifelse( relation == &quot;E&quot; , &quot;covered employee&quot; , ifelse( relation == &quot;S&quot; , &quot;spouse of covered employee&quot; , ifelse( relation == &quot;D&quot; , &quot;dependent of covered employee&quot; , NA ) ) ) , ppo_plan = as.numeric( ppo == &#39;Y&#39; ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( mlces_df ) table( mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( mlces_df[ , &quot;patsex&quot; ] ) ) prop.table( table( mlces_df[ , c( &quot;patsex&quot; , &quot;claimant_relationship_to_policyholder&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( mlces_df[ , &quot;totpdchg&quot; ] , 0.5 ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to persons under 18: sub_mlces_df &lt;- subset( mlces_df , ( ( claimyr - patbrtyr ) &lt; 18 ) ) Calculate the mean (average) of this subset: mean( sub_mlces_df[ , &quot;totpdchg&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( totpdchg ~ ppo_plan , mlces_df ) Perform a chi-squared test of association: this_table &lt;- table( mlces_df[ , c( &quot;ppo_plan&quot; , &quot;patsex&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( totpdchg ~ ppo_plan + patsex , data = mlces_df ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for MLCES users, this code replicates previously-presented examples: library(dplyr) mlces_tbl &lt;- as_tibble( mlces_df ) Calculate the mean (average) of a linear variable, overall and by groups: mlces_tbl %&gt;% summarize( mean = mean( totpdchg ) ) mlces_tbl %&gt;% group_by( claimant_relationship_to_policyholder ) %&gt;% summarize( mean = mean( totpdchg ) ) Replication Example This example matches statistics in Table II-A’s 1999 row numbers 52 and 53 from the Database: Match Claimants Exceeding Deductible: # $0 deductible stopifnot( nrow( mlces_df ) == 1591738 ) # $1,000 deductible mlces_above_1000_df &lt;- subset( mlces_df , totpdchg &gt; 1000 ) stopifnot( nrow( mlces_above_1000_df ) == 402550 ) Match the Excess Charges Above Deductible: # $0 deductible stopifnot( round( sum( mlces_df[ , &#39;totpdchg&#39; ] ) , 0 ) == 2599356658 ) # $1,000 deductible stopifnot( round( sum( mlces_above_1000_df[ , &#39;totpdchg&#39; ] - 1000 ) , 0 ) == 1883768786 ) "],["national-beneficiary-survey-nbs.html", "National Beneficiary Survey (NBS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " National Beneficiary Survey (NBS) The principal microdata for U.S. disability researchers interested in Social Security program performance. One table with one row per respondent. A complex sample designed to generalize to Americans between age 18 and full retirement age, covered by either Social Security Disability Insurance (SSDI) or Supplemental Security Income (SSI). Released at irregular intervals, with 2004, 2005, 2006, 2010, 2015, 2017, and 2019 available. Administered by the Social Security Administration. Please skim before you begin: National Beneficiary Survey: Disability Statistics, 2015 National Beneficiary Survey - General Waves Round 7: User’s Guide This poem # social safety net # poverty acrobatics # trap or trampoline Download, Import, Preparation Download and import the round 7 file: library(haven) zip_tf &lt;- tempfile() zip_url &lt;- &quot;https://www.ssa.gov/disabilityresearch/documents/R7NBSPUF_STATA.zip&quot; download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) nbs_tbl &lt;- read_stata( zip_tf ) nbs_df &lt;- data.frame( nbs_tbl ) names( nbs_df ) &lt;- tolower( names( nbs_df ) ) nbs_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # nbs_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NBS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nbs_df , file = nbs_fn , compress = FALSE ) Load the same object: # nbs_df &lt;- readRDS( nbs_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) # representative beneficiary sample nbs_design &lt;- svydesign( id = ~ r7_a_psu_pub , strata = ~ r7_a_strata , weights = ~ r7_wtr7_ben , data = subset( nbs_df , r7_wtr7_ben &gt; 0 ) ) # cross-sectional successful worker sample nbs_design &lt;- svydesign( id = ~ r7_a_psu_pub , strata = ~ r7_a_strata , weights = ~ r7_wtr7_cssws , data = subset( nbs_df , r7_wtr7_cssws &gt; 0 ) ) # longitudinal successful worker sample lngsws_design &lt;- svydesign( id = ~ r7_a_psu_pub , strata = ~ r7_a_strata , weights = ~ r7_wtr7_lngsws , data = subset( nbs_df , r7_wtr7_lngsws &gt; 0 ) ) Variable Recoding Add new columns to the data set: nbs_design &lt;- update( nbs_design , male = as.numeric( r7_orgsampinfo_sex == 1 ) , age_categories = factor( r7_c_intage_pub , labels = c( &quot;18-25&quot; , &quot;26-40&quot; , &quot;41-55&quot; , &quot;56 and older&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nbs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_categories , nbs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nbs_design ) svyby( ~ one , ~ age_categories , nbs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ r7_c_hhsize_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_c_hhsize_pub , ~ age_categories , nbs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ r7_c_hhsize_pub , nbs_design , na.rm = TRUE ) svyby( ~ r7_c_hhsize_pub , ~ age_categories , nbs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ r7_n_totssbenlastmnth_pub , nbs_design , 0.5 , na.rm = TRUE ) svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ r7_n_ssilastmnth_pub , denominator = ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) Subsetting Restrict the survey design to currently covered by Medicare: sub_nbs_design &lt;- subset( nbs_design , r7_c_curmedicare == 1 ) Calculate the mean (average) of this subset: svymean( ~ r7_n_totssbenlastmnth_pub , sub_nbs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ r7_n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nbs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ r7_n_totssbenlastmnth_pub , nbs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , nbs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( r7_n_totssbenlastmnth_pub ~ male , nbs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + r7_c_hhsize_pub , nbs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( r7_n_totssbenlastmnth_pub ~ male + r7_c_hhsize_pub , nbs_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NBS users, this code replicates previously-presented examples: library(srvyr) nbs_srvyr_design &lt;- as_survey( nbs_design ) Calculate the mean (average) of a linear variable, overall and by groups: nbs_srvyr_design %&gt;% summarize( mean = survey_mean( r7_n_totssbenlastmnth_pub , na.rm = TRUE ) ) nbs_srvyr_design %&gt;% group_by( age_categories ) %&gt;% summarize( mean = survey_mean( r7_n_totssbenlastmnth_pub , na.rm = TRUE ) ) Replication Example This example matches the percentages and t-tests from the final ten rows of Exhibit 4. ex_4 &lt;- data.frame( variable_label = c( &#39;coping with stress&#39; , &#39;concentrating&#39; , &#39;getting around outside of the home&#39; , &#39;shopping for personal items&#39; , &#39;preparing meals&#39; , &#39;getting into or out of bed&#39; , &#39;bathing or dressing&#39; , &#39;getting along with others&#39; , &#39;getting around inside the house&#39; , &#39;eating&#39; ) , variable_name = c( &quot;r3_i60_i&quot; , &quot;r3_i59_i&quot; , &quot;r3_i47_i&quot; , &quot;r3_i53_i&quot; , &quot;r3_i55_i&quot; , &quot;r3_i49_i&quot; , &quot;r3_i51_i&quot; , &quot;r3_i61_i&quot; , &quot;r3_i45_i&quot; , &quot;r3_i57_i&quot; ) , overall = c( 61 , 58 , 47 , 39 , 37 , 34 , 30 , 27 , 23 , 14 ) , di_only = c( 60 , 54 , 47 , 36 , 35 , 36 , 30 , 23 , 24 , 13 ) , concurrent = c( 63 , 63 , 47 , 43 , 41 , 34 , 33 , 31 , 23 , 15 ) , concurrent_vs_di = c( F , T , F , F , F , F , F , T , F , F ) , ssi = c( 61 , 62 , 47 , 40 , 39 , 33 , 29 , 31 , 22 , 15 ) , ssi_vs_di = c( F , T , F , F , F , F , F , T , F , F ) ) Download, import, and recode the round 3 file: r3_tf &lt;- tempfile() r3_url &lt;- &quot;https://www.ssa.gov/disabilityresearch/documents/nbsr3pufstata.zip&quot; download.file( r3_url , r3_tf , mode = &#39;wb&#39; ) r3_tbl &lt;- read_stata( r3_tf ) r3_df &lt;- data.frame( r3_tbl ) names( r3_df ) &lt;- tolower( names( r3_df ) ) r3_design &lt;- svydesign( id = ~ r3_a_psu_pub , strata = ~ r3_a_strata , weights = ~ r3_wtr3_ben , data = subset( r3_df , r3_wtr3_ben &gt; 0 ) ) r3_design &lt;- update( r3_design , benefit_type = factor( r3_orgsampinfo_bstatus , levels = c( 2 , 3 , 1 ) , labels = c( &#39;di_only&#39; , &#39;concurrent&#39; , &#39;ssi&#39; ) ) ) Calculate the final ten rows of exhibit 4 and confirm each statistics and t-test matches: for( i in seq( nrow( ex_4 ) ) ){ this_formula &lt;- as.formula( paste( &quot;~&quot; , ex_4[ i , &#39;variable_name&#39; ] ) ) overall_percent &lt;- svymean( this_formula , r3_design ) stopifnot( 100 * round( coef( overall_percent ) , 2 ) == ex_4[ i , &#39;overall_percent&#39; ] ) benefit_percent &lt;- svyby( this_formula , ~ benefit_type , r3_design , svymean ) stopifnot( all.equal( 100 * as.numeric( round( coef( benefit_percent ) , 2 ) ) , as.numeric( ex_4[ i , c( &#39;di_only&#39; , &#39;concurrent&#39; , &#39;ssi&#39; ) ] ) ) ) ttest_formula &lt;- as.formula( paste( ex_4[ i , &#39;variable_name&#39; ] , &quot;~ benefit_type&quot; ) ) di_only_con_design &lt;- subset( r3_design , benefit_type %in% c( &#39;di_only&#39; , &#39;concurrent&#39; ) ) con_ttest &lt;- svyttest( ttest_formula , di_only_con_design ) stopifnot( all.equal( as.logical( con_ttest$p.value &lt; 0.05 ) , as.logical( ex_4[ i , &#39;concurrent_vs_di&#39; ] ) ) ) di_only_ssi_design &lt;- subset( r3_design , benefit_type %in% c( &#39;di_only&#39; , &#39;ssi&#39; ) ) ssi_ttest &lt;- svyttest( ttest_formula , di_only_ssi_design ) stopifnot( all.equal( as.logical( ssi_ttest$p.value &lt; 0.05 ) , as.logical( ex_4[ i , &#39;ssi_vs_di&#39; ] ) ) ) } "],["national-health-and-nutrition-examination-survey-nhanes.html", "National Health and Nutrition Examination Survey (NHANES) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Direct Method of Age-Adjustment Replication Example", " National Health and Nutrition Examination Survey (NHANES) Doctors and dentists accompany survey interviewers in a mobile medical center that travels the country. While survey researchers read the questionnaires, medical professionals administer laboratory tests and conduct a full medical examination. The blood work and in-person check-up allow epidemiologists to answer questions like, “how many people have diabetes but don’t know they have diabetes?” Many tables containing information from the various examinations, generally one row per respondent. A complex sample survey designed to generalize to the civilian non-institutionalized U.S. population. Released biennially since 1999-2000. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: About the National Health and Nutrition Examination Survey NHANES Tutorials This poem # doctor, dentist, labs # mobile examination #vanlife interviews Download, Import, Preparation Download and import the demographics (demo) and total cholesterol laboratory (tchol) data: library(haven) nhanes_2015_2016_demo_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.XPT&quot; nhanes_2017_2018_demo_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT&quot; nhanes_2015_2016_tchol_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/TCHOL_I.XPT&quot; nhanes_2017_2018_tchol_url &lt;- &quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/TCHOL_J.XPT&quot; nhanes_2015_2016_demo_tbl &lt;- read_xpt( nhanes_2015_2016_demo_url ) nhanes_2017_2018_demo_tbl &lt;- read_xpt( nhanes_2017_2018_demo_url ) nhanes_2015_2016_tchol_tbl &lt;- read_xpt( nhanes_2015_2016_tchol_url ) nhanes_2017_2018_tchol_tbl &lt;- read_xpt( nhanes_2017_2018_tchol_url ) nhanes_2015_2016_demo_df &lt;- data.frame( nhanes_2015_2016_demo_tbl ) nhanes_2017_2018_demo_df &lt;- data.frame( nhanes_2017_2018_demo_tbl ) nhanes_2015_2016_tchol_df &lt;- data.frame( nhanes_2015_2016_tchol_tbl ) nhanes_2017_2018_tchol_df &lt;- data.frame( nhanes_2017_2018_tchol_tbl ) Specify which variables to keep from both the demo and tchol data files, then stack the four years: demo_vars &lt;- c( # unique person identifier (merge variable) &quot;SEQN&quot; , # the two-year interviewed + MEC examined weight &quot;WTMEC2YR&quot; , # note that this is a special weight for only # individuals who took the mobile examination center (MEC) exam # there is one other weight available - WTINT2YR - # that should be used when MEC variables are not part of the analysis # interviewed only or interviewed + MEC &quot;RIDSTATR&quot; , # primary sampling unit varaible, used in complex design &quot;SDMVPSU&quot; , # strata variable, used in complex design &quot;SDMVSTRA&quot; , # race / ethnicity &quot;RIDRETH3&quot; , # age &quot;RIDAGEYR&quot; , # gender &quot;RIAGENDR&quot; , # pregnant at interview &quot;RIDEXPRG&quot; ) nhanes_2015_2018_demo_df &lt;- rbind( nhanes_2015_2016_demo_df[ , demo_vars ] , nhanes_2017_2018_demo_df[ , demo_vars ] ) tchol_vars &lt;- c( # unique person identifier (merge variable) &quot;SEQN&quot; , # laboratory total cholesterol variable # https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/TCHOL_J.htm &quot;LBXTC&quot; ) nhanes_2015_2018_tchol_df &lt;- rbind( nhanes_2015_2016_tchol_df[ , tchol_vars ] , nhanes_2017_2018_tchol_df[ , tchol_vars ] ) Merge the two pooled datasets, limit the data.frame to mobile examination component respondents: nhanes_full_df &lt;- merge( nhanes_2015_2018_demo_df , nhanes_2015_2018_tchol_df , all = TRUE ) names( nhanes_full_df ) &lt;- tolower( names( nhanes_full_df ) ) nhanes_df &lt;- subset( nhanes_full_df , ridstatr %in% 2 ) Scale the mobile examination component two-year weight to generalize to the pooled, four year period: nhanes_df[ , &#39;wtmec4yr&#39; ] &lt;- nhanes_df[ , &#39;wtmec2yr&#39; ] / 2 Save locally   Save the object at any point: # nhanes_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NHANES&quot; , &quot;this_file.rds&quot; ) # saveRDS( nhanes_df , file = nhanes_fn , compress = FALSE ) Load the same object: # nhanes_df &lt;- readRDS( nhanes_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) nhanes_design &lt;- svydesign( id = ~ sdmvpsu , strata = ~ sdmvstra , nest = TRUE , weights = ~ wtmec4yr , data = nhanes_df ) Variable Recoding Add new columns to the data set: nhanes_design &lt;- update( nhanes_design , one = 1 , # define high total cholesterol as 1 if mg/dL is at or above 240 and zero otherwise. hi_tchol = ifelse( lbxtc &gt;= 240 , 1 , 0 ) , gender = factor( riagendr , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , age_categories = factor( 1 + findInterval( ridageyr , c( 20 , 40 , 60 ) ) , levels = 1:4 , labels = c( &quot;0-19&quot; , &quot;20-39&quot; , &quot;40-59&quot; , &quot;60+&quot; ) ) , # recode the ridreth3 variable as: # mexican american and other hispanic -&gt; 4 # non-hispanic white -&gt; 1 # non-hispanic black -&gt; 2 # non-hispanic asian -&gt; 3 # other race including multi-racial -&gt; 5 race_ethnicity = factor( c( 4 , 4 , 1 , 2 , NA , 3 , 5 )[ ridreth3 ] , levels = 1:5 , labels = c( &#39;nh white&#39; , &#39;nh black&#39; , &#39;nh asian&#39; , &#39;hispanic&#39; , &#39;other&#39; ) ) , pregnant_at_interview = ifelse( ridexprg %in% 1:2 , as.numeric( ridexprg == 1 ) , NA ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nhanes_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ race_ethnicity , nhanes_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nhanes_design ) svyby( ~ one , ~ race_ethnicity , nhanes_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ lbxtc , nhanes_design , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ riagendr , nhanes_design ) svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ lbxtc , nhanes_design , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ riagendr , nhanes_design ) svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ lbxtc , nhanes_design , 0.5 , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ lbxtc , denominator = ~ ridageyr , nhanes_design , na.rm = TRUE ) Subsetting Restrict the survey design to respondents aged 60 or older: sub_nhanes_design &lt;- subset( nhanes_design , age_categories == &quot;60+&quot; ) Calculate the mean (average) of this subset: svymean( ~ lbxtc , sub_nhanes_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ lbxtc , nhanes_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nhanes_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ lbxtc , nhanes_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ pregnant_at_interview , nhanes_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( lbxtc ~ pregnant_at_interview , nhanes_design ) Perform a chi-squared test of association for survey data: svychisq( ~ pregnant_at_interview + riagendr , nhanes_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( lbxtc ~ pregnant_at_interview + riagendr , nhanes_design ) summary( glm_result ) Intermish intravenous capri sun Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NHANES users, this code replicates previously-presented examples: library(srvyr) nhanes_srvyr_design &lt;- as_survey( nhanes_design ) Calculate the mean (average) of a linear variable, overall and by groups: nhanes_srvyr_design %&gt;% summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) ) nhanes_srvyr_design %&gt;% group_by( race_ethnicity ) %&gt;% summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) ) Direct Method of Age-Adjustment Replication Example This example matches the total cholesterol statistics and standard errors in Table 1 from Data Brief 363: Match the crude estimates in the footnote and also in the unadjusted age categories: crude_overall &lt;- svymean( ~ hi_tchol , subset( nhanes_design , ridageyr &gt;= 20 ) , na.rm = TRUE ) stopifnot( round( coef( crude_overall ) , 3 ) == 0.115 ) crude_by_gender &lt;- svyby( ~ hi_tchol , ~ gender , subset( nhanes_design , ridageyr &gt;= 20 ) , svymean , na.rm = TRUE ) stopifnot( round( coef( crude_by_gender )[ 1 ] , 3 ) == 0.103 ) stopifnot( round( coef( crude_by_gender )[ 2 ] , 3 ) == 0.126 ) crude_by_age &lt;- svyby( ~ hi_tchol , ~ age_categories , subset( nhanes_design , ridageyr &gt;= 20 ) , svymean , na.rm = TRUE ) stopifnot( round( coef( crude_by_age )[ 1 ] , 3 ) == 0.075 ) stopifnot( round( coef( crude_by_age )[ 2 ] , 3 ) == 0.157 ) stopifnot( round( coef( crude_by_age )[ 3 ] , 3 ) == 0.114 ) stopifnot( round( SE( crude_by_age )[ 1 ] , 3 ) == 0.005 ) stopifnot( round( SE( crude_by_age )[ 2 ] , 3 ) == 0.011 ) stopifnot( round( SE( crude_by_age )[ 3 ] , 3 ) == 0.008 ) Sum up 2000 Census totals based on the age groupings specified in footnote: pop_by_age &lt;- data.frame( age_categories = c( &quot;0-19&quot; , &quot;20-39&quot; , &quot;40-59&quot; , &quot;60+&quot; ) , Freq = c( 78782657 , 77670618 , 72816615 , 45363752 ) ) Create a design with the nationwide population stratified to the above census counts: nhanes_age_adjusted &lt;- postStratify( subset( nhanes_design , !is.na( hi_tchol ) ) , ~ age_categories , pop_by_age ) Match the overall adjusted estimates: results_overall &lt;- svymean( ~ hi_tchol , subset( nhanes_age_adjusted , ridageyr &gt;= 20 ) , na.rm = TRUE ) stopifnot( round( coef( results_overall ) , 3 ) == 0.114 ) stopifnot( round( SE( results_overall ) , 3 ) == 0.006 ) Create a design stratified to census counts broken out by gender, then match those estimates: nhanes_by_gender &lt;- svystandardize( nhanes_design , by = ~ age_categories , # stratification variable over = ~ gender , # break out variable population = pop_by_age , # data.frame containing census populations excluding.missing = ~ hi_tchol # analysis variable of interest ) results_by_gender &lt;- svyby( ~ hi_tchol , ~ gender , subset( nhanes_by_gender , ridageyr &gt;= 20 ) , svymean , na.rm=TRUE ) stopifnot( round( coef( results_by_gender )[ 1 ] , 3 ) == 0.105 ) stopifnot( round( coef( results_by_gender )[ 2 ] , 3 ) == 0.121 ) stopifnot( round( SE( results_by_gender )[ 1 ] , 3 ) == 0.007 ) stopifnot( round( SE( results_by_gender )[ 2 ] , 3 ) == 0.008 ) Create a design stratified to census counts broken out by race/ethnicity, then match those estimates: nhanes_by_race &lt;- svystandardize( nhanes_design , by = ~ age_categories , # stratification variable over = ~ race_ethnicity , # break out variable population = pop_by_age , # data.frame containing census populations excluding.missing = ~ hi_tchol # analysis variable of interest ) results_by_race_ethnicity &lt;- svyby( ~ hi_tchol , ~ race_ethnicity , design = subset( nhanes_by_race , ridageyr &gt;= 20 ) , svymean , na.rm=TRUE ) stopifnot( round( coef( results_by_race_ethnicity )[ 1 ] , 3 ) == 0.117 ) stopifnot( round( coef( results_by_race_ethnicity )[ 2 ] , 3 ) == 0.100 ) stopifnot( round( coef( results_by_race_ethnicity )[ 3 ] , 3 ) == 0.116 ) stopifnot( round( coef( results_by_race_ethnicity )[ 4 ] , 3 ) == 0.109 ) stopifnot( round( SE( results_by_race_ethnicity )[ 1 ] , 3 ) == 0.007 ) stopifnot( round( SE( results_by_race_ethnicity )[ 2 ] , 3 ) == 0.009 ) stopifnot( round( SE( results_by_race_ethnicity )[ 3 ] , 3 ) == 0.011 ) stopifnot( round( SE( results_by_race_ethnicity )[ 4 ] , 3 ) == 0.009 ) "],["national-immunization-survey-nis.html", "National Immunization Survey (NIS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " National Immunization Survey (NIS) The vaccination coverage rate tracker for national, state, and selected local areas. One table with one row per sampled toddler. A complex sample survey designed to generalize to children aged 19-35 months in the United States. Released annually since 1995, plus an adolescent (13-17 years) sample since 2008. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: About NIS National Immunization Survey-Child: A User’s Guide for the 2021 Public-Use Data File This poem # i hear babies cry # protesting lungs of iron # a wonderful world Download, Import, Preparation Download the fixed-width file: dat_tf &lt;- tempfile() dat_url &lt;- &quot;https://ftp.cdc.gov/pub/Vaccines_NIS/NISPUF21.DAT&quot; download.file( dat_url , dat_tf , mode = &#39;wb&#39; ) Edit then execute the import script provided by the CDC: library(Hmisc) r_tf &lt;- tempfile() r_script_url &lt;- &quot;https://ftp.cdc.gov/pub/Vaccines_NIS/NISPUF21.R&quot; r_input_lines &lt;- readLines( r_script_url ) # do not let the script do the save() r_input_lines &lt;- gsub( &quot;^save\\\\(&quot; , &quot;# save(&quot; , r_input_lines ) # redirect the path to the flat file to the local save location of `dat_tf` r_input_lines &lt;- gsub( &#39;\\\\&quot;path\\\\-to\\\\-file\\\\/(.*)\\\\.DAT\\\\&quot;&#39; , &quot;dat_tf&quot; , r_input_lines ) # save the edited script locally writeLines( r_input_lines , r_tf ) # run the edited script source( r_tf , echo = TRUE ) # rename the resultant data.frame object nis_df &lt;- NISPUF21 names( nis_df ) &lt;- tolower( names( nis_df ) ) nis_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # nis_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NIS&quot; , &quot;this_file.rds&quot; ) # saveRDS( nis_df , file = nis_fn , compress = FALSE ) Load the same object: # nis_df &lt;- readRDS( nis_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nis_design &lt;- svydesign( id = ~ seqnumhh , strata = ~ stratum , weights = ~ provwt_c , data = subset( nis_df , provwt_c &gt; 0 ) ) Variable Recoding Add new columns to the data set: nis_design &lt;- update( nis_design , first_fed_formula = ifelse( bf_formr20 %in% 888 , NA , bf_formr20 ) , dtap_3p = as.numeric( ( p_numdah &gt;= 3 ) | ( p_numdhi &gt;= 3 ) | ( p_numdih &gt;= 3 ) | ( p_numdta &gt;= 3 ) | ( p_numdtp &gt;= 3 ) ) , dtap_4p = as.numeric( ( p_numdah &gt;= 4 ) | ( p_numdhi &gt;= 4 ) | ( p_numdih &gt;= 4 ) | ( p_numdta &gt;= 4 ) | ( p_numdtp &gt;= 4 ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nis_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state , nis_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nis_design ) svyby( ~ one , ~ state , nis_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ first_fed_formula , nis_design , na.rm = TRUE ) svyby( ~ first_fed_formula , ~ state , nis_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , nis_design , na.rm = TRUE ) svyby( ~ sex , ~ state , nis_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ first_fed_formula , nis_design , na.rm = TRUE ) svyby( ~ first_fed_formula , ~ state , nis_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , nis_design , na.rm = TRUE ) svyby( ~ sex , ~ state , nis_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ first_fed_formula , nis_design , 0.5 , na.rm = TRUE ) svyby( ~ first_fed_formula , ~ state , nis_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ bf_exclr06 , denominator = ~ bf_endr06 , nis_design , na.rm = TRUE ) Subsetting Restrict the survey design to toddlers up to date on polio shots: sub_nis_design &lt;- subset( nis_design , p_utdpol == 1 ) Calculate the mean (average) of this subset: svymean( ~ first_fed_formula , sub_nis_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ first_fed_formula , nis_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ first_fed_formula , ~ state , nis_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nis_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ first_fed_formula , nis_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ first_fed_formula , nis_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ first_fed_formula , nis_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ dtap_3p , nis_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( first_fed_formula ~ dtap_3p , nis_design ) Perform a chi-squared test of association for survey data: svychisq( ~ dtap_3p + sex , nis_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( first_fed_formula ~ dtap_3p + sex , nis_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NIS users, this code replicates previously-presented examples: library(srvyr) nis_srvyr_design &lt;- as_survey( nis_design ) Calculate the mean (average) of a linear variable, overall and by groups: nis_srvyr_design %&gt;% summarize( mean = survey_mean( first_fed_formula , na.rm = TRUE ) ) nis_srvyr_design %&gt;% group_by( state ) %&gt;% summarize( mean = survey_mean( first_fed_formula , na.rm = TRUE ) ) Replication Example This example matches the statistics and standard errors from Data User’s Guide Table 4: results &lt;- svyby( ~ p_utd431h314_rout_s , ~ raceethk , nis_design , svymean ) coefficients &lt;- results[ , &quot;p_utd431h314_rout_sUTD&quot; , drop = FALSE ] standard_errors &lt;- results[ , &quot;se.p_utd431h314_rout_sUTD&quot; , drop = FALSE ] stopifnot( round( coefficients[ &quot;HISPANIC&quot; , ] , 3 ) == .711 ) stopifnot( round( coefficients[ &quot;NON-HISPANIC WHITE ONLY&quot; , ] , 3 ) == .742 ) stopifnot( round( coefficients[ &quot;NON-HISPANIC BLACK ONLY&quot; , ] , 3 ) == .647 ) stopifnot( round( standard_errors[ &quot;HISPANIC&quot; , ] , 3 ) == .015 ) stopifnot( round( standard_errors[ &quot;NON-HISPANIC WHITE ONLY&quot; , ] , 3 ) == .009 ) stopifnot( round( standard_errors[ &quot;NON-HISPANIC BLACK ONLY&quot; , ] , 3 ) == .022 ) "],["national-plan-and-provider-enumeration-system-nppes.html", "National Plan and Provider Enumeration System (NPPES) Download, Import, Preparation Analysis Examples with base R   Intermish Analysis Examples with dplyr  ", " National Plan and Provider Enumeration System (NPPES) The registry of every medical practitioner actively operating in the United States healthcare industry. A single large table with one row per enumerated health care provider. A census of individuals and organizations that bill for medical services in the United States. Updated weekly with new providers. Maintained by the United States Centers for Medicare &amp; Medicaid Services (CMS) Please skim before you begin: NPI: What You Need To Know Wikipedia Entry This poem # how many doctors # ranked sergeant, last name pepper # practice in the states? Download, Import, Preparation Download and import the national file: library(readr) tf &lt;- tempfile() npi_datapage &lt;- readLines( &quot;http://download.cms.gov/nppes/NPI_Files.html&quot; ) latest_files &lt;- grep( &#39;NPPES_Data_Dissemination_&#39; , npi_datapage , value = TRUE ) latest_files &lt;- latest_files[ !grepl( &#39;Weekly Update&#39; , latest_files ) ] this_url &lt;- paste0( &quot;http://download.cms.gov/nppes/&quot;, gsub( &quot;(.*)(NPPES_Data_Dissemination_.*\\\\.zip)(.*)$&quot;, &quot;\\\\2&quot;, latest_files ) ) download.file( this_url , tf , mode = &#39;wb&#39; ) npi_files &lt;- unzip( tf , exdir = tempdir() ) npi_filepath &lt;- grep( &quot;npidata_pfile_20050523-([0-9]+)\\\\.csv&quot; , npi_files , value = TRUE ) column_names &lt;- names( read.csv( npi_filepath , nrow = 1 )[ FALSE , , ] ) column_names &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , tolower( column_names ) ) column_types &lt;- ifelse( grepl( &quot;code&quot; , column_names ) &amp; !grepl( &quot;country|state|gender|taxonomy|postal&quot; , column_names ) , &#39;n&#39; , &#39;c&#39; ) columns_to_import &lt;- c( &quot;entity_type_code&quot; , &quot;provider_gender_code&quot; , &quot;provider_enumeration_date&quot; , &quot;is_sole_proprietor&quot; , &quot;provider_business_practice_location_address_state_name&quot; ) stopifnot( all( columns_to_import %in% column_names ) ) # readr::read_csv() columns must match their order in the csv file columns_to_import &lt;- columns_to_import[ order( match( columns_to_import , column_names ) ) ] nppes_tbl &lt;- readr::read_csv( npi_filepath , col_names = columns_to_import , col_types = paste0( ifelse( column_names %in% columns_to_import , column_types , &#39;_&#39; ) , collapse = &quot;&quot; ) , skip = 1 ) nppes_df &lt;- data.frame( nppes_tbl ) Save locally   Save the object at any point: # nppes_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NPPES&quot; , &quot;this_file.rds&quot; ) # saveRDS( nppes_df , file = nppes_fn , compress = FALSE ) Load the same object: # nppes_df &lt;- readRDS( nppes_fn ) Analysis Examples with base R   Variable Recoding Add new columns to the data set: nppes_df &lt;- transform( nppes_df , individual = as.numeric( entity_type_code ) , provider_enumeration_year = as.numeric( substr( provider_enumeration_date , 7 , 10 ) ) , state_name = provider_business_practice_location_address_state_name ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( nppes_df ) table( nppes_df[ , &quot;provider_gender_code&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( nppes_df[ , &quot;is_sole_proprietor&quot; ] ) ) prop.table( table( nppes_df[ , c( &quot;is_sole_proprietor&quot; , &quot;provider_gender_code&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( nppes_df[ , &quot;provider_enumeration_year&quot; ] , 0.5 , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to California: sub_nppes_df &lt;- subset( nppes_df , state_name = &#39;CA&#39; ) Calculate the mean (average) of this subset: mean( sub_nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( provider_enumeration_year ~ individual , nppes_df ) Perform a chi-squared test of association: this_table &lt;- table( nppes_df[ , c( &quot;individual&quot; , &quot;is_sole_proprietor&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( provider_enumeration_year ~ individual + is_sole_proprietor , data = nppes_df ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for NPPES users, this code replicates previously-presented examples: library(dplyr) nppes_tbl &lt;- as_tibble( nppes_df ) Calculate the mean (average) of a linear variable, overall and by groups: nppes_tbl %&gt;% summarize( mean = mean( provider_enumeration_year , na.rm = TRUE ) ) nppes_tbl %&gt;% group_by( provider_gender_code ) %&gt;% summarize( mean = mean( provider_enumeration_year , na.rm = TRUE ) ) "],["national-survey-on-drug-use-and-health-nsduh.html", "National Survey on Drug Use and Health (NSDUH) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " National Survey on Drug Use and Health (NSDUH) The primary survey to measure of prevalence of substance use and its correlates in the United States. One table with one row per sampled respondent. A complex survey designed to generalize to civilian, non-institutional americans aged 12 and older. Released periodically since 1979 and annually since 1990. Administered by the Substance Abuse and Mental Health Services Administration. Please skim before you begin: 2021 National Survey on Drug Use and Health (NSDUH): Public Use File Codebook 2021 National Survey on Drug Use and Health (NSDUH): Methodological Summary and Definitions This poem # drinking and thinking # about your first time, were you # smoking and joking? Download, Import, Preparation Download and import the national file: zip_tf &lt;- tempfile() zip_url &lt;- paste0( &quot;https://www.datafiles.samhsa.gov/sites/default/files/field-uploads-protected/&quot; , &quot;studies/NSDUH-2021/NSDUH-2021-datasets/NSDUH-2021-DS0001/&quot; , &quot;NSDUH-2021-DS0001-bundles-with-study-info/NSDUH-2021-DS0001-bndl-data-r_v1.zip&quot; ) download.file( zip_url , zip_tf , mode = &#39;wb&#39; ) nsduh_rdata &lt;- unzip( zip_tf , exdir = tempdir() ) nsduh_rdata_contents &lt;- load( nsduh_rdata ) nsduh_df_name &lt;- grep( &#39;PUF&#39; , nsduh_rdata_contents , value = TRUE ) nsduh_df &lt;- get( nsduh_df_name ) names( nsduh_df ) &lt;- tolower( names( nsduh_df ) ) nsduh_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # nsduh_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NSDUH&quot; , &quot;this_file.rds&quot; ) # saveRDS( nsduh_df , file = nsduh_fn , compress = FALSE ) Load the same object: # nsduh_df &lt;- readRDS( nsduh_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) nsduh_design &lt;- svydesign( id = ~ verep , strata = ~ vestr_c , data = nsduh_df , weights = ~ analwt_c , nest = TRUE ) Variable Recoding Add new columns to the data set: nsduh_design &lt;- update( nsduh_design , one = 1 , health = factor( health , levels = 1:5 , labels = c( &quot;excellent&quot; , &quot;very good&quot; , &quot;good&quot; , &quot;fair&quot; , &quot;poor&quot; ) ) , age_first_cigarette = ifelse( cigtry &gt; 99 , NA , cigtry ) , age_tried_cocaine = ifelse( cocage &gt; 99 , NA , cocage ) , ever_used_marijuana = as.numeric( ifelse( mjever &lt; 4 , mjever == 1 , NA ) ) , county_type = factor( coutyp4 , levels = 1:3 , labels = c( &quot;large metro&quot; , &quot;small metro&quot; , &quot;nonmetro&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nsduh_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ county_type , nsduh_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nsduh_design ) svyby( ~ one , ~ county_type , nsduh_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ health , nsduh_design , na.rm = TRUE ) svyby( ~ health , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ health , nsduh_design , na.rm = TRUE ) svyby( ~ health , ~ county_type , nsduh_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ age_first_cigarette , nsduh_design , 0.5 , na.rm = TRUE ) svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ age_first_cigarette , denominator = ~ age_tried_cocaine , nsduh_design , na.rm = TRUE ) Subsetting Restrict the survey design to individuals who are pregnant: sub_nsduh_design &lt;- subset( nsduh_design , preg == 1 ) Calculate the mean (average) of this subset: svymean( ~ age_first_cigarette , sub_nsduh_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ age_first_cigarette , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsduh_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ age_first_cigarette , nsduh_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ age_first_cigarette , nsduh_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ ever_used_marijuana , nsduh_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( age_first_cigarette ~ ever_used_marijuana , nsduh_design ) Perform a chi-squared test of association for survey data: svychisq( ~ ever_used_marijuana + health , nsduh_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( age_first_cigarette ~ ever_used_marijuana + health , nsduh_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NSDUH users, this code replicates previously-presented examples: library(srvyr) nsduh_srvyr_design &lt;- as_survey( nsduh_design ) Calculate the mean (average) of a linear variable, overall and by groups: nsduh_srvyr_design %&gt;% summarize( mean = survey_mean( age_first_cigarette , na.rm = TRUE ) ) nsduh_srvyr_design %&gt;% group_by( county_type ) %&gt;% summarize( mean = survey_mean( age_first_cigarette , na.rm = TRUE ) ) Replication Example This matches the prevalence and SE of alcohol use in the past month from Codebook Table G.2: result &lt;- svymean( ~ alcmon , nsduh_design ) stopifnot( round( coef( result ) , 3 ) == 0.474 ) stopifnot( round( SE( result ) , 4 ) == 0.0043 ) "],["national-survey-of-family-growth-nsfg.html", "National Survey of Family Growth (NSFG) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " National Survey of Family Growth (NSFG) The principal survey to measure reproductive behavior in the United States population. Multiple tables with one row per respondent for the female and male tables, then a separate table with one row per pregnancy. A complex sample survey designed to generalize to the 15-49 year old population of the United States, by gender. Released every couple of years since 1973. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: Sample Design Documentation Wikipedia Entry This poem # family structure # questions cuz radar fails at # storks with bassinets Download, Import, Preparation library(SAScii) library(readr) dat_url &lt;- &quot;https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NSFG/2017_2019_FemRespData.dat&quot; sas_url &lt;- file.path( dirname( dat_url ) , &quot;sas/2017_2019_FemRespSetup.sas&quot; ) sas_positions &lt;- parse.SAScii( sas_url ) sas_positions[ , &#39;varname&#39; ] &lt;- tolower( sas_positions[ , &#39;varname&#39; ] ) sas_positions[ , &#39;column_types&#39; ] &lt;- ifelse( sas_positions[ , &#39;char&#39; ] , &quot;c&quot; , &quot;d&quot; ) nsfg_tbl &lt;- read_fwf( dat_url , fwf_widths( abs( sas_positions[ , &#39;width&#39; ] ) , col_names = sas_positions[ , &#39;varname&#39; ] ) , col_types = paste0( sas_positions[ , &#39;column_types&#39; ] , collapse = &quot;&quot; ) , na = c( &quot;&quot; , &quot;.&quot; ) ) nsfg_df &lt;- data.frame( nsfg_tbl ) Save locally   Save the object at any point: # nsfg_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NSFG&quot; , &quot;this_file.rds&quot; ) # saveRDS( nsfg_df , file = nsfg_fn , compress = FALSE ) Load the same object: # nsfg_df &lt;- readRDS( nsfg_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nsfg_design &lt;- svydesign( id = ~ secu , strata = ~ sest , data = nsfg_df , weights = ~ wgt2017_2019 , nest = TRUE ) Variable Recoding Add new columns to the data set: nsfg_design &lt;- update( nsfg_design , one = 1 , birth_control_pill = as.numeric( constat1 == 6 ) , age_categories = factor( findInterval( ager , c( 15 , 20 , 25 , 30 , 35 , 40 ) ) , labels = c( &#39;15-19&#39; , &#39;20-24&#39; , &#39;25-29&#39; , &#39;30-34&#39; , &#39;35-39&#39; , &#39;40-49&#39; ) ) , marstat = factor( marstat , levels = c( 1:6 , 8:9 ) , labels = c( &quot;Married to a person of the opposite sex&quot; , &quot;Not married but living together with a partner of the opposite sex&quot; , &quot;Widowed&quot; , &quot;Divorced or annulled&quot; , &quot;Separated, because you and your spouse are not getting along&quot; , &quot;Never been married&quot; , &quot;Refused&quot; , &quot;Don&#39;t know&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nsfg_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_categories , nsfg_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nsfg_design ) svyby( ~ one , ~ age_categories , nsfg_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ pregnum , nsfg_design , na.rm = TRUE ) svyby( ~ pregnum , ~ age_categories , nsfg_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ marstat , nsfg_design ) svyby( ~ marstat , ~ age_categories , nsfg_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ pregnum , nsfg_design , na.rm = TRUE ) svyby( ~ pregnum , ~ age_categories , nsfg_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ marstat , nsfg_design ) svyby( ~ marstat , ~ age_categories , nsfg_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ pregnum , nsfg_design , 0.5 , na.rm = TRUE ) svyby( ~ pregnum , ~ age_categories , nsfg_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ pregnum , denominator = ~ lbpregs , nsfg_design , na.rm = TRUE ) Subsetting Restrict the survey design to ever cohabited: sub_nsfg_design &lt;- subset( nsfg_design , timescoh &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ pregnum , sub_nsfg_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ pregnum , nsfg_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ pregnum , ~ age_categories , nsfg_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsfg_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ pregnum , nsfg_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ pregnum , nsfg_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ pregnum , nsfg_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ birth_control_pill , nsfg_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( pregnum ~ birth_control_pill , nsfg_design ) Perform a chi-squared test of association for survey data: svychisq( ~ birth_control_pill + marstat , nsfg_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( pregnum ~ birth_control_pill + marstat , nsfg_design ) summary( glm_result ) Intermish wilma’s revenge Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NSFG users, this code replicates previously-presented examples: library(srvyr) nsfg_srvyr_design &lt;- as_survey( nsfg_design ) Calculate the mean (average) of a linear variable, overall and by groups: nsfg_srvyr_design %&gt;% summarize( mean = survey_mean( pregnum , na.rm = TRUE ) ) nsfg_srvyr_design %&gt;% group_by( age_categories ) %&gt;% summarize( mean = survey_mean( pregnum , na.rm = TRUE ) ) Replication Example This example matches the Variance Estimates for Percentages using SAS (9.4) and STATA (14): Match the sum of the weights: result &lt;- svytotal( ~ one , nsfg_design ) stopifnot( round( coef( result ) , 0 ) == 72671926 ) stopifnot( round( SE( result ) , 0 ) == 3521465 ) Match row percentages of women currently using the pill by age: row_percents &lt;- c( 19.5112 , 23.7833 , 19.6916 , 15.2800 , 6.4965 , 6.5215 ) std_err_row_percents &lt;- c( 1.8670 , 2.1713 , 2.2773 , 1.7551 , 0.9895 , 1.0029 ) results &lt;- svyby( ~ birth_control_pill , ~ age_categories , nsfg_design , svymean ) stopifnot( all( round( coef( results ) * 100 , 4 ) == row_percents ) ) stopifnot( all( round( SE( results ) * 100 , 4 ) == std_err_row_percents ) ) "],["programme-for-the-international-assessment-of-adult-competencies-piaac.html", "Programme for the International Assessment of Adult Competencies (PIAAC) Download, Import, Preparation Analysis Examples with the survey library   Intermish Replication Example", " Programme for the International Assessment of Adult Competencies (PIAAC) A cross-national study designed to understand the skills of workers in advanced-nation labor markets. One row per sampled adult. A multiply-imputed, complex sample survey designed to generalize to the population aged 16 to 65 across thirty three OECD nations. No expected release timeline. Administered by the Organisation for Economic Co-operation and Development. Please skim before you begin: Technical Report of the Survey of Adult Skills Wikipedia Entry This poem # what color collar # workforce poets, potters, or # pythagoreans Download, Import, Preparation library(haven) library(httr) tf &lt;- tempfile() this_url &lt;- &quot;https://webfs.oecd.org/piaac/puf-data/SAS/SAS7BDAT/prgusap1_2012.sas7bdat&quot; GET( this_url , write_disk( tf ) ) piaac_tbl &lt;- read_sas( tf ) piaac_df &lt;- data.frame( piaac_tbl ) names( piaac_df ) &lt;- tolower( names( piaac_df ) ) Save locally   Save the object at any point: # piaac_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;PIAAC&quot; , &quot;this_file.rds&quot; ) # saveRDS( piaac_df , file = piaac_fn , compress = FALSE ) Load the same object: # piaac_df &lt;- readRDS( piaac_fn ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) pvals &lt;- c( &quot;pvlit&quot; , &quot;pvnum&quot; , &quot;pvpsl&quot; ) pvars &lt;- outer( pvals , 1:10 , paste0 ) non.pvals &lt;- names(piaac_df)[ !( names(piaac_df) %in% pvars ) ] for(k in 1:10){ piaac_imp &lt;- piaac_df[ , c( non.pvals , paste0( pvals , k ) ) ] for( j in pvals ){ piaac_imp[ , j ] &lt;- piaac_imp[ , paste0( j , k ) ] piaac_imp[ , paste0( j , k ) ] &lt;- NULL } if( k == 1 ){ piaac_mi &lt;- list( piaac_imp ) } else { piaac_mi &lt;- c( piaac_mi , list( piaac_imp ) ) } } jk.method &lt;- unique( piaac_df[ , &#39;vemethod&#39; ] ) stopifnot(length(jk.method) == 1) stopifnot(jk.method %in% c(&quot;JK1&quot;, &quot;JK2&quot;)) if (jk.method == &quot;JK2&quot;) jk.method &lt;- &quot;JKn&quot; piaac_design &lt;- svrepdesign( weights = ~spfwt0 , repweights = &quot;spfwt[1-9]&quot; , rscales = rep( 1 , 80 ) , scale = ifelse( jk.method == &quot;JKn&quot; , 1 , 79/80 ) , type = jk.method , data = imputationList( piaac_mi ) , mse = TRUE ) Variable Recoding Add new columns to the data set: piaac_design &lt;- update( piaac_design , one = 1 , sex = factor( gender_r , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , age_categories = factor( ageg10lfs , levels = 1:5 , labels = c( &quot;24 or less&quot; , &quot;25-34&quot; , &quot;35-44&quot; , &quot;45-54&quot; , &quot;55 plus&quot; ) ) , working_at_paid_job_last_week = as.numeric( c_q01a == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( piaac_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( piaac_design , svyby( ~ one , ~ age_categories , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ one ) ) ) MIcombine( with( piaac_design , svyby( ~ one , ~ age_categories , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( piaac_design , svymean( ~ sex ) ) ) MIcombine( with( piaac_design , svyby( ~ sex , ~ age_categories , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ pvnum , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svytotal , na.rm = TRUE ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ sex ) ) ) MIcombine( with( piaac_design , svyby( ~ sex , ~ age_categories , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( piaac_design , svyquantile( ~ pvnum , 0.5 , se = TRUE , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svyquantile , 0.5 , se = TRUE , ci = TRUE , na.rm = TRUE ) ) ) Estimate a ratio: MIcombine( with( piaac_design , svyratio( numerator = ~ pvnum , denominator = ~ pvlit , na.rm = TRUE ) ) ) Subsetting Restrict the survey design to self-reported fair or poor health: sub_piaac_design &lt;- subset( piaac_design , i_q08 %in% 4:5 ) Calculate the mean (average) of this subset: MIcombine( with( sub_piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( piaac_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( piaac_design , svyvar( ~ pvnum , na.rm = TRUE ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: # MIsvyciprop( ~ working_at_paid_job_last_week , piaac_design , # method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: # MIsvyttest( pvnum ~ working_at_paid_job_last_week , piaac_design ) Perform a chi-squared test of association for survey data: # MIsvychisq( ~ working_at_paid_job_last_week + sex , piaac_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( piaac_design , svyglm( pvnum ~ working_at_paid_job_last_week + sex ) ) ) summary( glm_result ) Intermish office worker landfill Replication Example This example matches the statistics and standard errors from OECD’s Technical Report Table 18.9. usa_pvlit &lt;- MIcombine( with( piaac_design , svymean( ~ pvlit , na.rm = TRUE ) ) ) usa_pvnum &lt;- MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) usa_pvpsl &lt;- MIcombine( with( piaac_design , svymean( ~ pvpsl , na.rm = TRUE ) ) ) stopifnot( round( coef( usa_pvlit ) ) == 270 ) stopifnot( round( SE( usa_pvlit ) , 1 ) == 1.0 ) stopifnot( round( coef( usa_pvnum ) ) == 253 ) stopifnot( round( SE( usa_pvnum ) , 1 ) == 1.2 ) stopifnot( round( coef( usa_pvpsl ) ) == 277 ) stopifnot( round( SE( usa_pvpsl ) , 1 ) == 1.1 ) "],["social-security-public-use-data-files-ssa.html", "Social Security Public-Use Data Files (SSA) Download, Import, Preparation Analysis Examples with base R   Intermish Analysis Examples with dplyr   Replication Example", " Social Security Public-Use Data Files (SSA) Microdata from administrative sources like the Master Beneficiary Record, Supplemental Security Record. Tables contain either one record per person or one record per person per year. A systematic sample of either social security number holders (most americans) or program recipients (current beneficiaries). Multiply 1% samples by 100 to get weighted statistics, 5% samples by 20. No expected release timeline. Released by the Office of Research, Evaluation, and Statistics, US Social Security Administration. Please skim before you begin: The 2006 Earnings Public-Use Microdata File: An Introduction Comparing Earnings Estimates from the 2006 Public-Use File and the Annual Statistical Supplement This poem # annual earnings. # for pensioner payouts, see # the &#39;04 extract Download, Import, Preparation Download and import the 1951-2006 one percent files with one record per person and per person-year: library(haven) library(httr) tf &lt;- tempfile() ssa_url &lt;- &quot;https://www.ssa.gov/policy/docs/microdata/epuf/epuf2006_sas_files.zip&quot; GET( ssa_url , write_disk( tf ) ) ssa_files &lt;- unzip( tf , exdir = tempdir() ) ssa_fn &lt;- grep( &#39;demographic&#39; , ssa_files , value = TRUE ) annual_fn &lt;- grep( &#39;annual&#39; , ssa_files , value = TRUE ) ssa_tbl &lt;- read_sas( ssa_fn ) annual_tbl &lt;- read_sas( annual_fn ) ssa_df &lt;- data.frame( ssa_tbl ) annual_df &lt;- data.frame( annual_tbl ) names( ssa_df ) &lt;- tolower( names( ssa_df ) ) names( annual_df ) &lt;- tolower( names( annual_df ) ) Sum up 1951-1952 and 1953-2006 earnings, and also 1953-2006 credits, copying the naming convention: summed_earnings_5152 &lt;- with( subset( annual_df , year_earn %in% 1951:1952 ) , aggregate( annual_earnings , list( id ) , sum ) ) names( summed_earnings_5152 ) &lt;- c( &#39;id&#39; , &#39;tot_cov_earn5152&#39; ) summed_earnings_5306 &lt;- with( subset( annual_df , year_earn &gt; 1952 ) , aggregate( annual_earnings , list( id ) , sum ) ) names( summed_earnings_5306 ) &lt;- c( &#39;id&#39; , &#39;tot_cov_earn5306&#39; ) summed_quarters_5306 &lt;- with( subset( annual_df , year_earn &gt; 1952 ) , aggregate( annual_qtrs , list( id ) , sum ) ) names( summed_quarters_5306 ) &lt;- c( &#39;id&#39; , &#39;qc5306&#39; ) Isolate a single year of earnings: earnings_2006 &lt;- annual_df[ annual_df[ , &#39;year_earn&#39; ] == 2006 , c( &#39;id&#39; , &#39;annual_earnings&#39; ) ] names( earnings_2006 ) &lt;- c( &#39;id&#39; , &#39;tot_cov_earn06&#39; ) Merge each new column on to the person-level table, then add zeroes to person-years without earnings: stopifnot( all( !is.na( ssa_df ) ) ) before_nrow &lt;- nrow( ssa_df ) ssa_df &lt;- merge( ssa_df , summed_earnings_5152 , all.x = TRUE ) ssa_df &lt;- merge( ssa_df , summed_earnings_5306 , all.x = TRUE ) ssa_df &lt;- merge( ssa_df , summed_quarters_5306 , all.x = TRUE ) ssa_df &lt;- merge( ssa_df , earnings_2006 , all.x = TRUE ) ssa_df[ is.na( ssa_df ) ] &lt;- 0 stopifnot( nrow( ssa_df ) == before_nrow ) Save locally   Save the object at any point: # ssa_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;SSA&quot; , &quot;this_file.rds&quot; ) # saveRDS( ssa_df , file = ssa_fn , compress = FALSE ) Load the same object: # ssa_df &lt;- readRDS( ssa_fn ) Analysis Examples with base R   Variable Recoding Add new columns to the data set: ssa_df &lt;- transform( ssa_df , decade_of_birth = floor( yob / 10 ) * 10 , sex = factor( sex , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , tot_cov_earn3706 = ( tot_cov_earn3750 + tot_cov_earn5152 + tot_cov_earn5306 ) , qc3706 = ( qc3750 + qc5152 + qc5306 ) , any_earnings_2006 = ( tot_cov_earn06 &gt; 0 ) , earnings_periods = factor( ifelse( ( tot_cov_earn5152 + tot_cov_earn5306 &gt; 0 ) &amp; tot_cov_earn3750 &gt; 0 , 1 , ifelse( tot_cov_earn5152 &gt; 0 | tot_cov_earn5306 &gt; 0 , 2 , ifelse( tot_cov_earn3750 &gt; 0 , 3 , 4 ) ) ) , levels = 1:4 , labels = c( &#39;Earnings in both periods&#39; , &#39;Earnings during 1951-2006 only&#39; , &#39;Earnings during 1937-1950 only&#39; , &#39;No earnings&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( ssa_df ) table( ssa_df[ , &quot;sex&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( ssa_df[ , &quot;decade_of_birth&quot; ] ) ) prop.table( table( ssa_df[ , c( &quot;decade_of_birth&quot; , &quot;sex&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , 0.5 ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to individuals with at least forty lifetime credits: sub_ssa_df &lt;- subset( ssa_df , qc3706 &gt;= 40 ) Calculate the mean (average) of this subset: mean( sub_ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( ssa_df[ , &quot;tot_cov_earn3706&quot; ] ) tapply( ssa_df[ , &quot;tot_cov_earn3706&quot; ] , ssa_df[ , &quot;sex&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( tot_cov_earn3706 ~ any_earnings_2006 , ssa_df ) Perform a chi-squared test of association: this_table &lt;- table( ssa_df[ , c( &quot;any_earnings_2006&quot; , &quot;decade_of_birth&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( tot_cov_earn3706 ~ any_earnings_2006 + decade_of_birth , data = ssa_df ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for SSA users, this code replicates previously-presented examples: library(dplyr) ssa_tbl &lt;- as_tibble( ssa_df ) Calculate the mean (average) of a linear variable, overall and by groups: ssa_tbl %&gt;% summarize( mean = mean( tot_cov_earn3706 ) ) ssa_tbl %&gt;% group_by( sex ) %&gt;% summarize( mean = mean( tot_cov_earn3706 ) ) Replication Example This example matches statistics in The 2006 Earnings Public-Use Microdata File: An Introduction: Chart 5. Percentage distribution of individuals in EPUF, by capped Social Security taxable earnings status: chart_five_results &lt;- prop.table( table( ssa_df[ , &#39;earnings_periods&#39; ] ) ) chart_five_results &lt;- round( 100 * chart_five_results ) stopifnot( chart_five_results[ &#39;Earnings in both periods&#39; ] == 16 ) stopifnot( chart_five_results[ &#39;Earnings during 1951-2006 only&#39; ] == 55 ) stopifnot( chart_five_results[ &#39;Earnings during 1937-1950 only&#39; ] == 4 ) stopifnot( chart_five_results[ &#39;No earnings&#39; ] == 25 ) Table 4. Average and median Social Security taxable earnings in EPUF, by sex, 1951–2006 (in dollars): nonzero_2006_earners &lt;- ssa_df[ ssa_df[ , &#39;tot_cov_earn06&#39; ] &gt; 0 , &#39;tot_cov_earn06&#39; ] stopifnot( round( mean( nonzero_2006_earners ) , 0 ) == 30953 ) stopifnot( round( quantile( nonzero_2006_earners )[ 3 ] , 0 ) == 24000 ) Table A1. Number and percentage distribution of individuals with Social Security taxable earnings records in EPUF, by sex, 1951–2006: nonzero_2006_earners &lt;- ssa_df[ ssa_df[ , &#39;tot_cov_earn06&#39; ] &gt; 0 , ] stopifnot( round( mean( nonzero_2006_earners[ , &#39;tot_cov_earn06&#39; ] ) , 0 ) == 30953 ) stopifnot( round( quantile( nonzero_2006_earners[ , &#39;tot_cov_earn06&#39; ] )[ 3 ] , 0 ) == 24000 ) This example matches statistics in Comparing Earnings Estimates from the 2006 Earnings Public-Use File and the Annual Statistical Supplement: Table 4. Comparing Supplement and EPUF estimates: Number of all, male, and female workers with any earnings during the year, 1951–2006: stopifnot( round( nrow( nonzero_2006_earners ) * 100 , -3 ) == 156280000 ) earners_in_2006_by_sex &lt;- table( nonzero_2006_earners[ , &#39;sex&#39; ] ) * 100 stopifnot( round( earners_in_2006_by_sex[ &#39;male&#39; ] , -3 ) == 81576000 ) stopifnot( round( earners_in_2006_by_sex[ &#39;female&#39; ] , -3 ) == 74681000 ) "],["trend-analysis-of-complex-survey-data.html", "Trend Analysis of Complex Survey Data Download, Import, Preparation Append Polynomials to Each Year Unadjusted Analysis Examples Calculate Joinpoints Needed Calculate Predicted Marginals Identify Joinpoint(s) or Breakpoint(s) Interpret and Conclude", " Trend Analysis of Complex Survey Data Contributed by Thomas Yokota &lt;thomasyokota@gmail.com&gt; Professor Vito Muggeo wrote the joinpoint analysis section of the code below and the segmented package. Professor Dr. Thomas Lumley wrote the svypredmeans function to replicate SUDAAN’s PREDMARG command and match the CDC to the decimal. Dr. Richard Lowry, M.D. at the Centers for Disease Control &amp; Prevention wrote the original linear trend analysis and then answered our infinite questions. Thanks to everyone. The purpose of this analysis is to make statistically valid statements such as, “there was a significant linear decrease in the prevalence of high school aged americans who have ever smoked a cigarette across the period 1999-2011” with complex sample survey data. This step-by-step walkthrough exactly reproduces the statistics presented in the Center for Disease Control &amp; Prevention’s (CDC) linear trend analysis. This analysis may complement qualitative evaluation on prevalence changes observed from surveillance data by providing quantitative evidence, such as when a joinpoint (also called breakpoint or changepoint) occurred; however, this analysis does not explain why or how changes in trends occur. Download, Import, Preparation Download and import the multi-year stacked file: library(SAScii) library(readr) sas_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/sadc_2019/2019-SADC-SAS-Input-Program.sas&quot; dat_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/sadc_2019/sadc_2019_national.dat&quot; sas_positions &lt;- parse.SAScii( sas_url ) sas_positions[ , &#39;varname&#39; ] &lt;- tolower( sas_positions[ , &#39;varname&#39; ] ) variables_to_keep &lt;- c( &quot;sex&quot; , &quot;grade&quot; , &quot;race4&quot; , &quot;q30&quot; , &quot;year&quot; , &quot;psu&quot; , &quot;stratum&quot; , &quot;weight&quot; ) sas_positions[ , &#39;column_types&#39; ] &lt;- ifelse( !( sas_positions[ , &#39;varname&#39; ] %in% variables_to_keep ) , &quot;_&quot; , ifelse( sas_positions[ , &#39;char&#39; ] , &quot;c&quot; , &quot;d&quot; ) ) yrbss_tbl &lt;- read_fwf( dat_url , fwf_widths( abs( sas_positions[ , &#39;width&#39; ] ) , col_names = sas_positions[ , &#39;varname&#39; ] ) , col_types = paste0( sas_positions[ , &#39;column_types&#39; ] , collapse = &quot;&quot; ) , na = c( &quot;&quot; , &quot;.&quot; ) ) yrbss_df &lt;- data.frame( yrbss_tbl ) Restrict the dataset to only years shown in the original analysis and re-name the main variable: yrbss_df &lt;- subset( yrbss_df , year %in% seq( 1991 , 2011 , 2 ) ) yrbss_df[ , &#39;ever_smoked&#39; ] &lt;- as.numeric( yrbss_df[ , &#39;q30&#39; ] == 1 ) yrbss_df[ , &#39;q30&#39; ] &lt;- NULL Recode each categorical variable to factor class: yrbss_df[ , &#39;sex&#39; ] &lt;- relevel( factor( yrbss_df[ , &#39;sex&#39; ] ) , ref = &quot;2&quot; ) for ( i in c( &#39;race4&#39; , &#39;grade&#39; ) ){ yrbss_df[ , i ] &lt;- relevel( factor( yrbss_df[ , i ] ) , ref = &quot;1&quot; ) } Append Polynomials to Each Year “The polynomials we have used as predictors to this point are natural polynomials, generated from the linear predictor by centering and then powering the linear predictor.” For more detail on this subject, see page 216 of Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences By Jacob Cohen, Patricia Cohen, Stephen G. West, Leona S. Aiken distinct_years_available &lt;- length( seq( 1991 , 2011 , 2 ) ) # store the linear polynomials c11l &lt;- contr.poly( distinct_years_available )[ , &quot;.L&quot; ] # store the quadratic polynomials c11q &lt;- contr.poly( distinct_years_available )[ , &quot;.Q&quot; ] # store the cubic polynomials c11c &lt;- contr.poly( distinct_years_available )[ , &quot;.C&quot; ] For each record in the data set, tack on the linear, quadratic, and cubic contrast value, these contrast values will serve as replacement for the linear year variable in any regression: # year^1 term (linear) yrbss_df[ , &quot;t11l&quot; ] &lt;- c11l[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] # year^2 term (quadratic) yrbss_df[ , &quot;t11q&quot; ] &lt;- c11q[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] # year^3 term (cubic) yrbss_df[ , &quot;t11c&quot; ] &lt;- c11c[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] Unadjusted Analysis Examples Construct a complex sample survey design and match the published unadjusted prevalence rates: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) des &lt;- svydesign( id = ~psu , strata = ~interaction( stratum , year ) , data = yrbss_df , weights = ~weight , nest = TRUE ) prevalence_over_time &lt;- svyby( ~ ever_smoked , ~ year , des , svymean , na.rm = TRUE ) # confirm prevalence rates match published estimates # of high school students that ever smoked stopifnot( all.equal( round( coef( prevalence_over_time ) , 3 ) , c( .701 , .695 , .713 , .702 , .704 , .639 , .584 , .543 , .503 , .463 , .447 ) , check.attributes = FALSE ) ) Calculate Joinpoints Needed Using the orthogonal coefficients (linear, quadratic, cubic terms) that we previously added to our yrbss_df object before constructing the multi-year stacked survey design, determine how many joinpoints will be needed for a trend analysis. Epidemiological models typically control for possible confounding variables such as age, sex, and race/ethnicity, so those have been included alongside the linear, cubic, and quadratic year terms. Calculate the “ever smoked” regression, adjusted by sex, grade, race/ethnicity, and linear year contrast: linyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l , design = des , family = quasibinomial ) summary( linyear ) The linear year-contrast variable t11l is significant. Therefore, there is probably going to be some sort of trend. A linear trend by itself does not need joinpoints. Not one, just zero joinpoints. If the linear term were the only significant term (out of linear, quadratic, cubic), then we would not need to calculate a joinpoint. In other words, we would not need to figure out where to best break our time trend into two, three, or even four segments. Since the linear trend is significant, we know there is at least one change across the entire 1991 to 2011 period. Interpretation note about segments of time: The linear term t11l was significant, so we probably have a significant linear trend somewhere to report. Now we need to figure out when that significant linear trend started and when it ended. It might be semantically true that there was a significant linear decrease in high school aged smoking over the entire period of our data 1991-2011; however, it’s inexact to end this analysis after only detecting a linear trend. The purpose of the following few steps is to cordon off different time points from one another. As you’ll see later, there actually was not any detectable decrease from 1991-1999. The entirety of the decline in smoking occurred over the period from 1999-2011. So these next (methodologically tricky) steps serve to provide you and your audience with a more careful statement of statistical significance. It’s not technically wrong to conclude that smoking declined over the period of 1991-2011, it’s just verbose. Think of it as the difference between “humans first walked on the moon in the sixties” and “humans first walked on the moon in 1969” - both statements are correct, but the latter exhibits greater scientific precision. Calculate the “ever smoked” binomial regression, adjusted by sex, grade, race/ethnicity, and both linear and quadratic year contrasts. Notice the addition of t11q: quadyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l + t11q , design = des , family = quasibinomial ) summary( quadyear ) The linear year-contrast variable is significant but the quadratic year-contrast variable is also significant. Therefore, we should use joinpoint software (the segmented package) for this analysis. A significant quadratic trend needs one joinpoint. Since both linear and quadratic terms are significant, we can also move ahead and test whether the cubic term is also significant. Calculate the “ever smoked” binomial regression, adjusted by sex, grade, race/ethnicity, and linear, quadratic, and cubic year contrasts. Notice the addition of t11c: cubyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l + t11q + t11c , design = des , family = quasibinomial ) summary( cubyear ) The cubic year-contrast term is also significant in this model. Therefore, we might potentially evaluate this trend using two joinpoints. In other words, a significant result for all linear, quadratic, and cubic year contrasts at this point means we might be able to evaluate three distinct trends (separated by our two joinpoints) across the broader 1991 - 2011 time period of analysis. Although we might now have the statistical ability to analyze three distinct time periods (separated by two joinpoints) across our data, the utility of this depends on the circumstances. Cubic and higher polynomials account for not only the direction of change but also the pace of that change, allowing statistical statements that might not be of interest to an audience: While it might be an exercise in precision to conclude that smoking rates dropped quickest across 1999-2003 and less quickly across 2003-2011, that scientific pair of findings may not be as compelling as the simpler (quadratic but not cubic) statement that smoking rates have dropped across the period of 1999-2011. Calculate Predicted Marginals Calculate the survey-year-independent predictor effects and store these results: marginals &lt;- svyglm( formula = ever_smoked ~ sex + race4 + grade , design = des , family = quasibinomial ) Run these marginals through the svypredmeans function. For archaeology fans out there, this function emulates the PREDMARG statement in the ancient language of SUDAAN: ( means_for_joinpoint &lt;- svypredmeans( marginals , ~factor( year ) ) ) Clean up these results a bit in preparation for a joinpoint analysis: # coerce the results to a data.frame object means_for_joinpoint &lt;- as.data.frame( means_for_joinpoint ) # extract the row names as the survey year means_for_joinpoint[ , &quot;year&quot; ] &lt;- as.numeric( rownames( means_for_joinpoint ) ) # must be sorted, just in case it&#39;s not already means_for_joinpoint &lt;- means_for_joinpoint[ order( means_for_joinpoint[ , &quot;year&quot; ] ) , ] Identify Joinpoint(s) or Breakpoint(s) Let’s take a look at how confident we are in the value at each adjusted timepoint. Carrying out a trend analysis requires creating new weights to fit a piecewise linear regression. First, create that weight variable: means_for_joinpoint[ , &quot;wgt&quot; ] &lt;- with( means_for_joinpoint, ( mean / SE ) ^ 2 ) Second, fit a piecewise linear regression, estimating the ‘starting’ linear model with the usual lm function using the log values and the weights: o &lt;- lm( log( mean ) ~ year , weights = wgt , data = means_for_joinpoint ) Now that the regression has been structured correctly, estimate the year that our complex survey trend should be broken into two or more segments: library(segmented) # find only one joinpoint os &lt;- segmented( o , ~year ) summary( os ) Look for the Estimated Break-Point(s) in that result - that’s the critical number from this joinpoint analysis. The segmented package uses an iterative procedure (described in the article below); between-year solutions are returned and should be rounded to the nearest time point in the analysis. The joinpoint software implements two estimating algorithms: the grid-search and the Hudson algorithm. For more detail about these methods, see Muggeo V. (2003) Estimating regression models with unknown break-points. Statistics in Medicine, 22: 3055-3071.. Obtain the annual percent change estimates for each time point: slope( os , APC = TRUE ) The confidence intervals for the annual percent change (APC) may be different from the ones returned by NCI’s Joinpoint Software; for further details, check out Muggeo V. (2010) A Comment on `Estimating average annual per cent change in trend analysis’ by Clegg et al., Statistics in Medicine; 28, 3670-3682. Statistics in Medicine, 29, 1958-1960. This analysis returned similar results to the NCI’s Joinpoint Regression Program by estimating a joinpoint at year=1999 - and, more precisely, that the start of that decreasing trend in smoking prevalence happened at an APC of -3.92 percent. That is, slope2 from the output above. Remember that the cubic-year model above had significant terms as well. Therefore, it would be statistically defensible to calculate two joinpoints rather than only one. However, for this analyses, breaking the 1999-2011 trend into two separate downward trends might not be of interest to the audience. Looking at the slope2 and slope3 estimates and confidence intervals, we might be able to conclude that “ever smoking” decreased across 1999-2003 and also decreased (albeit less rapidly) across 2003-2011. However, communicating two consecutive downward trends might not be of much interest to a lay audience. Forgoing a second possible joinpoint makes sense when the direction of change is more compelling than the pace of change: # find two joinpoints rather than only one os2 &lt;- segmented( o , ~year , npsi = 2 ) summary( os2 ) slope( os2 , APC = TRUE ) Interpret and Conclude After identifying the joinpoint for smoking prevalence, we can create two regression models (one for each time segment - if we had two joinpoints, we would need three regression models). The first model covers the years leading up to (and including) the joinpoint (i.e., 1991 to 1999). The second model includes the years from the joinpoint forward (i.e., 1999 to 2011). So start with 1991, 1993, 1995, 1997, 1999, the five year-points before (and including) 1999: # calculate a five-timepoint linear contrast vector c5l &lt;- contr.poly( 5 )[ , 1 ] # tack the five-timepoint linear contrast vectors onto the current survey design object des &lt;- update( des , t5l = c5l[ match( year , seq( 1991 , 1999 , 2 ) ) ] ) pre_91_99 &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t5l , design = subset( des , year &lt;= 1999 ) , family = quasibinomial ) summary( pre_91_99 ) # confirm 1991-1999 trend coefficient matches published estimates stopifnot( round( pre_91_99$coefficients[&#39;t5l&#39;] , 5 ) == .03704 ) This reproduces the calculations behind the sentence on pdf page 6 of the original document: In this example, T5L_L had a p-value=0.52261 and beta=0.03704. Therefore, there was “no significant change in the prevalence of ever smoking a cigarette during 1991-1999.” Then move on to 1999, 2001, 2003, 2005, 2007, 2009, and 2011, the seven year-points after (and including) 1999: # calculate a seven-timepoint linear contrast vector c7l &lt;- contr.poly( 7 )[ , 1 ] # tack the seven-timepoint linear contrast vectors onto the current survey design object des &lt;- update( des , t7l = c7l[ match( year , seq( 1999 , 2011 , 2 ) ) ] ) post_99_11 &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t7l , design = subset( des , year &gt;= 1999 ) , family = quasibinomial ) summary( post_99_11 ) # confirm 1999-2011 trend coefficient matches published estimates stopifnot( round( post_99_11$coefficients[&#39;t7l&#39;] , 5 ) == -0.99165 ) This reproduces the calculations behind the sentence on pdf page 6 of the original document: In this example, T7L_R had a p-value&lt;0.0001 and beta=-0.99165. Therefore, there was a “significant linear decrease in the prevalence of ever smoking a cigarette during 1999-2011.” "],["youth-risk-behavior-surveillance-system-yrbss.html", "Youth Risk Behavior Surveillance System (YRBSS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " Youth Risk Behavior Surveillance System (YRBSS) The high school edition of the Behavioral Risk Factor Surveillance System (BRFSS). One table with one row per sampled youth respondent. A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States. Released biennially since 1993. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: Methodology of the Youth Risk Behavior Surveillance System Wikipedia Entry This poem # maladolescence # epidemiology # sex, drugs, rock and roll Download, Import, Preparation Load the SAScii library to interpret a SAS input program, and also re-arrange the SAS input program: library(SAScii) sas_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/2019XXH-SAS-Input-Program.sas&quot; sas_text &lt;- tolower( readLines( sas_url ) ) # find the (out of numerical order) # `site` location variable&#39;s position # within the SAS input program site_location &lt;- which( sas_text == &#39;@1 site $3.&#39; ) # find the start field&#39;s position # within the SAS input program input_location &lt;- which( sas_text == &quot;input&quot; ) # create a vector from 1 to the length of the text file sas_length &lt;- seq( length( sas_text ) ) # remove the site_location sas_length &lt;- sas_length[ -site_location ] # re-insert the site variable&#39;s location # immediately after the starting position sas_reorder &lt;- c( sas_length[ seq( input_location ) ] , site_location , sas_length[ seq( input_location + 1 , length( sas_length ) ) ] ) # re-order the sas text file sas_text &lt;- sas_text[ sas_reorder ] sas_tf &lt;- tempfile() writeLines( sas_text , sas_tf ) Download and import the national file: dat_tf &lt;- tempfile() dat_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/XXH2019_YRBS_Data.dat&quot; download.file( dat_url , dat_tf , mode = &#39;wb&#39; ) yrbss_df &lt;- read.SAScii( dat_tf , sas_tf ) names( yrbss_df ) &lt;- tolower( names( yrbss_df ) ) yrbss_df[ , &#39;one&#39; ] &lt;- 1 Save locally   Save the object at any point: # yrbss_fn &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; , &quot;this_file.rds&quot; ) # saveRDS( yrbss_df , file = yrbss_fn , compress = FALSE ) Load the same object: # yrbss_df &lt;- readRDS( yrbss_fn ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) yrbss_design &lt;- svydesign( ~ psu , strata = ~ stratum , data = yrbss_df , weights = ~ weight , nest = TRUE ) Variable Recoding Add new columns to the data set: yrbss_design &lt;- update( yrbss_design , q2 = q2 , never_rarely_wore_seat_belt = as.numeric( qn8 == 1 ) , ever_used_marijuana = as.numeric( qn45 == 1 ) , tried_to_quit_tobacco_past_year = as.numeric( q39 == 2 ) , used_tobacco_past_year = as.numeric( q39 &gt; 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( yrbss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , yrbss_design ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ bmipct , yrbss_design , 0.5 , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ tried_to_quit_tobacco_past_year , denominator = ~ used_tobacco_past_year , yrbss_design , na.rm = TRUE ) Subsetting Restrict the survey design to youths who ever drank alcohol: sub_yrbss_design &lt;- subset( yrbss_design , qn40 &gt; 1 ) Calculate the mean (average) of this subset: svymean( ~ bmipct , sub_yrbss_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( yrbss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ bmipct , yrbss_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( bmipct ~ never_rarely_wore_seat_belt , yrbss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( bmipct ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for YRBSS users, this code replicates previously-presented examples: library(srvyr) yrbss_srvyr_design &lt;- as_survey( yrbss_design ) Calculate the mean (average) of a linear variable, overall and by groups: yrbss_srvyr_design %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) yrbss_srvyr_design %&gt;% group_by( ever_used_marijuana ) %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) Replication Example This example matches statistics, standard errors, and confidence intervals from the “never/rarely wore seat belt” row of PDF page 29 of this CDC analysis software document. unwtd_count_result &lt;- unwtd.count( ~ never_rarely_wore_seat_belt , yrbss_design ) stopifnot( coef( unwtd_count_result ) == 11149 ) wtd_n_result &lt;- svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_seat_belt ) ) ) stopifnot( round( coef( wtd_n_result ) , 0 ) == 12132 ) share_result &lt;- svymean( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE ) stopifnot( round( coef( share_result ) , 4 ) == .0654 ) stopifnot( round( SE( share_result ) , 4 ) == .0065 ) ci_result &lt;- svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE , method = &quot;beta&quot; ) stopifnot( round( confint( ci_result )[1] , 4 ) == 0.0529 ) stopifnot( round( confint( ci_result )[2] , 2 ) == 0.08 ) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
