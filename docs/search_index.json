[["index.html", "Analyze Survey Data for Free Public Microdata, Easy to Type Website", " Analyze Survey Data for Free Public Microdata, Easy to Type Website Please ask questions about this book at stackoverflow.com with the R and survey tags. The survey microdata presented in this book require the R survey package by Dr. Thomas Lumley at the University of Auckland. Dr. Lumley wrote a textbook to showcase that software. This book replaces my archived blog, prior code, and the no longer maintained lodown package. A work of R is never finished, only abandoned. - Anthony Damico "],["american-community-survey-acs.html", "American Community Survey (ACS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Poverty and Inequality Estimation with convey   Analysis Examples with srvyr   Replication Example", " American Community Survey (ACS) The US Census Bureau’s annual replacement for the long-form decennial census. Two tables per state, the first with one row per household and the second with one row per individual within each household. The civilian population of the United States. Released annually since 2005. Administered and financed by the US Census Bureau. Please skim before you begin: Guidance for Data Users Wikipedia Entry This poem # one percent sample # the decennial census # in miniature Download, Import, Preparation Download and import the Alabama household file: (switch sas_hal to sas_hak for Alaska or sas_hus for the entire country) library(haven) tf_household &lt;- tempfile() this_url_household &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_hal.zip&quot; download.file( this_url_household , tf_household , mode = &#39;wb&#39; ) unzipped_files_household &lt;- unzip( tf_household , exdir = tempdir() ) acs_sas_household &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_household , value = TRUE ) acs_df_household &lt;- haven::read_sas( acs_sas_household ) names( acs_df_household ) &lt;- tolower( names( acs_df_household ) ) Download and import the Alabama person file: (switch sas_pal to sas_pak for Alaska or sas_pus for the entire country) tf_person &lt;- tempfile() this_url_person &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_pal.zip&quot; download.file( this_url_person , tf_person , mode = &#39;wb&#39; ) unzipped_files_person &lt;- unzip( tf_person , exdir = tempdir() ) acs_sas_person &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_person , value = TRUE ) acs_df_person &lt;- haven::read_sas( acs_sas_person ) names( acs_df_person ) &lt;- tolower( names( acs_df_person ) ) Remove overlapping column and merge household + person files: acs_df_household[ , &#39;rt&#39; ] &lt;- NULL acs_df_person[ , &#39;rt&#39; ] &lt;- NULL acs_df &lt;- merge( acs_df_household , acs_df_person ) stopifnot( nrow( acs_df ) == nrow( acs_df_person ) ) acs_df[ , &#39;one&#39; ] &lt;- 1 Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) acs_design &lt;- svrepdesign( weight = ~pwgtp , repweights = &#39;pwgtp[0-9]+&#39; , scale = 4 / 80 , rscales = rep( 1 , 80 ) , mse = TRUE , type = &#39;JK1&#39; , data = acs_df ) Variable Recoding Add new columns to the data set: acs_design &lt;- update( acs_design , state_name = factor( as.numeric( st ) , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L, 72L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;Puerto Rico&quot;) ) , cit = factor( cit , levels = 1:5 , labels = c( &#39;born in the u.s.&#39; , &#39;born in the territories&#39; , &#39;born abroad to american parents&#39; , &#39;naturalized citizen&#39; , &#39;non-citizen&#39; ) ) , poverty_level = as.numeric( povpip ) , married = as.numeric( mar %in% 1 ) , sex = factor( sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( acs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cit , acs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , acs_design ) svyby( ~ one , ~ cit , acs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ poverty_level , acs_design , 0.5 , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ssip , denominator = ~ pincp , acs_design , na.rm = TRUE ) Subsetting Restrict the survey design to senior citizens: sub_acs_design &lt;- subset( acs_design , agep &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ poverty_level , sub_acs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ poverty_level , acs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( acs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ poverty_level , acs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ married , acs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( poverty_level ~ married , acs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ married + sex , acs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( poverty_level ~ married + sex , acs_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for ACS users, this code calculates the gini coefficient on complex sample survey data: library(convey) acs_design &lt;- convey_prep( acs_design ) svygini( ~ hincp , acs_design , na.rm = TRUE ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ACS users, this code replicates previously-presented examples: library(srvyr) acs_srvyr_design &lt;- as_survey( acs_design ) Calculate the mean (average) of a linear variable, overall and by groups: acs_srvyr_design %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) acs_srvyr_design %&gt;% group_by( cit ) %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) Replication Example The example below matches statistics, standard errors, and margin of errors from the 2021 PUMS tallies: Match the sum of the weights: stopifnot( round( coef( svytotal( ~ one , acs_design ) ) , 0 ) == 5039877 ) Compute the population by age: pums_estimate &lt;- c(288139L, 299245L, 336727L, 334606L, 327102L, 635004L, 641405L, 615709L, 335431L, 341926L, 538367L, 265742L, 80474L) pums_standard_error &lt;- c(2727L, 5368L, 6067L, 4082L, 4485L, 5716L, 4420L, 3706L, 4836L, 5100L, 2158L, 3363L, 3186L) pums_margin_of_error &lt;- c(4486L, 8830L, 9981L, 6715L, 7378L, 9402L, 7271L, 6096L, 7956L, 8389L, 3550L, 5532L, 5240L) results &lt;- svytotal( ~ as.numeric( agep %in% 0:4 ) + as.numeric( agep %in% 5:9 ) + as.numeric( agep %in% 10:14 ) + as.numeric( agep %in% 15:19 ) + as.numeric( agep %in% 20:24 ) + as.numeric( agep %in% 25:34 ) + as.numeric( agep %in% 35:44 ) + as.numeric( agep %in% 45:54 ) + as.numeric( agep %in% 55:59 ) + as.numeric( agep %in% 60:64 ) + as.numeric( agep %in% 65:74 ) + as.numeric( agep %in% 75:84 ) + as.numeric( agep %in% 85:100 ) , acs_design ) stopifnot( all( round( coef( results ) , 0 ) == pums_estimate ) ) stopifnot( all( round( SE( results ) , 0 ) == pums_standard_error ) ) stopifnot( all( round( SE( results ) * 1.645 , 0 ) == pums_margin_of_error ) ) "],["european-social-survey-ess.html", "European Social Survey (ESS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " European Social Survey (ESS) The European Social Survey measures political opinion and behavior across the continent. One table per country with one row per sampled respondent. A complex sample survey designed to generalize to residents aged 15 and older in participating nations. Released biennially since 2002. Headquartered at City, University of London and governed by a scientific team across Europe. Please skim before you begin: Findings from the European Social Survey Wikipedia Entry This poem # pent up belief gauge # open border monarchists # survey for your thoughts Download, Import, Preparation Register at the ESS Data Portal at https://ess-search.nsd.no/. Choose ESS round 8 - 2016. Welfare attitudes, Attitudes to climate change. Download the integrated file and also the sample design (SDDF) files as SAV (SPSS) files: library(foreign) ess_int_df &lt;- read.spss( file.path( path.expand( &quot;~&quot; ) , &quot;ESS8e02_2.sav&quot; ) , to.data.frame = TRUE , use.value.labels = FALSE ) ess_sddf_df &lt;- read.spss( file.path( path.expand( &quot;~&quot; ) , &quot;ESS8SDDFe01_1.sav&quot; ) , to.data.frame = TRUE , use.value.labels = FALSE ) ess_df &lt;- merge( ess_int_df , ess_sddf_df , by = c( &#39;cntry&#39; , &#39;idno&#39; ) ) stopifnot( nrow( ess_df ) == nrow( ess_int_df ) ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) ## Loading required package: grid ## Loading required package: Matrix ## Loading required package: survival ## ## Attaching package: &#39;survey&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## dotchart options( survey.lonely.psu = &quot;adjust&quot; ) ess_df[ , &#39;anweight&#39; ] &lt;- ess_df[ , &#39;pspwght&#39; ] * ess_df[ , &#39;pweight&#39; ] * 10000 ess_design &lt;- svydesign( ids = ~psu , strata = ~stratum , weights = ~anweight , data = ess_df , nest = TRUE ) Variable Recoding Add new columns to the data set: ess_design &lt;- update( ess_design , one = 1 , gndr = factor( gndr , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , netusoft = factor( netusoft , levels = 1:5 , labels = c( &#39;Never&#39; , &#39;Only occasionally&#39; , &#39;A few times a week&#39; , &#39;Most days&#39; , &#39;Every day&#39; ) ) , belonging_to_particular_religion = as.numeric( rlgblg == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( ess_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cntry , ess_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , ess_design ) svyby( ~ one , ~ cntry , ess_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ ppltrst , ess_design , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ gndr , ess_design , na.rm = TRUE ) svyby( ~ gndr , ~ cntry , ess_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ ppltrst , ess_design , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ gndr , ess_design , na.rm = TRUE ) svyby( ~ gndr , ~ cntry , ess_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ ppltrst , ess_design , 0.5 , na.rm = TRUE ) svyby( ~ ppltrst , ~ cntry , ess_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ppltrst , denominator = ~ pplfair , ess_design , na.rm = TRUE ) Subsetting Restrict the survey design to voters: sub_ess_design &lt;- subset( ess_design , vote == 1 ) Calculate the mean (average) of this subset: svymean( ~ ppltrst , sub_ess_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ ppltrst , ess_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ ppltrst , ~ cntry , ess_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ess_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ ppltrst , ess_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ ppltrst , ess_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ ppltrst , ess_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ belonging_to_particular_religion , ess_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( ppltrst ~ belonging_to_particular_religion , ess_design ) Perform a chi-squared test of association for survey data: svychisq( ~ belonging_to_particular_religion + gndr , ess_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( ppltrst ~ belonging_to_particular_religion + gndr , ess_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ESS users, this code replicates previously-presented examples: library(srvyr) ## ## Attaching package: &#39;srvyr&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter ess_srvyr_design &lt;- as_survey( ess_design ) Calculate the mean (average) of a linear variable, overall and by groups: ess_srvyr_design %&gt;% summarize( mean = survey_mean( ppltrst , na.rm = TRUE ) ) ess_srvyr_design %&gt;% group_by( cntry ) %&gt;% summarize( mean = survey_mean( ppltrst , na.rm = TRUE ) ) Replication Example This example below matches statistics and confidence intervals within 0.1% from the Guide to Using Weights and Sample Design Indicators with ESS Data published_proportions &lt;- c( 0.166 , 0.055 , 0.085 , 0.115 , 0.578 ) published_lb &lt;- c( 0.146 , 0.045 , 0.072 , 0.099 , 0.550 ) published_ub &lt;- c( 0.188 , 0.068 , 0.100 , 0.134 , 0.605 ) austrians &lt;- subset( ess_design , cntry == &#39;AT&#39; ) ( results &lt;- svymean( ~ netusoft , austrians , na.rm = TRUE ) ) stopifnot( all( round( coef( results ) , 3 ) == published_proportions ) ) ( ci_results &lt;- confint( results ) ) stopifnot( all( abs( ci_results[ , 1 ] - published_lb ) &lt; 0.0015 ) ) stopifnot( all( abs( ci_results[ , 2 ] - published_ub ) &lt; 0.0015 ) ) "],["medicare-current-beneficiary-survey-mcbs.html", "Medicare Current Beneficiary Survey (MCBS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " Medicare Current Beneficiary Survey (MCBS) The monitoring system for Medicare enrollees in the United States on topics not available in the program’s administrative data, such as out of pocket expenditure and beneficiary satisfaction. Survey and supplemental tables with one row per sampled individual, although downloadable datasets not linkable. A complex sample survey designed to generalize to all elderly and disabled individuals with at least one month of program enrollment during the calendar year. Released annually as a public use file since 2015. Conducted by the Office of Enterprise Data and Analytics (OEDA) of the Centers for Medicare &amp; Medicaid Services (CMS) through a contract with NORC at the University of Chicago. Please skim before you begin: MCBS Methodology Report MCBS Advanced Tutorial on Weighting and Variance Estimation This poem # old, or disabled # access to medical care, # utilization Download, Import, Preparation tf &lt;- tempfile() this_url &lt;- &quot;https://www.cms.gov/files/zip/cspuf2019.zip&quot; download.file( this_url , tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) mcbs_csv &lt;- grep( &#39;\\\\.csv$&#39; , unzipped_files , value = TRUE ) mcbs_df &lt;- read.csv( mcbs_csv ) names( mcbs_df ) &lt;- tolower( names( mcbs_df ) ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) mcbs_design &lt;- svrepdesign( weight = ~cspufwgt , repweights = &#39;cspuf[0-9]+&#39; , mse = TRUE , type = &#39;Fay&#39; , rho = 0.3 , data = mcbs_df ) Variable Recoding Add new columns to the data set: mcbs_design &lt;- update( mcbs_design , one = 1 , csp_age = factor( csp_age , levels = 1:3 , labels = c( &#39;01: younger than 65&#39; , &#39;02: 65 to 74&#39; , &#39;03: 75 or older&#39; ) ) , two_or_more_chronic_conditions = as.numeric( csp_nchrncnd &gt; 1 ) , csp_sex = factor( csp_sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( mcbs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ csp_age , mcbs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , mcbs_design ) svyby( ~ one , ~ csp_age , mcbs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ pamtoop , mcbs_design ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ csp_sex , mcbs_design ) svyby( ~ csp_sex , ~ csp_age , mcbs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ pamtoop , mcbs_design ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ csp_sex , mcbs_design ) svyby( ~ csp_sex , ~ csp_age , mcbs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ pamtoop , mcbs_design , 0.5 ) svyby( ~ pamtoop , ~ csp_age , mcbs_design , svyquantile , 0.5 , ci = TRUE ) Estimate a ratio: svyratio( numerator = ~ pamtoop , denominator = ~ pamttot , mcbs_design ) Subsetting Restrict the survey design to household income below $25,000: sub_mcbs_design &lt;- subset( mcbs_design , csp_income == 1 ) Calculate the mean (average) of this subset: svymean( ~ pamtoop , sub_mcbs_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ pamtoop , mcbs_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ pamtoop , ~ csp_age , mcbs_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( mcbs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ pamtoop , mcbs_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ pamtoop , mcbs_design , deff = TRUE ) # SRS with replacement svymean( ~ pamtoop , mcbs_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ two_or_more_chronic_conditions , mcbs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( pamtoop ~ two_or_more_chronic_conditions , mcbs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ two_or_more_chronic_conditions + csp_sex , mcbs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( pamtoop ~ two_or_more_chronic_conditions + csp_sex , mcbs_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for MCBS users, this code replicates previously-presented examples: library(srvyr) mcbs_srvyr_design &lt;- as_survey( mcbs_design ) Calculate the mean (average) of a linear variable, overall and by groups: mcbs_srvyr_design %&gt;% summarize( mean = survey_mean( pamtoop ) ) mcbs_srvyr_design %&gt;% group_by( csp_age ) %&gt;% summarize( mean = survey_mean( pamtoop ) ) Replication Example This example matches the weighted total from the 2019 Data User’s Guide: Cost Supplement File Public Use File stopifnot( round( coef( svytotal( ~ one , mcbs_design ) ) , 0 ) == 56307461 ) "],["medical-large-claims-experience-study-mlces.html", "Medical Large Claims Experience Study (MLCES) Download, Import, Preparation Analysis Examples with base R   Intermish Analysis Examples with dplyr   Replication Example", " Medical Large Claims Experience Study (MLCES) The best private health insurance claims data available to the public. This data should be used to calibrate other data sets, probably nothing more. One table with one row per individual with nonzero total paid charges. A convenience sample of group (employer-sponsored) health insurance claims from seven private health insurers in the United States. 1997 thru 1999 with no expected updates in the future. Provided by the Society of Actuaries (SOA). Please skim before you begin: Group Medical Insurance Claims Database Collection and Analysis Report Claim Severities, Claim Relativities, and Age: Evidence from SOA Group Health Data This poem # skewed by black swan tails # means, medians sing adieu # claims distribution Download, Import, Preparation Download and import the 1999 medical claims file: tf &lt;- tempfile() this_url &lt;- &quot;https://www.soa.org/Files/Research/1999.zip&quot; download.file( this_url , tf , mode = &#39;wb&#39; ) unzipped_files &lt;- unzip( tf , exdir = tempdir() ) mlces_df &lt;- read.csv( unzipped_files ) names( mlces_df ) &lt;- tolower( names( mlces_df ) ) Analysis Examples with base R   Load a data frame: Variable Recoding Add new columns to the data set: mlces_df &lt;- transform( mlces_df , one = 1 , claimant_relationship_to_policyholder = ifelse( relation == &quot;E&quot; , &quot;covered employee&quot; , ifelse( relation == &quot;S&quot; , &quot;spouse of covered employee&quot; , ifelse( relation == &quot;D&quot; , &quot;dependent of covered employee&quot; , NA ) ) ) , ppo_plan = as.numeric( ppo == &#39;Y&#39; ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( mlces_df ) table( mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( mlces_df[ , &quot;patsex&quot; ] ) ) prop.table( table( mlces_df[ , c( &quot;patsex&quot; , &quot;claimant_relationship_to_policyholder&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( mlces_df[ , &quot;totpdchg&quot; ] , 0.5 ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to persons under 18: sub_mlces_df &lt;- subset( mlces_df , ( ( claimyr - patbrtyr ) &lt; 18 ) ) Calculate the mean (average) of this subset: mean( sub_mlces_df[ , &quot;totpdchg&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( totpdchg ~ ppo_plan , mlces_df ) Perform a chi-squared test of association: this_table &lt;- table( mlces_df[ , c( &quot;ppo_plan&quot; , &quot;patsex&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( totpdchg ~ ppo_plan + patsex , data = mlces_df ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for MLCES users, this code replicates previously-presented examples: library(dplyr) mlces_tbl &lt;- tbl_df( mlces_df ) Calculate the mean (average) of a linear variable, overall and by groups: mlces_tbl %&gt;% summarize( mean = mean( totpdchg ) ) mlces_tbl %&gt;% group_by( claimant_relationship_to_policyholder ) %&gt;% summarize( mean = mean( totpdchg ) ) Replication Example The example below matches statistics in Table II-A’s 1999 row numbers 52 and 53 from the Tables II – XI 1997–1999 Database: Match Claimants Exceeding Deductible: # $0 deductible stopifnot( nrow( mlces_df ) == 1591738 ) # $1,000 deductible charges_above_1000 &lt;- subset( mlces_df , totpdchg &gt; 1000 ) stopifnot( nrow( charges_above_1000 ) == 402550 ) Match the Excess Charges Above Deductible: # $0 deductible stopifnot( round( sum( mlces_df[ , &#39;totpdchg&#39; ] ) , 0 ) == 2599356658 ) # $1,000 deductible stopifnot( round( sum( charges_above_1000[ , &#39;totpdchg&#39; ] - 1000 ) , 0 ) == 1883768786 ) "],["trend-analysis-of-complex-survey-data.html", "Trend Analysis of Complex Survey Data Download, Import, Preparation Append Polynomials to Each Year Unadjusted Analysis Examples Calculate Joinpoints Needed Calculate Predicted Marginals Identify Joinpoint(s) or Breakpoint(s) Interpret and Conclude", " Trend Analysis of Complex Survey Data Contributed by Thomas Yokota &lt;thomasyokota@gmail.com&gt; Professor Vito Muggeo wrote the joinpoint analysis section of the code below and the segmented package. Professor Dr. Thomas Lumley wrote the svypredmeans function to replicate SUDAAN’s PREDMARG command and match the CDC to the decimal. Dr. Richard Lowry, M.D. at the Centers for Disease Control &amp; Prevention wrote the original linear trend analysis and then answered our infinite questions. Thanks to everyone. The purpose of this analysis is to make statistically valid statements such as, “there was a significant linear decrease in the prevalence of high school aged americans who have ever smoked a cigarette across the period 1999-2011” with complex sample survey data. This step-by-step walkthrough exactly reproduces the statistics presented in the Center for Disease Control &amp; Prevention’s (CDC) linear trend analysis. This analysis may complement qualitative evaluation on prevalence changes observed from surveillance data by providing quantitative evidence, such as when a joinpoint (also called breakpoint or changepoint) occurred; however, this analysis does not explain why or how changes in trends occur. Download, Import, Preparation Download and import the multi-year stacked file: library(SAScii) library(readr) sas_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/sadc_2019/2019-SADC-SAS-Input-Program.sas&quot; dat_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/sadc_2019/sadc_2019_national.dat&quot; sas_positions &lt;- parse.SAScii( sas_url ) sas_positions[ , &#39;varname&#39; ] &lt;- tolower( sas_positions[ , &#39;varname&#39; ] ) variables_to_keep &lt;- c( &quot;sex&quot; , &quot;grade&quot; , &quot;race4&quot; , &quot;q30&quot; , &quot;year&quot; , &quot;psu&quot; , &quot;stratum&quot; , &quot;weight&quot; ) sas_positions[ , &#39;column_types&#39; ] &lt;- ifelse( !( sas_positions[ , &#39;varname&#39; ] %in% variables_to_keep ) , &quot;_&quot; , ifelse( sas_positions[ , &#39;char&#39; ] , &quot;c&quot; , &quot;d&quot; ) ) yrbss_tbl &lt;- read_fwf( dat_url , fwf_widths( abs( sas_positions[ , &#39;width&#39; ] ) , col_names = sas_positions[ , &#39;varname&#39; ] ) , col_types = paste0( sas_positions[ , &#39;column_types&#39; ] , collapse = &quot;&quot; ) , na = c( &quot;&quot; , &quot;.&quot; ) ) yrbss_df &lt;- data.frame( yrbss_tbl ) Restrict the dataset to only years shown in the original analysis and re-name the main variable: yrbss_df &lt;- subset( yrbss_df , year %in% seq( 1991 , 2011 , 2 ) ) yrbss_df[ , &#39;ever_smoked&#39; ] &lt;- as.numeric( yrbss_df[ , &#39;q30&#39; ] == 1 ) yrbss_df[ , &#39;q30&#39; ] &lt;- NULL Recode each categorical variable to factor class: yrbss_df[ , &#39;sex&#39; ] &lt;- relevel( factor( yrbss_df[ , &#39;sex&#39; ] ) , ref = &quot;2&quot; ) for ( i in c( &#39;race4&#39; , &#39;grade&#39; ) ){ yrbss_df[ , i ] &lt;- relevel( factor( yrbss_df[ , i ] ) , ref = &quot;1&quot; ) } Append Polynomials to Each Year “The polynomials we have used as predictors to this point are natural polynomials, generated from the linear predictor by centering and then powering the linear predictor.” For more detail on this subject, see page 216 of Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences By Jacob Cohen, Patricia Cohen, Stephen G. West, Leona S. Aiken distinct_years_available &lt;- length( seq( 1991 , 2011 , 2 ) ) # store the linear polynomials c11l &lt;- contr.poly( distinct_years_available )[ , &quot;.L&quot; ] # store the quadratic polynomials c11q &lt;- contr.poly( distinct_years_available )[ , &quot;.Q&quot; ] # store the cubic polynomials c11c &lt;- contr.poly( distinct_years_available )[ , &quot;.C&quot; ] For each record in the data set, tack on the linear, quadratic, and cubic contrast value, these contrast values will serve as replacement for the linear year variable in any regression: # year^1 term (linear) yrbss_df[ , &quot;t11l&quot; ] &lt;- c11l[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] # year^2 term (quadratic) yrbss_df[ , &quot;t11q&quot; ] &lt;- c11q[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] # year^3 term (cubic) yrbss_df[ , &quot;t11c&quot; ] &lt;- c11c[ match( yrbss_df[ , &quot;year&quot; ] , seq( 1991 , 2011 , 2 ) ) ] Unadjusted Analysis Examples Construct a complex sample survey design and match the published unadjusted prevalence rates: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) des &lt;- svydesign( id = ~psu , strata = ~interaction( stratum , year ) , data = yrbss_df , weights = ~weight , nest = TRUE ) prevalence_over_time &lt;- svyby( ~ ever_smoked , ~ year , des , svymean , na.rm = TRUE ) # confirm prevalence rates match published estimates # of high school students that ever smoked stopifnot( all.equal( round( coef( prevalence_over_time ) , 3 ) , c( .701 , .695 , .713 , .702 , .704 , .639 , .584 , .543 , .503 , .463 , .447 ) , check.attributes = FALSE ) ) Calculate Joinpoints Needed Using the orthogonal coefficients (linear, quadratic, cubic terms) that we previously added to our yrbss_df object before constructing the multi-year stacked survey design, determine how many joinpoints will be needed for a trend analysis. Epidemiological models typically control for possible confounding variables such as age, sex, and race/ethnicity, so those have been included alongside the linear, cubic, and quadratic year terms. Calculate the “ever smoked” regression, adjusted by sex, grade, race/ethnicity, and linear year contrast: linyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l , design = des , family = quasibinomial ) summary( linyear ) The linear year-contrast variable t11l is significant. Therefore, there is probably going to be some sort of trend. A linear trend by itself does not need joinpoints. Not one, just zero joinpoints. If the linear term were the only significant term (out of linear, quadratic, cubic), then we would not need to calculate a joinpoint. In other words, we would not need to figure out where to best break our time trend into two, three, or even four segments. Since the linear trend is significant, we know there is at least one change across the entire 1991 to 2011 period. Interpretation note about segments of time: The linear term t11l was significant, so we probably have a significant linear trend somewhere to report. Now we need to figure out when that significant linear trend started and when it ended. It might be semantically true that there was a significant linear decrease in high school aged smoking over the entire period of our data 1991-2011; however, it’s inexact to end this analysis after only detecting a linear trend. The purpose of the following few steps is to cordon off different time points from one another. As you’ll see later, there actually was not any detectable decrease from 1991-1999. The entirety of the decline in smoking occurred over the period from 1999-2011. So these next (methodologically tricky) steps serve to provide you and your audience with a more careful statement of statistical significance. It’s not technically wrong to conclude that smoking declined over the period of 1991-2011, it’s just verbose. Think of it as the difference between “humans first walked on the moon in the sixties” and “humans first walked on the moon in 1969” - both statements are correct, but the latter exhibits greater scientific precision. Calculate the “ever smoked” binomial regression, adjusted by sex, grade, race/ethnicity, and both linear and quadratic year contrasts. Notice the addition of t11q: quadyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l + t11q , design = des , family = quasibinomial ) summary( quadyear ) The linear year-contrast variable is significant but the quadratic year-contrast variable is also significant. Therefore, we should use joinpoint software (the segmented package) for this analysis. A significant quadratic trend needs one joinpoint. Since both linear and quadratic terms are significant, we can also move ahead and test whether the cubic term is also significant. Calculate the “ever smoked” binomial regression, adjusted by sex, grade, race/ethnicity, and linear, quadratic, and cubic year contrasts. Notice the addition of t11c: cubyear &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t11l + t11q + t11c , design = des , family = quasibinomial ) summary( cubyear ) The cubic year-contrast term is also significant in this model. Therefore, we might potentially evaluate this trend using two joinpoints. In other words, a significant result for all linear, quadratic, and cubic year contrasts at this point means we might be able to evaluate three distinct trends (separated by our two joinpoints) across the broader 1991 - 2011 time period of analysis. Although we might now have the statistical ability to analyze three distinct time periods (separated by two joinpoints) across our data, the utility of this depends on the circumstances. Cubic and higher polynomials account for not only the direction of change but also the pace of that change, allowing statistical statements that might not be of interest to an audience: While it might be an exercise in precision to conclude that smoking rates dropped quickest across 1999-2003 and less quickly across 2003-2011, that scientific pair of findings may not be as compelling as the simpler (quadratic but not cubic) statement that smoking rates have dropped across the period of 1999-2011. Calculate Predicted Marginals Calculate the survey-year-independent predictor effects and store these results: marginals &lt;- svyglm( formula = ever_smoked ~ sex + race4 + grade , design = des , family = quasibinomial ) Run these marginals through the svypredmeans function. For archaeology fans out there, this function emulates the PREDMARG statement in the ancient language of SUDAAN: ( means_for_joinpoint &lt;- svypredmeans( marginals , ~factor( year ) ) ) Clean up these results a bit in preparation for a joinpoint analysis: # coerce the results to a data.frame object means_for_joinpoint &lt;- as.data.frame( means_for_joinpoint ) # extract the row names as the survey year means_for_joinpoint[ , &quot;year&quot; ] &lt;- as.numeric( rownames( means_for_joinpoint ) ) # must be sorted, just in case it&#39;s not already means_for_joinpoint &lt;- means_for_joinpoint[ order( means_for_joinpoint[ , &quot;year&quot; ] ) , ] Identify Joinpoint(s) or Breakpoint(s) Let’s take a look at how confident we are in the value at each adjusted timepoint. Carrying out a trend analysis requires creating new weights to fit a piecewise linear regression. First, create that weight variable: means_for_joinpoint[ , &quot;wgt&quot; ] &lt;- with( means_for_joinpoint, ( mean / SE ) ^ 2 ) Second, fit a piecewise linear regression, estimating the ‘starting’ linear model with the usual lm function using the log values and the weights: o &lt;- lm( log( mean ) ~ year , weights = wgt , data = means_for_joinpoint ) Now that the regression has been structured correctly, estimate the year that our complex survey trend should be broken into two or more segments: library(segmented) # find only one joinpoint os &lt;- segmented( o , ~year ) summary( os ) Look for the Estimated Break-Point(s) in that result - that’s the critical number from this joinpoint analysis. The segmented package uses an iterative procedure (described in the article below); between-year solutions are returned and should be rounded to the nearest time point in the analysis. The joinpoint software implements two estimating algorithms: the grid-search and the Hudson algorithm. For more detail about these methods, see Muggeo V. (2003) Estimating regression models with unknown break-points. Statistics in Medicine, 22: 3055-3071.. Obtain the annual percent change estimates for each time point: slope( os , APC = TRUE ) The confidence intervals for the annual percent change (APC) may be different from the ones returned by NCI’s Joinpoint Software; for further details, check out Muggeo V. (2010) A Comment on `Estimating average annual per cent change in trend analysis’ by Clegg et al., Statistics in Medicine; 28, 3670-3682. Statistics in Medicine, 29, 1958-1960. This analysis returned similar results to the NCI’s Joinpoint Regression Program by estimating a joinpoint at year=1999 - and, more precisely, that the start of that decreasing trend in smoking prevalence happened at an APC of -3.92 percent. That is, slope2 from the output above. Remember that the cubic-year model above had significant terms as well. Therefore, it would be statistically defensible to calculate two joinpoints rather than only one. However, for this analyses, breaking the 1999-2011 trend into two separate downward trends might not be of interest to the audience. Looking at the slope2 and slope3 estimates and confidence intervals, we might be able to conclude that “ever smoking” decreased across 1999-2003 and also decreased (albeit less rapidly) across 2003-2011. However, communicating two consecutive downward trends might not be of much interest to a lay audience. Forgoing a second possible joinpoint makes sense when the direction of change is more compelling than the pace of change: # find two joinpoints rather than only one os2 &lt;- segmented( o , ~year , npsi = 2 ) summary( os2 ) slope( os2 , APC = TRUE ) Interpret and Conclude After identifying the joinpoint for smoking prevalence, we can create two regression models (one for each time segment - if we had two joinpoints, we would need three regression models). The first model covers the years leading up to (and including) the joinpoint (i.e., 1991 to 1999). The second model includes the years from the joinpoint forward (i.e., 1999 to 2011). So start with 1991, 1993, 1995, 1997, 1999, the five year-points before (and including) 1999: # calculate a five-timepoint linear contrast vector c5l &lt;- contr.poly( 5 )[ , 1 ] # tack the five-timepoint linear contrast vectors onto the current survey design object des &lt;- update( des , t5l = c5l[ match( year , seq( 1991 , 1999 , 2 ) ) ] ) pre_91_99 &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t5l , design = subset( des , year &lt;= 1999 ) , family = quasibinomial ) summary( pre_91_99 ) # confirm 1991-1999 trend coefficient matches published estimates stopifnot( round( pre_91_99$coefficients[&#39;t5l&#39;] , 5 ) == .03704 ) This reproduces the calculations behind the sentence on pdf page 6 of the original document: In this example, T5L_L had a p-value=0.52261 and beta=0.03704. Therefore, there was “no significant change in the prevalence of ever smoking a cigarette during 1991-1999.” Then move on to 1999, 2001, 2003, 2005, 2007, 2009, and 2011, the seven year-points after (and including) 1999: # calculate a seven-timepoint linear contrast vector c7l &lt;- contr.poly( 7 )[ , 1 ] # tack the seven-timepoint linear contrast vectors onto the current survey design object des &lt;- update( des , t7l = c7l[ match( year , seq( 1999 , 2011 , 2 ) ) ] ) post_99_11 &lt;- svyglm( ever_smoked ~ sex + race4 + grade + t7l , design = subset( des , year &gt;= 1999 ) , family = quasibinomial ) summary( post_99_11 ) # confirm 1999-2011 trend coefficient matches published estimates stopifnot( round( post_99_11$coefficients[&#39;t7l&#39;] , 5 ) == -0.99165 ) This reproduces the calculations behind the sentence on pdf page 6 of the original document: In this example, T7L_R had a p-value&lt;0.0001 and beta=-0.99165. Therefore, there was a “significant linear decrease in the prevalence of ever smoking a cigarette during 1999-2011.” "],["youth-risk-behavior-surveillance-system-yrbss.html", "Youth Risk Behavior Surveillance System (YRBSS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Replication Example", " Youth Risk Behavior Surveillance System (YRBSS) The high school edition of the Behavioral Risk Factor Surveillance System (BRFSS), a scientific study of good kids who do bad things. One table with one row per sampled youth respondent. A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States. Released biennially since 1993. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: Methodology of the Youth Risk Behavior Surveillance System Wikipedia Entry This poem # maladolescence # epidemiology # sex, drugs, rock and roll Download, Import, Preparation Load the SAScii library to interpret a SAS input program, and also re-arrange the SAS input program: library(SAScii) sas_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/2019XXH-SAS-Input-Program.sas&quot; sas_text &lt;- tolower( readLines( sas_url ) ) # find the (out of numerical order) # `site` location variable&#39;s position # within the SAS input program site_location &lt;- which( sas_text == &#39;@1 site $3.&#39; ) # find the start field&#39;s position # within the SAS input program input_location &lt;- which( sas_text == &quot;input&quot; ) # create a vector from 1 to the length of the text file sas_length &lt;- seq( length( sas_text ) ) # remove the site_location sas_length &lt;- sas_length[ -site_location ] # re-insert the site variable&#39;s location # immediately after the starting position sas_reorder &lt;- c( sas_length[ seq( input_location ) ] , site_location , sas_length[ seq( input_location + 1 , length( sas_length ) ) ] ) # re-order the sas text file sas_text &lt;- sas_text[ sas_reorder ] sas_tf &lt;- tempfile() writeLines( sas_text , sas_tf ) Download and import the national file: dat_tf &lt;- tempfile() dat_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/XXH2019_YRBS_Data.dat&quot; download.file( dat_url , dat_tf , mode = &#39;wb&#39; ) yrbss_df &lt;- read.SAScii( dat_tf , sas_tf ) names( yrbss_df ) &lt;- tolower( names( yrbss_df ) ) yrbss_df[ , &#39;one&#39; ] &lt;- 1 Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) yrbss_design &lt;- svydesign( ~ psu , strata = ~ stratum , data = yrbss_df , weights = ~ weight , nest = TRUE ) Variable Recoding Add new columns to the data set: yrbss_design &lt;- update( yrbss_design , q2 = q2 , never_rarely_wore_seat_belt = as.numeric( qn8 == 1 ) , ever_used_marijuana = as.numeric( qn45 == 1 ) , tried_to_quit_tobacco_past_year = as.numeric( q39 == 2 ) , used_tobacco_past_year = as.numeric( q39 &gt; 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( yrbss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , yrbss_design ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ bmipct , yrbss_design , 0.5 , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svyquantile , 0.5 , ci = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ tried_to_quit_tobacco_past_year , denominator = ~ used_tobacco_past_year , yrbss_design , na.rm = TRUE ) Subsetting Restrict the survey design to youths who ever drank alcohol: sub_yrbss_design &lt;- subset( yrbss_design , qn40 &gt; 1 ) Calculate the mean (average) of this subset: svymean( ~ bmipct , sub_yrbss_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( yrbss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ bmipct , yrbss_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( bmipct ~ never_rarely_wore_seat_belt , yrbss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( bmipct ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for YRBSS users, this code replicates previously-presented examples: library(srvyr) yrbss_srvyr_design &lt;- as_survey( yrbss_design ) Calculate the mean (average) of a linear variable, overall and by groups: yrbss_srvyr_design %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) yrbss_srvyr_design %&gt;% group_by( ever_used_marijuana ) %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) Replication Example The example below matches statistics, standard errors, and confidence intervals from the “never/rarely wore seat belt” row of PDF page 29 of this CDC analysis software document. unwtd_count_result &lt;- unwtd.count( ~ never_rarely_wore_seat_belt , yrbss_design ) stopifnot( coef( unwtd_count_result ) == 11149 ) wtd_n_result &lt;- svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_seat_belt ) ) ) stopifnot( round( coef( wtd_n_result ) , 0 ) == 12132 ) share_result &lt;- svymean( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE ) stopifnot( round( coef( share_result ) , 4 ) == .0654 ) stopifnot( round( SE( share_result ) , 4 ) == .0065 ) ci_result &lt;- svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE , method = &quot;beta&quot; ) stopifnot( round( confint( ci_result )[1] , 4 ) == 0.0529 ) stopifnot( round( confint( ci_result )[2] , 2 ) == 0.08 ) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
