[["index.html", "Analyze Survey Data for Free Public Microdata, Easy to Type Website", " Analyze Survey Data for Free Public Microdata, Easy to Type Website Please ask questions about the content of this book at stackoverflow.com with the R and survey tags. The survey microdata presented in this book require the R survey package by Dr. Thomas Lumley at the University of Auckland. Dr. Lumley wrote a textbook to showcase that software. This book replaces my archived blog, prior code, and the no longer maintained lodown package. A work of R is never finished, only abandoned. - Anthony Damico "],["american-community-survey-acs.html", "American Community Survey (ACS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Poverty and Inequality Estimation with convey   Analysis Examples with srvyr   Technical Documentation Replication Example", " American Community Survey (ACS) The US Census Bureau’s annual replacement for the long-form decennial census. Two tables per state, the first with one row per household and the second with one row per individual within each household. The civilian population of the United States. Released annually since 2005. Administered and financed by the US Census Bureau. Please skim before you begin: Guidance for Data Users Wikipedia Entry This poem # one percent sample # the decennial census # in miniature Download, Import, Preparation Load the haven library to import sas files: library(haven) Download and import the Alabama household file: (switch sas_hal to sas_hak for Alaska or sas_hus for the entire country) tf_household &lt;- tempfile() this_url_household &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_hal.zip&quot; download.file( this_url_household , tf_household , mode = &#39;wb&#39; ) unzipped_files_household &lt;- unzip( tf_household , exdir = tempdir() ) acs_sas_household &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_household , value = TRUE ) acs_df_household &lt;- haven::read_sas( acs_sas_household ) names( acs_df_household ) &lt;- tolower( names( acs_df_household ) ) Download and import the Alabama person file: (switch sas_pal to sas_pak for Alaska or sas_pus for the entire country) tf_person &lt;- tempfile() this_url_person &lt;- &quot;https://www2.census.gov/programs-surveys/acs/data/pums/2021/1-Year/sas_pal.zip&quot; download.file( this_url_person , tf_person , mode = &#39;wb&#39; ) unzipped_files_person &lt;- unzip( tf_person , exdir = tempdir() ) acs_sas_person &lt;- grep( &#39;\\\\.sas7bdat$&#39; , unzipped_files_person , value = TRUE ) acs_df_person &lt;- haven::read_sas( acs_sas_person ) names( acs_df_person ) &lt;- tolower( names( acs_df_person ) ) Remove overlapping column and merge household + person files: acs_df_household[ , &#39;rt&#39; ] &lt;- NULL acs_df_person[ , &#39;rt&#39; ] &lt;- NULL acs_df &lt;- merge( acs_df_household , acs_df_person ) stopifnot( nrow( acs_df ) == nrow( acs_df_person ) ) acs_df[ , &#39;one&#39; ] &lt;- 1 Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) acs_design &lt;- svrepdesign( weight = ~pwgtp , repweights = &#39;pwgtp[0-9]+&#39; , scale = 4 / 80 , rscales = rep( 1 , 80 ) , mse = TRUE , type = &#39;JK1&#39; , data = acs_df ) Variable Recoding Add new columns to the data set: acs_design &lt;- update( acs_design , state_name = factor( as.numeric( st ) , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L, 72L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;Puerto Rico&quot;) ) , cit = factor( cit , levels = 1:5 , labels = c( &#39;born in the u.s.&#39; , &#39;born in the territories&#39; , &#39;born abroad to american parents&#39; , &#39;naturalized citizen&#39; , &#39;non-citizen&#39; ) ) , poverty_level = as.numeric( povpip ) , married = as.numeric( mar %in% 1 ) , sex = factor( sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( acs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cit , acs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , acs_design ) svyby( ~ one , ~ cit , acs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ poverty_level , acs_design , 0.5 , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ssip , denominator = ~ pincp , acs_design , na.rm = TRUE ) Subsetting Restrict the survey design to senior citizens: sub_acs_design &lt;- subset( acs_design , agep &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ poverty_level , sub_acs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ poverty_level , acs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( acs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ poverty_level , acs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ married , acs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( poverty_level ~ married , acs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ married + sex , acs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( poverty_level ~ married + sex , acs_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for ACS users, this code calculates the gini coefficient on complex sample survey data: library(convey) acs_design &lt;- convey_prep( acs_design ) svygini( ~ hincp , acs_design , na.rm = TRUE ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ACS users, this code replicates previously-presented examples: library(srvyr) acs_srvyr_design &lt;- as_survey( acs_design ) Calculate the mean (average) of a linear variable, overall and by groups: acs_srvyr_design %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) acs_srvyr_design %&gt;% group_by( cit ) %&gt;% summarize( mean = survey_mean( poverty_level , na.rm = TRUE ) ) Technical Documentation Replication Example The example below matches statistics, standard errors, and margin of errors from the 2021 PUMS tallies: Match the sum of the weights: stopifnot( round( coef( svytotal( ~ one , acs_design ) ) , 0 ) == 5039877 ) Compute the population by age: pums_estimate &lt;- c(288139L, 299245L, 336727L, 334606L, 327102L, 635004L, 641405L, 615709L, 335431L, 341926L, 538367L, 265742L, 80474L) pums_standard_error &lt;- c(2727L, 5368L, 6067L, 4082L, 4485L, 5716L, 4420L, 3706L, 4836L, 5100L, 2158L, 3363L, 3186L) pums_margin_of_error &lt;- c(4486L, 8830L, 9981L, 6715L, 7378L, 9402L, 7271L, 6096L, 7956L, 8389L, 3550L, 5532L, 5240L) results &lt;- svytotal( ~ as.numeric( agep %in% 0:4 ) + as.numeric( agep %in% 5:9 ) + as.numeric( agep %in% 10:14 ) + as.numeric( agep %in% 15:19 ) + as.numeric( agep %in% 20:24 ) + as.numeric( agep %in% 25:34 ) + as.numeric( agep %in% 35:44 ) + as.numeric( agep %in% 45:54 ) + as.numeric( agep %in% 55:59 ) + as.numeric( agep %in% 60:64 ) + as.numeric( agep %in% 65:74 ) + as.numeric( agep %in% 75:84 ) + as.numeric( agep %in% 85:100 ) , acs_design ) stopifnot( all( round( coef( results ) , 0 ) == pums_estimate ) ) stopifnot( all( round( SE( results ) , 0 ) == pums_standard_error ) ) stopifnot( all( round( SE( results ) * 1.645 , 0 ) == pums_margin_of_error ) ) "],["structural-equation-models-sem-with-complex-survey-data.html", "Structural Equation Models (SEM) with Complex Survey Data Load the 2008 Wave of the European Social Survey German and Spanish Microdata Two-factor CFA of attitudes toward the welfare state Invariance testing on Schwarz human values while accounting for the survey design. An example with a Latent Variable Regression", " Structural Equation Models (SEM) with Complex Survey Data Contributed by Dr. Daniel Oberski &lt;daniel.oberski@gmail.com&gt; The R lavaan.survey package by Dr. Daniel Oberski fits structural equation models to complex survey microdata, described in his JSS article. install.packages( &quot;lavaan.survey&quot; , repos = &quot;http://cran.rstudio.com/&quot; ) Load the 2008 Wave of the European Social Survey German and Spanish Microdata library(lodown) library(lavaan.survey) options( survey.lonely.psu = &quot;adjust&quot; ) # retrieve a listing of all available extracts for the european social survey ess_cat &lt;- get_catalog( &quot;ess&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; ) ) # limit the catalog to only wave #4 for germany and spain ess_cat &lt;- subset( ess_cat , wave == 4 &amp; grepl( &quot;c=DE|c=ES&quot; , full_url ) ) # download the ess microdata lodown( &quot;ess&quot; , ess_cat , your_email = &quot;email@address.com&quot; ) Immediately pull the German files into the workspace: # load Germany&#39;s round four main data file.. ess4.de &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4DE.rds&quot; ) ) # load Germany&#39;s round four sample design data file (sddf).. ess4.de.sddf &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4_DE_SDDF.rds&quot; ) ) The stratify variable is not literally equal to the actual strata but contains more information (which we don’t need here). Create a new variable that only uses the actual stratification namely, East v. West Germany, by using a regular expression / string substitution function to take the data.frame object’s stratify variable, convert it to a character variable, search for a dash, and keep only the text before the dash. Then, convert that resultant vector of ones and twos into a factor variable, labeled East versus West Germany. ess4.de.sddf$stratify &lt;- factor( gsub( &quot;(\\\\d+)-.+&quot; , &quot;\\\\1&quot; , as.character( ess4.de.sddf$stratify ) ) ) levels(ess4.de.sddf$stratify) &lt;- c(&quot;West Germany&quot;, &quot;East Germany&quot;) Check against ESS documentation statement that “The number of sampling points is 109 in the West, and 59 in the East”: stopifnot(tapply(ess4.de.sddf$psu, ess4.de.sddf$stratify, function(x) length(unique(x))) == c(109, 59)) Merge these two files together, creating a single table: ess4.de.m &lt;- merge( ess4.de , ess4.de.sddf) stopifnot( nrow( ess4.de ) == nrow( ess4.de.m ) &amp; nrow( ess4.de.sddf ) == nrow( ess4.de.m ) ) Create a survey design object: ess4.de.design &lt;- svydesign( ids = ~psu , strata = ~stratify , probs = ~prob , data = ess4.de.m ) Two-factor CFA of attitudes toward the welfare state This analysis uses the model of the below article. Please see the article for more information. Roosma, F., Gelissen, J., &amp; van Oorschot, W. (2013). The multidimensionality of welfare state attitudes: a European cross-national study. Social indicators research, 113(1), 235-255. Formulate the two-factor CFA using lavaan syntax: model.cfa &lt;- &quot;range =~ gvjbevn + gvhlthc + gvslvol + gvslvue + gvcldcr + gvpdlwk goals =~ sbprvpv + sbeqsoc + sbcwkfm&quot; Fit the model using lavaan, accounting for possible nonnormality using the MLM estimator: fit.cfa.ml &lt;- lavaan( model.cfa , data = ess4.de.m , estimator = &quot;MLM&quot; , int.ov.free = TRUE , auto.var = TRUE , auto.fix.first = TRUE , auto.cov.lv.x = TRUE ) Show some fit measure results, note the “scaling correction” which accounts for nonnormality: fit.cfa.ml Fit the two-factor model while taking the survey design into account: fit.cfa.surv &lt;- lavaan.survey( fit.cfa.ml , survey.design = ess4.de.design ) Show some fit measure results, “scaling correction” now accounts for both nonnormality and survey design. fit.cfa.surv Display parameter estimates and standard errors accounting for survey design: summary( fit.cfa.surv , standardized = TRUE ) Invariance testing on Schwarz human values while accounting for the survey design. For more information on this analysis, see: Davidov, E., Schmidt, P., &amp; Schwartz, S. H. (2008). “Bringing values back in: The adequacy of the European Social Survey to measure values in 20 countries”. Public opinion quarterly, 72(3), 420-445. Test the measurement equivalence of Schwarz human values from round 4 of the ESS, comparing Germany with Spain. First load the Spanish data so these can be merged: # load Spain&#39;s round four main data file.. ess4.es &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4ES.rds&quot; ) ) # load Spain&#39;s round four sample design data file (sddf).. ess4.es.sddf &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4_ES_SDDF.rds&quot; ) ) Merge these two files together, creating a single table: ess4.es.m &lt;- merge( ess4.es , ess4.es.sddf) stopifnot( nrow( ess4.es ) == nrow( ess4.es.m ) &amp; nrow( ess4.es.sddf ) == nrow( ess4.es.m ) ) Make sure PSU names are unique between the two countries. Paste on a “de-” to the German PSUs, and by pasting an “es-” to the front of the Spanish PSUs. ess4.de.m$psu &lt;- paste( &quot;de&quot; , ess4.de.m$psu , sep=&quot;-&quot; ) ess4.es.m$psu &lt;- paste( &quot;es&quot; , ess4.es.m$psu , sep=&quot;-&quot; ) Stack the two countries into a single table, then construct a survey design: ess4.m &lt;- rbind( ess4.de.m , ess4.es.m ) ess4.design &lt;- svydesign( ids = ~psu, strata = ~stratify , probs = ~prob , data = ess4.m ) Model based on Schwarz human value theory. Note that this is the basic starting model, not the final model used by Davidov et al. They merge certain values and allow cross-loadings: free.values.model.syntax &lt;- &quot; Universalism =~ ipeqopt + ipudrst + impenv Benevolence =~ iphlppl + iplylfr Tradition =~ ipmodst + imptrad Conformity =~ ipfrule + ipbhprp Security =~ impsafe + ipstrgv &quot; Fit two-group configural invariance model: free.values.fit &lt;- lavaan( free.values.model.syntax , data = ess4.m , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE , estimator = &quot;MLM&quot; , group = &quot;cntry&quot; ) summary( free.values.fit , standardized = TRUE ) Fit a two-group metric invariance model: free.values.fit.eq &lt;- lavaan( free.values.model.syntax , data = ess4.m , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE , estimator = &quot;MLM&quot; , group = &quot;cntry&quot; , group.equal = &quot;loadings&quot; ) summary( free.values.fit.eq , standardized = TRUE ) Metric invariance test (anova() would work here too, but not below): lavTestLRT( free.values.fit , free.values.fit.eq , SB.classic = TRUE ) Compare chisquares of the survey and non-survey SEM analyses for the configural invariance model: free.values.fit.surv &lt;- lavaan.survey( free.values.fit , ess4.design ) free.values.fit free.values.fit.surv Compare chisquares of the survey and non-survey SEM analyses for the metric invariance model: free.values.fit.eq.surv &lt;- lavaan.survey( free.values.fit.eq , ess4.design ) free.values.fit.eq free.values.fit.eq.surv Perform metric invariance test accounting for the survey design: lavTestLRT(free.values.fit.surv, free.values.fit.eq.surv, SB.classic = TRUE) The two models are more dissimilar after survey design is accounted for. An example with a Latent Variable Regression See Davidov, E., Meuleman, B., Billiet, J., &amp; Schmidt, P. (2008). Values and support for immigration: A cross-country comparison. European Sociological Review, 24(5), 583-599. The human values scale again, but this time: only two value dimensions are modeled. the two latent value dimensions are used to predict anti-immigration attitudes in the two countries. a test is performed on the difference between countries in latent regression coefficients. reg.syntax &lt;- &quot; SelfTranscendence =~ ipeqopt + ipudrst + impenv + iphlppl + iplylfr Conservation =~ ipmodst + imptrad + ipfrule + ipbhprp + impsafe + ipstrgv ALLOW =~ imdfetn + impcntr ALLOW ~ SelfTranscendence + Conservation &quot; reg.vals.fit &lt;- lavaan( reg.syntax , data = ess4.m , group = &quot;cntry&quot; , estimator = &quot;MLM&quot; , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE ) reg.vals.fit.eq &lt;- lavaan( reg.syntax , data = ess4.m , group = &quot;cntry&quot; , group.equal = &quot;regressions&quot; , estimator = &quot;MLM&quot; , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE ) summary( reg.vals.fit.eq , standardize = TRUE ) Test whether the relationship between values and anti-immigration attitudes is equal in Germany and Spain: lavTestLRT( reg.vals.fit , reg.vals.fit.eq , SB.classic = TRUE) Now do the same but accounting for the sampling design: reg.vals.fit.surv &lt;- lavaan.survey( reg.vals.fit , ess4.design ) reg.vals.fit.eq.surv &lt;- lavaan.survey( reg.vals.fit.eq , ess4.design ) lavTestLRT(reg.vals.fit.surv, reg.vals.fit.eq.surv, SB.classic = TRUE) The two models are less dissimilar after survey design is accounted for. "],["statistically-significant-trends-with-multiple-years-of-complex-survey-data.html", "Statistically Significant Trends with Multiple Years of Complex Survey Data Data Importation Load Required Packages, Options, External Functions Harmonize and Stack Multiple Years of Survey Data Construct a Multi-Year Stacked Complex Survey Design Object Review the unadjusted results Calculate the Number of Joinpoints Needed Calculate the Adjusted Prevalence and Predicted Marginals Identify the Breakpoint/Changepoint Make statistically defensible statements about linear trends with complex survey data", " Statistically Significant Trends with Multiple Years of Complex Survey Data Contributed by Thomas Yokota &lt;thomasyokota@gmail.com&gt; Palermo Professor Vito Muggeo wrote the joinpoint analysis section of the code below to demonstrate that the segmented package eliminates the need for external (registration-only, windows-only, workflow-disrupting) software. survey package creator and professor Dr. Thomas Lumley wrote the svypredmeans function to replicate SUDAAN’s PREDMARG command and match the CDC to the decimal. Dr. Richard Lowry, M.D. at the Centers for Disease Control &amp; Prevention wrote the original linear trend analysis and then answered our infinite questions. Thanks to everyone. The purpose of this analysis is to make statistically valid statements such as, “there was a significant linear decrease in the prevalence of high school aged americans who have ever smoked a cigarette across the period 1999-2011” with complex sample survey data. This step-by-step walkthrough exactly reproduces the statistics presented in the Center for Disease Control &amp; Prevention’s (CDC) linear trend analysis, using free and open source methods rather than proprietary or restricted software. The example below displays only linearized designs (created with the svydesign function). For more detail about how to reproduce this analysis with a replicate-weighted design (created with the svrepdesign function), see the methods note below section #4. Data Importation Prior to running this analysis script, the Youth Risk Behavioral Surveillance System (YRBSS) 1991-2011 single-year files must all be loaded as R data files (.rda) on your local machine. Running the download automation script will create the appropriate files. If you need assistance with the data-loading step, first review the main YRBSS blog post. options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) library(lodown) # retrieve a listing of all available extracts for the youth risk behavioral surveillance system yrbss_cat &lt;- get_catalog( &quot;yrbss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; ) ) # limit the catalog to only years 2005-2015 yrbss_cat &lt;- subset( yrbss_cat , year %in% seq( 2005 , 2015 , 2 ) ) # download the yrbss microdata lodown( &quot;yrbss&quot; , yrbss_cat ) Load Required Packages, Options, External Functions # install.packages( c( &quot;segmented&quot; , &quot;ggplot2&quot; , &quot;ggthemes&quot; , &quot;texreg&quot; ) ) # Muggeo V. (2008) Segmented: an R package to fit regression models with broken-line relationships. R News, 8, 1: 20-25. library(segmented) library(ggplot2) library(ggthemes) library(texreg) Harmonize and Stack Multiple Years of Survey Data This step is clearly dataset-specific. In order for your trend analysis to work, you’ll need to figure out how to align the variables from multiple years of data into a trendable, stacked data.frame object. # initiate an empty `y` object y &lt;- NULL # loop through each year of YRBSS microdata for ( year in seq( 2005 , 2015 , 2 ) ){ # load the current year x &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; , paste0( year , &quot; main.rds&quot; ) ) ) # tack on a `year` column x$year &lt;- year if( year == 2005 ) x$raceeth &lt;- NA # stack that year of data alongside the others, # ignoring mis-matching columns y &lt;- rbind( x[ c( &quot;q2&quot; , &quot;q3&quot; , &quot;q4&quot; , &quot;qn10&quot; , &quot;year&quot; , &quot;psu&quot; , &quot;stratum&quot; , &quot;weight&quot; , &quot;raceeth&quot; ) ] , y ) # clear the single-year of microdata from RAM rm( x ) } # convert every column to numeric type y[ , ] &lt;- sapply( y[ , ] , as.numeric ) # construct year-specific recodes so that # &quot;ever smoked a cigarette&quot; // grade // sex // race-ethnicity align across years y &lt;- transform( y , rode_with_drunk_driver = qn10 , raceeth = ifelse( year == 2005 , ifelse( q4 %in% 6 , 1 , ifelse( q4 %in% 3 , 2 , ifelse( q4 %in% c( 4 , 7 ) , 3 , ifelse( q4 %in% c( 1 , 2 , 5 , 8 ) , 4 , NA ) ) ) ) , ifelse( raceeth %in% 5 , 1 , ifelse( raceeth %in% 3 , 2 , ifelse( raceeth %in% c( 6 , 7 ) , 3 , ifelse( raceeth %in% c( 1 , 2 , 4 , 8 ) , 4 , NA ) ) ) ) ) , grade = ifelse( q3 == 5 , NA , as.numeric( q3 ) ) , sex = ifelse( q2 %in% 1:2 , q2 , NA ) ) # again remove unnecessary variables, keeping only the complex sample survey design columns # plus independent/dependent variables to be used in the regression analyses y &lt;- y[ c( &quot;year&quot; , &quot;psu&quot; , &quot;stratum&quot; , &quot;weight&quot; , &quot;rode_with_drunk_driver&quot; , &quot;raceeth&quot; , &quot;sex&quot; , &quot;grade&quot; ) ] # set female to the reference group y$sex &lt;- relevel( factor( y$sex ) , ref = &quot;2&quot; ) # set ever smoked=yes // white // 9th graders as the reference groups for ( i in c( &#39;rode_with_drunk_driver&#39; , &#39;raceeth&#39; , &#39;grade&#39; ) ) y[ , i ] &lt;- relevel( factor( y[ , i ] ) , ref = &quot;1&quot; ) Construct a Multi-Year Stacked Complex Survey Design Object Before constructing a multi-year stacked design object, check out ?contr.poly - this function implements polynomials used in our trend analysis during step #6. For more detail on this subject, see page 216 of Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences By Jacob Cohen, Patricia Cohen, Stephen G. West, Leona S. Aiken “The polynomials we have used as predictors to this point are natural polynomials, generated from the linear predictor by centering and the powering the linear predictor.” # extract a linear contrast vector of length eleven, # because we have eleven distinct years of yrbss data `seq( 2005 , 2015 , 2 )` c6l &lt;- contr.poly( 6 )[ , 1 ] # also extract a quadratic (squared) contrast vector c6q &lt;- contr.poly( 6 )[ , 2 ] # just in case, extract a cubic contrast vector c6c &lt;- contr.poly( 6 )[ , 3 ] # for each record in the data set, tack on the linear, quadratic, and cubic contrast value # these contrast values will serve as replacement for the linear `year` variable in any regression. # year^1 term (linear) y$t6l &lt;- c6l[ match( y$year , seq( 2005 , 2015 , 2 ) ) ] # year^2 term (quadratic) y$t6q &lt;- c6q[ match( y$year , seq( 2005 , 2015 , 2 ) ) ] # year^3 term (cubic) y$t6c &lt;- c6c[ match( y$year , seq( 2005 , 2015 , 2 ) ) ] # construct a complex sample survey design object # stacking multiple years and accounting for `year` in the nested strata des &lt;- svydesign( id = ~psu , strata = ~interaction( stratum , year ) , data = y , weights = ~weight , nest = TRUE ) Now we’ve got a multi-year stack of complex survey designs with linear, quadratic, and cubic contrast values appended. If you’d like more detail about stacking multiple years of complex survey data, review the CDC’s manual on the topic. Hopefully we won’t need anything beyond cubic, but let’s find out. Methods note about how to stack replication designs: This is only relevant if you are trying to create a des like the object above but just have replicate weights and do not have the clustering information (psu). It is straightforward to construct a replication design from a linearized design (see as.svrepdesign). However, for privacy reasons, going in the opposite direction is much more challenging. Therefore, you’ll need to do some dataset-specific homework on how to best stack multiple years of a replicate-weighted design you construct a multiple-year-stacked survey design like the object above. If you’d like to experiment with how the two approaches differ (theoretically, very little), these publicly-available survey data sets include both replicate weights and, separately, clustering information: Medical Expenditure Panel Survey National Health and Nutrition Examination Survey Consumer Expenditure Survey In most cases, omitting the year variable from the strata = ~interaction( stratum , year ) construction of des above will make your standard errors larger (conservative) -&gt; ergo -&gt; you can probably just rbind( file_with_repweights_year_one , file_with_repweights_year_two , ... ) so long as the survey design has not changed in structure over the time period that you are analyzing. Once you have the rbound replicate weights object for every year, you could just construct one huge multi-year svrepdesign object. Make sure you include scale, rscales, rho, and whatever else the svrepdesign() call asks for. If you are worried you missed something, check attributes( your_single_year_replication_design_object ). This solution is likely to be a decent approach in most cases. If you need to be very conservative with your computation of trend statistical significance, you might attempt to re-construct fake clusters for yourself using a regression. Search for “malicious” in this confidentiality explanation document. The purpose here, though, isn’t to identify individual respondents in the dataset, it’s to get a variable like psu above that gives you reasonable standard errors. Look for the object your.replicate.weights in that script. You could reconstruct a fake psu for each record in your data set with something as easy as.. # # fake_psu should be a one-record-per-person vector object # # that can immediately be appended onto your data set. # fake_psu &lt;- kmeans( your.replicate.weights , 20 ) ..where 20 is the (completely made up) number of clusters x strata. Hopefully the methodology documents (or the people who wrote them) will at least tell you how many clusters there were in the original sample, even if the clusters themselves were not disclosed. At the point you’ve made fake clusters, they will surely be worse than the real clusters (i.e. conservative standard errors) and you can construct a multiple-year survey design with: # des &lt;- svydesign( id = ~ your_fake_psus , strata = ~ year , data = y , weights = ~ weight , nest = TRUE ) This approach will probably be conservative probably. Review the unadjusted results Here’s the change over time for smoking prevalence among youth. Unadjusted prevalence rates (Figure 1) suggest a significant change in smoking prevalence. # immediately remove records with missing smoking status des_ns &lt;- subset( des , !is.na( rode_with_drunk_driver ) ) # calculate unadjusted, un-anythinged &quot;ever smoked&quot; rates by year # note that this reproduces the unadjusted &quot;ever smoked&quot; statistics at the top of # pdf page 6 of http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_conducting_trend_analyses.pdf unadjusted &lt;- svyby( ~ rode_with_drunk_driver , ~ year , svymean , design = des_ns , vartype = c( &#39;ci&#39; , &#39;se&#39; ) ) # coerce that result into a `data.frame` object my_plot &lt;- data.frame( unadjusted ) # plot the unadjusted decline in smoking ggplot( my_plot , aes( x = year, y = rode_with_drunk_driver1 ) ) + geom_point() + geom_errorbar( aes( ymax = ci_u.rode_with_drunk_driver1 , ymin = ci_l.rode_with_drunk_driver1 ) , width = .2 ) + geom_line() + theme_tufte() + ggtitle( &quot;Figure 1. Unadjusted smoking prevalence 1999-2011&quot; ) + theme( plot.title = element_text( size = 9 , face = &quot;bold&quot; ) ) Calculate the Number of Joinpoints Needed Using the orthogonal coefficients (linear, quadratic, cubic terms) that we previously added to our data.frame object before constructing the multi-year stacked survey design, let’s now determine how many joinpoints will be needed for a trend analysis. Epidemiological models typically control for possible confounding variables such as sex and race, so let’s add them in with the linear, cubic, and quadratic year terms. Calculate the “ever smoked” binomial regression, adjusted by sex, age, race-ethnicity, and a linear year contrast. linyear &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , family = quasibinomial ) summary( linyear ) The linear year-contrast variable t11l is hugely significant here. Therefore, there is probably going to be some sort of trend. A linear trend does not need joinpoints. Not one, just zero joinpoints. If the linear term were the only significant term (out of linear, quadratic, cubic, etc.), then we would not need to calculate a joinpoint. In other words, we would not need to figure out where to best break our time trend into two, three, or even four segments. The linear trend is significant, so we should keep going. Interpretation note about segments of time: The linear term t11l was significant, so we probably have a significant linear trend somewhere to report. Now we need to figure out when that significant linear trend started and when it ended. It might be semantically true that there was a significant linear decrease in high school aged smoking over the entire period of our data 1991-2011; however, it’s inexact, unrefined to give up after only detecting a linear trend. The purpose of the following few steps is really to cordon off different time points from one another. As you’ll see later, there actually was not any detectable decrease from 1991-1999. The entirety of the decline in smoking occurred over the period from 1999-2011. So these next (methodologically tricky) steps serve to provide you and your audience with a more careful statement of statistical significance. It’s not technically wrong to conclude that smoking declined over the period of 1991-2011, it’s just verbose. Think of it as the difference between “humans first walked on the moon in the sixties” and “humans first walked on the moon in 1969” - both statements are correct, but the latter exhibits greater scientific precision. Calculate the “ever smoked” binomial regression, adjusted by sex, age, race-ethnicity, and both linear and quadratic year contrasts. Notice the addition of t11q. quadyear &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l + t6q , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , family = quasibinomial ) summary( quadyear ) The linear year-contrast variable is hugely significant here but the quadratic year-contrast variable is also significant. Therefore, we should use joinpoint software for this analysis. A significant quadratic trend needs one joinpoint. Since both linear and quadratic terms are significant, we should move ahead and test whether the cubic term is also significant. Calculate the “ever smoked” binomial regression, adjusted by sex, age, race-ethnicity, and linear, quadratic, and cubic year contrasts. Notice the addition of t11c. cubyear &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l + t6q + t6c , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , family = quasibinomial ) summary( cubyear ) The cubic year-contrast term is not significant in this model. Therefore, we should stop testing the shape of this line. In other words, we can stop at a quadratic trend and do not need a cubic trend. That means we can stop at a single joinpoint. Remember: a linear trend requires zero joinpoints, a quadratic trend typically requires one joinpoint, a cubic trend usually requires two, and on and on. Note: if the cubic trend were significant, then we would increase the number of joinpoints to two instead of one but since the cubic term is not significant, we should stop with the previous regression. If we keep getting significant trends, we ought to continue testing whether higher terms continue to be significant. So year^4 requires three joinpoints, year^5 requires four joinpoints, and so on. If these terms continued to be significant, we would need to return to step #4 and add additional year^n terms to the model. Just for coherence’s sake, let’s assemble all of these results into a single table where you can see the linear, quadratic, and cubic models side-by-side. The quadratic trend best describes the relationship between prevalence of smoking and change-over-time. The decision to test beyond linear trends, however, is a decision for the individual researcher to make. It is a decision that can be driven by theoretical issues, existing literature, or the availability of data. htmlreg(list(linyear , quadyear , cubyear), doctype = F, html.tag = F, inline.css = T, head.tag = F, body.tag = F, center = F, single.row = T, caption = &quot;Table 1. Testing for linear trends&quot;) Calculate the Adjusted Prevalence and Predicted Marginals First, calculate the survey-year-independent predictor effects and store these results into a separate object. marginals &lt;- svyglm( formula = I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade , design = des_ns , family = quasibinomial ) Second, run these marginals through the svypredmeans function written by Dr. Thomas Lumley. For any archaeology fans out there, this function emulates the PREDMARG statement in the ancient language of SUDAAN. ( means_for_joinpoint &lt;- svypredmeans( marginals , ~factor( year ) ) ) Finally, clean up these results a bit in preparation for a joinpoint analysis. # coerce the results to a data.frame object means_for_joinpoint &lt;- as.data.frame( means_for_joinpoint ) # extract the row names as the survey year means_for_joinpoint$year &lt;- as.numeric( rownames( means_for_joinpoint ) ) # must be sorted, just in case it&#39;s not already means_for_joinpoint &lt;- means_for_joinpoint[ order( means_for_joinpoint$year ) , ] # rename columns so they do not conflict with variables in memory names( means_for_joinpoint ) &lt;- c( &#39;mean&#39; , &#39;se&#39; , &#39;yr&#39; ) # the above line is only because the ?segmented function (used below) # does not work if an object of the same name is also in memory. another_plot &lt;- means_for_joinpoint another_plot$ci_l.mean &lt;- another_plot$mean - (1.96 * another_plot$se) another_plot$ci_u.mean &lt;- another_plot$mean + (1.96 * another_plot$se) ggplot(another_plot, aes(x = yr, y = mean)) + geom_point() + geom_errorbar(aes(ymax = ci_u.mean, ymin = ci_l.mean), width=.2) + geom_line() + theme_tufte() + ggtitle(&quot;Figure 2. Adjusted smoking prevalence 1999-2011&quot;) + theme(plot.title = element_text(size=9, face=&quot;bold&quot;)) Identify the Breakpoint/Changepoint The original CDC analysis recommended some external software from the National Cancer Institute, which only runs on selected platforms. Dr. Vito Muggeo wrote this within-R solution using his segmented package available on CRAN. Let’s take a look at how confident we are in the value at each adjusted timepoint. Carrying out a trend analysis requires creating new weights to fit a piecewise linear regression. Figure 3 shows the relationship between variance at each datum and weighting. Larger circles display greater uncertainty and therefore lower weight. ggplot( means_for_joinpoint , aes( x = yr , y = mean ) ) + geom_point( aes( size = se ) ) + theme_tufte() + ggtitle( &quot;Figure 3. Standard Error at each timepoint\\n(smaller dots indicate greater confidence in each adjusted value)&quot; ) First, create that weight variable. means_for_joinpoint$wgt &lt;- with( means_for_joinpoint, ( mean / se ) ^ 2 ) Second, fit a piecewise linear regression. # estimate the &#39;starting&#39; linear model with the usual &quot;lm&quot; function using the log values and the weights. o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = means_for_joinpoint ) Now that the regression has been structured correctly, estimate the year that our complex survey trend should be broken into two segments (the changepoint/breakpoint/joinpoint). # the segmented() function uses a random process in its algorithm. # setting the random seed ensures reproducibility set.seed( 2015 ) # add a segmented variable (`yr` in this example) with 1 breakpoint os &lt;- segmented( o , ~yr ) # `os` is now a `segmented` object, which means it includes information on the fitted model, # such as parameter estimates, standard errors, residuals. summary( os ) See the Estimated Break-Point(s) in that result? That’s the critical number from this joinpoint analysis. Note that the above number is not an integer! The R segmented package uses an iterative procedure (described in the article below) and therefore between-year solutions are returned. The joinpoint software implements two estimating algorithms: the grid-search and the Hudson algorithm. For more detail about these methods, see Muggeo V. (2003) Estimating regression models with unknown break-points. Statistics in Medicine, 22: 3055-3071.. # figuring out the breakpoint year was the purpose of this joinpoint analysis. ( your_breakpoint &lt;- round( as.vector( os$psi[, &quot;Est.&quot; ] ) ) ) # so. that&#39;s a joinpoint. that&#39;s where the two line segments join. okay? # obtain the annual percent change (APC=) estimates for each time point slope( os , APC = TRUE ) The returned CIs for the annual percent change (APC) may be different from the ones returned by NCI’s Joinpoint Software; for further details, check out Muggeo V. (2010) A Comment on `Estimating average annual per cent change in trend analysis’ by Clegg et al., Statistics in Medicine; 28, 3670-3682. Statistics in Medicine, 29, 1958-1960. This analysis returned similar results to the NCI’s Joinpoint Regression Program by estimating a changepoint at year=1999 - and, more precisely, that the start of that decreasing trend in smoking prevalence happened at an APC of -3.92 percent. That is, slope2 from the output above. Make statistically defensible statements about linear trends with complex survey data After identifying the change point for smoking prevalence, we can create two regression models (one for each time segment). (If we had two joinpoints, we would need three regression models.) The first model covers the years leading up to (and including) the changepoint (i.e., 1991 to 1999). The second model includes the years from the changepoint forward (i.e., 1999 to 2011). So start with 1991, 1993, 1995, 1997, 1999, the five year-points before (and including 1999). # calculate a five-timepoint linear contrast vector c3l &lt;- contr.poly( 3 )[ , 1 ] # tack the five-timepoint linear contrast vectors onto the current survey design object des_ns &lt;- update( des_ns , t3l = c3l[ match( year , seq( 2005 , 2009 , 2 ) ) ] ) pre_91_99 &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t3l , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 &amp; year &lt;= 2009 ) , family = quasibinomial ) summary( pre_91_99 ) Reproduce the sentence on pdf page 6 of the original document. In this example, T5L_L had a p-value=0.52261 and beta=0.03704. Therefore, there was “no significant change in the prevalence of ever smoking a cigarette during 1991-1999.” Then move on to 1999, 2001, 2003, 2005, 2007, 2009, and 2011, the seven year-points after (and including 1999). # calculate a seven-timepoint linear contrast vector c4l &lt;- contr.poly( 4 )[ , 1 ] # tack the seven-timepoint linear contrast vectors onto the current survey design object des_ns &lt;- update( des_ns , t4l = c4l[ match( year , seq( 2009 , 2015 , 2 ) ) ] ) post_99_11 &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t4l , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 &amp; year &gt;= 2009 ) , family = quasibinomial ) summary( post_99_11 ) Reproduce the sentence on pdf page 6 of the original document. In this example, T7L_R had a p-value&lt;0.0001 and beta=-0.99165. Therefore, there was a “significant linear decrease in the prevalence of ever smoking a cigarette during 1999-2011.” Note also that the 1999-2011 time period saw a linear decrease, which supports the APC estimate in step #8. Here’s everything displayed as a single coherent table. htmlreg(list(pre_91_99, post_99_11), doctype = F, html.tag = F, inline.css = T, head.tag = F, body.tag = F, center = F, single.row = T, caption = &quot;Table 2. Linear trends pre-post changepoint&quot;) This analysis may complement qualitative evaluation on prevalence changes observed from surveillance data by providing quantitative evidence, such as when a change point occurred. This analysis does not explain why or how changes in trends occur. "],["youth-risk-behavior-surveillance-system-yrbss.html", "Youth Risk Behavior Surveillance System (YRBSS) Download, Import, Preparation Analysis Examples with the survey library   Intermish Analysis Examples with srvyr   Technical Documentation Replication Example", " Youth Risk Behavior Surveillance System (YRBSS) The high school edition of the Behavioral Risk Factor Surveillance System (BRFSS), a scientific study of good kids who do bad things. One table with one row per sampled youth respondent. A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States. Released biennially since 1993. Administered by the Centers for Disease Control and Prevention. Please skim before you begin: Methodology of the Youth Risk Behavior Surveillance System Wikipedia Entry This poem # maladolescence # epidemiology # sex, drugs, rock and roll Download, Import, Preparation Load the SAScii library to interpret a SAS input program, and also re-arrange the SAS input program: library(SAScii) sas_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/2019XXH-SAS-Input-Program.sas&quot; sas_text &lt;- tolower( readLines( sas_url ) ) # find the (out of numerical order) # `site` location variable&#39;s position # within the SAS input program site_location &lt;- which( sas_text == &#39;@1 site $3.&#39; ) # find the start field&#39;s position # within the SAS input program input_location &lt;- which( sas_text == &quot;input&quot; ) # create a vector from 1 to the length of the text file sas_length &lt;- seq( length( sas_text ) ) # remove the site_location sas_length &lt;- sas_length[ -site_location ] # re-insert the site variable&#39;s location # immediately after the starting position sas_reorder &lt;- c( sas_length[ seq( input_location ) ] , site_location , sas_length[ seq( input_location + 1 , length( sas_length ) ) ] ) # re-order the sas text file sas_text &lt;- sas_text[ sas_reorder ] sas_tf &lt;- tempfile() writeLines( sas_text , sas_tf ) Download and import the national file: dat_tf &lt;- tempfile() dat_url &lt;- &quot;https://www.cdc.gov/healthyyouth/data/yrbs/files/2019/XXH2019_YRBS_Data.dat&quot; download.file( dat_url , dat_tf , mode = &#39;wb&#39; ) yrbss_df &lt;- read.SAScii( dat_tf , sas_tf ) names( yrbss_df ) &lt;- tolower( names( yrbss_df ) ) yrbss_df[ , &#39;one&#39; ] &lt;- 1 Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) yrbss_design &lt;- svydesign( ~ psu , strata = ~ stratum , data = yrbss_df , weights = ~ weight , nest = TRUE ) Variable Recoding Add new columns to the data set: yrbss_design &lt;- update( yrbss_design , q2 = q2 , never_rarely_wore_seat_belt = as.numeric( qn8 == 1 ) , ever_used_marijuana = as.numeric( qn45 == 1 ) , tried_to_quit_tobacco_past_year = as.numeric( q39 == 2 ) , used_tobacco_past_year = as.numeric( q39 &gt; 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( yrbss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , yrbss_design ) svyby( ~ one , ~ ever_used_marijuana , yrbss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_used_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ bmipct , yrbss_design , 0.5 , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ tried_to_quit_tobacco_past_year , denominator = ~ used_tobacco_past_year , yrbss_design , na.rm = TRUE ) Subsetting Restrict the survey design to youths who ever drank alcohol: sub_yrbss_design &lt;- subset( yrbss_design , qn40 &gt; 1 ) Calculate the mean (average) of this subset: svymean( ~ bmipct , sub_yrbss_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ bmipct , ~ ever_used_marijuana , yrbss_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( yrbss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ bmipct , yrbss_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( bmipct ~ never_rarely_wore_seat_belt , yrbss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( bmipct ~ never_rarely_wore_seat_belt + q2 , yrbss_design ) summary( glm_result ) Intermish https://en.wikipedia.org/wiki/Lorem_ipsum Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for YRBSS users, this code replicates previously-presented examples: library(srvyr) yrbss_srvyr_design &lt;- as_survey( yrbss_design ) Calculate the mean (average) of a linear variable, overall and by groups: yrbss_srvyr_design %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) yrbss_srvyr_design %&gt;% group_by( ever_used_marijuana ) %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) Technical Documentation Replication Example The example below matches statistics, standard errors, and confidence intervals from the “never/rarely wore seat belt” row of PDF page 29 of this CDC analysis software document. unwtd_count_result &lt;- unwtd.count( ~ never_rarely_wore_seat_belt , yrbss_design ) stopifnot( coef( unwtd_count_result ) == 11149 ) wtd_n_result &lt;- svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_seat_belt ) ) ) stopifnot( round( coef( wtd_n_result ) , 0 ) == 12132 ) share_result &lt;- svymean( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE ) stopifnot( round( coef( share_result ) , 4 ) == .0654 ) stopifnot( round( SE( share_result ) , 4 ) == .0065 ) ci_result &lt;- svyciprop( ~ never_rarely_wore_seat_belt , yrbss_design , na.rm = TRUE , method = &quot;beta&quot; ) stopifnot( round( confint( ci_result )[1] , 4 ) == 0.0529 ) stopifnot( round( confint( ci_result )[2] , 2 ) == 0.08 ) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
