[["index.html", "Analyze Survey Data for Free Step by Step Instructions to Explore Public Microdata from an Easy to Type Website", " Analyze Survey Data for Free Step by Step Instructions to Explore Public Microdata from an Easy to Type Website Edited by Anthony Joseph Damico &lt;ajdamico@gmail.com&gt;. Contact me directly if you have funding available to add chapters to this book, or for consultancy work in survey analysis or syntax translation across SAS, SPSS, Stata, SUDAAN, and R. Please ask questions about the content of this book at stackoverflow.com with the R and survey tags. Public data is the original crowdsourcing. "],["prerequisites.html", "Prerequisites", " Prerequisites This book assumes a basic understanding of the R language. If you like my style of pedagogy, you could try watching my introductory video lectures. Otherwise, review the R learning options at flowingdata.com. The R lodown package depends on most of the packages used in this text, so these three lines should install many of the R packages presented here, including the survey and RSQLite R packages. Windows users may need to install the external software Rtools prior to installing lodown from github. install.packages( &quot;devtools&quot; , repos = &quot;http://cran.rstudio.com/&quot; ) library(devtools) install_github( &quot;ajdamico/lodown&quot; , dependencies = TRUE ) The survey microdata presented in this book require the R survey package by Dr. Thomas Lumley at the University of Auckland. Dr. Lumley wrote a textbook to showcase that software. The R convey package estimates measures of inequality, poverty and wellbeing. Guilherme Jacob, Dr. Djalma Pessoa, and I have written this book about inequality measurement with complex survey microdata to accompany the software. install.packages( &quot;convey&quot; , repos = &quot;http://cran.rstudio.com/&quot; ) The R srvyr package by Greg Freedman Ellis allows dplyr-like syntax for many survey package commands. For detailed usage examples, review his vignettes. install.packages( &quot;srvyr&quot; , repos = &quot;http://cran.rstudio.com/&quot; ) "],["the-census-of-82-countries-made-laptop-friendly.html", "The Census of 82 Countries Made Laptop-Friendly", " The Census of 82 Countries Made Laptop-Friendly "],["maps-and-the-art-of-survey-weighted-maintenance.html", "Maps and the Art of Survey-Weighted Maintenance", " Maps and the Art of Survey-Weighted Maintenance "],["statistically-significant-trends-with-multiple-years-of-complex-survey-data.html", "Statistically Significant Trends with Multiple Years of Complex Survey Data Data Importation Load Required Packages, Options, External Functions Harmonize and Stack Multiple Years of Survey Data Construct a Multi-Year Stacked Complex Survey Design Object Review the unadjusted results Calculate the Number of Joinpoints Needed Calculate the Adjusted Prevalence and Predicted Marginals Identify the Breakpoint/Changepoint Make statistically defensible statements about linear trends with complex survey data", " Statistically Significant Trends with Multiple Years of Complex Survey Data Contributed by Thomas Yokota &lt;thomasyokota@gmail.com&gt; Palermo Professor Vito Muggeo wrote the joinpoint analysis section of the code below to demonstrate that the segmented package eliminates the need for external (registration-only, windows-only, workflow-disrupting) software. survey package creator and professor Dr. Thomas Lumley wrote the svypredmeans function to replicate SUDAAN’s PREDMARG command and match the CDC to the decimal. Dr. Richard Lowry, M.D. at the Centers for Disease Control &amp; Prevention wrote the original linear trend analysis and then answered our infinite questions. Thanks to everyone. The purpose of this analysis is to make statistically valid statements such as, “there was a significant linear decrease in the prevalence of high school aged americans who have ever smoked a cigarette across the period 1999-2011” with complex sample survey data. This step-by-step walkthrough exactly reproduces the statistics presented in the Center for Disease Control &amp; Prevention’s (CDC) linear trend analysis, using free and open source methods rather than proprietary or restricted software. The example below displays only linearized designs (created with the svydesign function). For more detail about how to reproduce this analysis with a replicate-weighted design (created with the svrepdesign function), see the methods note below section #4. Data Importation Prior to running this analysis script, the Youth Risk Behavioral Surveillance System (YRBSS) 1991-2011 single-year files must all be loaded as R data files (.rda) on your local machine. Running the download automation script will create the appropriate files. If you need assistance with the data-loading step, first review the main YRBSS blog post. options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) library(lodown) # retrieve a listing of all available extracts for the youth risk behavioral surveillance system yrbss_cat &lt;- get_catalog( &quot;yrbss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; ) ) # limit the catalog to only years 2005-2015 yrbss_cat &lt;- subset( yrbss_cat , year %in% seq( 2005 , 2015 , 2 ) ) # download the yrbss microdata lodown( &quot;yrbss&quot; , yrbss_cat ) Load Required Packages, Options, External Functions # install.packages( c( &quot;segmented&quot; , &quot;ggplot2&quot; , &quot;ggthemes&quot; , &quot;texreg&quot; ) ) # Muggeo V. (2008) Segmented: an R package to fit regression models with broken-line relationships. R News, 8, 1: 20-25. library(segmented) library(ggplot2) library(ggthemes) library(texreg) Harmonize and Stack Multiple Years of Survey Data This step is clearly dataset-specific. In order for your trend analysis to work, you’ll need to figure out how to align the variables from multiple years of data into a trendable, stacked data.frame object. # initiate an empty `y` object y &lt;- NULL # loop through each year of YRBSS microdata for ( year in seq( 2005 , 2015 , 2 ) ){ # load the current year x &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; , paste0( year , &quot; main.rds&quot; ) ) ) # tack on a `year` column x$year &lt;- year if( year == 2005 ) x$raceeth &lt;- NA # stack that year of data alongside the others, # ignoring mis-matching columns y &lt;- rbind( x[ c( &quot;q2&quot; , &quot;q3&quot; , &quot;q4&quot; , &quot;qn10&quot; , &quot;year&quot; , &quot;psu&quot; , &quot;stratum&quot; , &quot;weight&quot; , &quot;raceeth&quot; ) ] , y ) # clear the single-year of microdata from RAM rm( x ) } # convert every column to numeric type y[ , ] &lt;- sapply( y[ , ] , as.numeric ) # construct year-specific recodes so that # &quot;ever smoked a cigarette&quot; // grade // sex // race-ethnicity align across years y &lt;- transform( y , rode_with_drunk_driver = qn10 , raceeth = ifelse( year == 2005 , ifelse( q4 %in% 6 , 1 , ifelse( q4 %in% 3 , 2 , ifelse( q4 %in% c( 4 , 7 ) , 3 , ifelse( q4 %in% c( 1 , 2 , 5 , 8 ) , 4 , NA ) ) ) ) , ifelse( raceeth %in% 5 , 1 , ifelse( raceeth %in% 3 , 2 , ifelse( raceeth %in% c( 6 , 7 ) , 3 , ifelse( raceeth %in% c( 1 , 2 , 4 , 8 ) , 4 , NA ) ) ) ) ) , grade = ifelse( q3 == 5 , NA , as.numeric( q3 ) ) , sex = ifelse( q2 %in% 1:2 , q2 , NA ) ) # again remove unnecessary variables, keeping only the complex sample survey design columns # plus independent/dependent variables to be used in the regression analyses y &lt;- y[ c( &quot;year&quot; , &quot;psu&quot; , &quot;stratum&quot; , &quot;weight&quot; , &quot;rode_with_drunk_driver&quot; , &quot;raceeth&quot; , &quot;sex&quot; , &quot;grade&quot; ) ] # set female to the reference group y$sex &lt;- relevel( factor( y$sex ) , ref = &quot;2&quot; ) # set ever smoked=yes // white // 9th graders as the reference groups for ( i in c( &#39;rode_with_drunk_driver&#39; , &#39;raceeth&#39; , &#39;grade&#39; ) ) y[ , i ] &lt;- relevel( factor( y[ , i ] ) , ref = &quot;1&quot; ) Construct a Multi-Year Stacked Complex Survey Design Object Before constructing a multi-year stacked design object, check out ?contr.poly - this function implements polynomials used in our trend analysis during step #6. For more detail on this subject, see page 216 of Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences By Jacob Cohen, Patricia Cohen, Stephen G. West, Leona S. Aiken “The polynomials we have used as predictors to this point are natural polynomials, generated from the linear predictor by centering and the powering the linear predictor.” # extract a linear contrast vector of length eleven, # because we have eleven distinct years of yrbss data `seq( 2005 , 2015 , 2 )` c6l &lt;- contr.poly( 6 )[ , 1 ] # also extract a quadratic (squared) contrast vector c6q &lt;- contr.poly( 6 )[ , 2 ] # just in case, extract a cubic contrast vector c6c &lt;- contr.poly( 6 )[ , 3 ] # for each record in the data set, tack on the linear, quadratic, and cubic contrast value # these contrast values will serve as replacement for the linear `year` variable in any regression. # year^1 term (linear) y$t6l &lt;- c6l[ match( y$year , seq( 2005 , 2015 , 2 ) ) ] # year^2 term (quadratic) y$t6q &lt;- c6q[ match( y$year , seq( 2005 , 2015 , 2 ) ) ] # year^3 term (cubic) y$t6c &lt;- c6c[ match( y$year , seq( 2005 , 2015 , 2 ) ) ] # construct a complex sample survey design object # stacking multiple years and accounting for `year` in the nested strata des &lt;- svydesign( id = ~psu , strata = ~interaction( stratum , year ) , data = y , weights = ~weight , nest = TRUE ) Now we’ve got a multi-year stack of complex survey designs with linear, quadratic, and cubic contrast values appended. If you’d like more detail about stacking multiple years of complex survey data, review the CDC’s manual on the topic. Hopefully we won’t need anything beyond cubic, but let’s find out. Methods note about how to stack replication designs: This is only relevant if you are trying to create a des like the object above but just have replicate weights and do not have the clustering information (psu). It is straightforward to construct a replication design from a linearized design (see as.svrepdesign). However, for privacy reasons, going in the opposite direction is much more challenging. Therefore, you’ll need to do some dataset-specific homework on how to best stack multiple years of a replicate-weighted design you construct a multiple-year-stacked survey design like the object above. If you’d like to experiment with how the two approaches differ (theoretically, very little), these publicly-available survey data sets include both replicate weights and, separately, clustering information: Medical Expenditure Panel Survey National Health and Nutrition Examination Survey Consumer Expenditure Survey In most cases, omitting the year variable from the strata = ~interaction( stratum , year ) construction of des above will make your standard errors larger (conservative) -&gt; ergo -&gt; you can probably just rbind( file_with_repweights_year_one , file_with_repweights_year_two , ... ) so long as the survey design has not changed in structure over the time period that you are analyzing. Once you have the rbound replicate weights object for every year, you could just construct one huge multi-year svrepdesign object. Make sure you include scale, rscales, rho, and whatever else the svrepdesign() call asks for. If you are worried you missed something, check attributes( your_single_year_replication_design_object ). This solution is likely to be a decent approach in most cases. If you need to be very conservative with your computation of trend statistical significance, you might attempt to re-construct fake clusters for yourself using a regression. Search for “malicious” in this confidentiality explanation document. The purpose here, though, isn’t to identify individual respondents in the dataset, it’s to get a variable like psu above that gives you reasonable standard errors. Look for the object your.replicate.weights in that script. You could reconstruct a fake psu for each record in your data set with something as easy as.. # # fake_psu should be a one-record-per-person vector object # # that can immediately be appended onto your data set. # fake_psu &lt;- kmeans( your.replicate.weights , 20 ) ..where 20 is the (completely made up) number of clusters x strata. Hopefully the methodology documents (or the people who wrote them) will at least tell you how many clusters there were in the original sample, even if the clusters themselves were not disclosed. At the point you’ve made fake clusters, they will surely be worse than the real clusters (i.e. conservative standard errors) and you can construct a multiple-year survey design with: # des &lt;- svydesign( id = ~ your_fake_psus , strata = ~ year , data = y , weights = ~ weight , nest = TRUE ) This approach will probably be conservative probably. Review the unadjusted results Here’s the change over time for smoking prevalence among youth. Unadjusted prevalence rates (Figure 1) suggest a significant change in smoking prevalence. # immediately remove records with missing smoking status des_ns &lt;- subset( des , !is.na( rode_with_drunk_driver ) ) # calculate unadjusted, un-anythinged &quot;ever smoked&quot; rates by year # note that this reproduces the unadjusted &quot;ever smoked&quot; statistics at the top of # pdf page 6 of http://www.cdc.gov/healthyyouth/yrbs/pdf/yrbs_conducting_trend_analyses.pdf unadjusted &lt;- svyby( ~ rode_with_drunk_driver , ~ year , svymean , design = des_ns , vartype = c( &#39;ci&#39; , &#39;se&#39; ) ) # coerce that result into a `data.frame` object my_plot &lt;- data.frame( unadjusted ) # plot the unadjusted decline in smoking ggplot( my_plot , aes( x = year, y = rode_with_drunk_driver1 ) ) + geom_point() + geom_errorbar( aes( ymax = ci_u.rode_with_drunk_driver1 , ymin = ci_l.rode_with_drunk_driver1 ) , width = .2 ) + geom_line() + theme_tufte() + ggtitle( &quot;Figure 1. Unadjusted smoking prevalence 1999-2011&quot; ) + theme( plot.title = element_text( size = 9 , face = &quot;bold&quot; ) ) Calculate the Number of Joinpoints Needed Using the orthogonal coefficients (linear, quadratic, cubic terms) that we previously added to our data.frame object before constructing the multi-year stacked survey design, let’s now determine how many joinpoints will be needed for a trend analysis. Epidemiological models typically control for possible confounding variables such as sex and race, so let’s add them in with the linear, cubic, and quadratic year terms. Calculate the “ever smoked” binomial regression, adjusted by sex, age, race-ethnicity, and a linear year contrast. linyear &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , family = quasibinomial ) summary( linyear ) The linear year-contrast variable t11l is hugely significant here. Therefore, there is probably going to be some sort of trend. A linear trend does not need joinpoints. Not one, just zero joinpoints. If the linear term were the only significant term (out of linear, quadratic, cubic, etc.), then we would not need to calculate a joinpoint. In other words, we would not need to figure out where to best break our time trend into two, three, or even four segments. The linear trend is significant, so we should keep going. Interpretation note about segments of time: The linear term t11l was significant, so we probably have a significant linear trend somewhere to report. Now we need to figure out when that significant linear trend started and when it ended. It might be semantically true that there was a significant linear decrease in high school aged smoking over the entire period of our data 1991-2011; however, it’s inexact, unrefined to give up after only detecting a linear trend. The purpose of the following few steps is really to cordon off different time points from one another. As you’ll see later, there actually was not any detectable decrease from 1991-1999. The entirety of the decline in smoking occurred over the period from 1999-2011. So these next (methodologically tricky) steps serve to provide you and your audience with a more careful statement of statistical significance. It’s not technically wrong to conclude that smoking declined over the period of 1991-2011, it’s just verbose. Think of it as the difference between “humans first walked on the moon in the sixties” and “humans first walked on the moon in 1969” - both statements are correct, but the latter exhibits greater scientific precision. Calculate the “ever smoked” binomial regression, adjusted by sex, age, race-ethnicity, and both linear and quadratic year contrasts. Notice the addition of t11q. quadyear &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l + t6q , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , family = quasibinomial ) summary( quadyear ) The linear year-contrast variable is hugely significant here but the quadratic year-contrast variable is also significant. Therefore, we should use joinpoint software for this analysis. A significant quadratic trend needs one joinpoint. Since both linear and quadratic terms are significant, we should move ahead and test whether the cubic term is also significant. Calculate the “ever smoked” binomial regression, adjusted by sex, age, race-ethnicity, and linear, quadratic, and cubic year contrasts. Notice the addition of t11c. cubyear &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t6l + t6q + t6c , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 ) , family = quasibinomial ) summary( cubyear ) The cubic year-contrast term is not significant in this model. Therefore, we should stop testing the shape of this line. In other words, we can stop at a quadratic trend and do not need a cubic trend. That means we can stop at a single joinpoint. Remember: a linear trend requires zero joinpoints, a quadratic trend typically requires one joinpoint, a cubic trend usually requires two, and on and on. Note: if the cubic trend were significant, then we would increase the number of joinpoints to two instead of one but since the cubic term is not significant, we should stop with the previous regression. If we keep getting significant trends, we ought to continue testing whether higher terms continue to be significant. So year^4 requires three joinpoints, year^5 requires four joinpoints, and so on. If these terms continued to be significant, we would need to return to step #4 and add additional year^n terms to the model. Just for coherence’s sake, let’s assemble all of these results into a single table where you can see the linear, quadratic, and cubic models side-by-side. The quadratic trend best describes the relationship between prevalence of smoking and change-over-time. The decision to test beyond linear trends, however, is a decision for the individual researcher to make. It is a decision that can be driven by theoretical issues, existing literature, or the availability of data. htmlreg(list(linyear , quadyear , cubyear), doctype = F, html.tag = F, inline.css = T, head.tag = F, body.tag = F, center = F, single.row = T, caption = &quot;Table 1. Testing for linear trends&quot;) Calculate the Adjusted Prevalence and Predicted Marginals First, calculate the survey-year-independent predictor effects and store these results into a separate object. marginals &lt;- svyglm( formula = I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade , design = des_ns , family = quasibinomial ) Second, run these marginals through the svypredmeans function written by Dr. Thomas Lumley. For any archaeology fans out there, this function emulates the PREDMARG statement in the ancient language of SUDAAN. ( means_for_joinpoint &lt;- svypredmeans( marginals , ~factor( year ) ) ) Finally, clean up these results a bit in preparation for a joinpoint analysis. # coerce the results to a data.frame object means_for_joinpoint &lt;- as.data.frame( means_for_joinpoint ) # extract the row names as the survey year means_for_joinpoint$year &lt;- as.numeric( rownames( means_for_joinpoint ) ) # must be sorted, just in case it&#39;s not already means_for_joinpoint &lt;- means_for_joinpoint[ order( means_for_joinpoint$year ) , ] # rename columns so they do not conflict with variables in memory names( means_for_joinpoint ) &lt;- c( &#39;mean&#39; , &#39;se&#39; , &#39;yr&#39; ) # the above line is only because the ?segmented function (used below) # does not work if an object of the same name is also in memory. another_plot &lt;- means_for_joinpoint another_plot$ci_l.mean &lt;- another_plot$mean - (1.96 * another_plot$se) another_plot$ci_u.mean &lt;- another_plot$mean + (1.96 * another_plot$se) ggplot(another_plot, aes(x = yr, y = mean)) + geom_point() + geom_errorbar(aes(ymax = ci_u.mean, ymin = ci_l.mean), width=.2) + geom_line() + theme_tufte() + ggtitle(&quot;Figure 2. Adjusted smoking prevalence 1999-2011&quot;) + theme(plot.title = element_text(size=9, face=&quot;bold&quot;)) Identify the Breakpoint/Changepoint The original CDC analysis recommended some external software from the National Cancer Institute, which only runs on selected platforms. Dr. Vito Muggeo wrote this within-R solution using his segmented package available on CRAN. Let’s take a look at how confident we are in the value at each adjusted timepoint. Carrying out a trend analysis requires creating new weights to fit a piecewise linear regression. Figure 3 shows the relationship between variance at each datum and weighting. Larger circles display greater uncertainty and therefore lower weight. ggplot( means_for_joinpoint , aes( x = yr , y = mean ) ) + geom_point( aes( size = se ) ) + theme_tufte() + ggtitle( &quot;Figure 3. Standard Error at each timepoint\\n(smaller dots indicate greater confidence in each adjusted value)&quot; ) First, create that weight variable. means_for_joinpoint$wgt &lt;- with( means_for_joinpoint, ( mean / se ) ^ 2 ) Second, fit a piecewise linear regression. # estimate the &#39;starting&#39; linear model with the usual &quot;lm&quot; function using the log values and the weights. o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = means_for_joinpoint ) Now that the regression has been structured correctly, estimate the year that our complex survey trend should be broken into two segments (the changepoint/breakpoint/joinpoint). # the segmented() function uses a random process in its algorithm. # setting the random seed ensures reproducibility set.seed( 2015 ) # add a segmented variable (`yr` in this example) with 1 breakpoint os &lt;- segmented( o , ~yr ) # `os` is now a `segmented` object, which means it includes information on the fitted model, # such as parameter estimates, standard errors, residuals. summary( os ) See the Estimated Break-Point(s) in that result? That’s the critical number from this joinpoint analysis. Note that the above number is not an integer! The R segmented package uses an iterative procedure (described in the article below) and therefore between-year solutions are returned. The joinpoint software implements two estimating algorithms: the grid-search and the Hudson algorithm. For more detail about these methods, see Muggeo V. (2003) Estimating regression models with unknown break-points. Statistics in Medicine, 22: 3055-3071.. # figuring out the breakpoint year was the purpose of this joinpoint analysis. ( your_breakpoint &lt;- round( as.vector( os$psi[, &quot;Est.&quot; ] ) ) ) # so. that&#39;s a joinpoint. that&#39;s where the two line segments join. okay? # obtain the annual percent change (APC=) estimates for each time point slope( os , APC = TRUE ) The returned CIs for the annual percent change (APC) may be different from the ones returned by NCI’s Joinpoint Software; for further details, check out Muggeo V. (2010) A Comment on `Estimating average annual per cent change in trend analysis’ by Clegg et al., Statistics in Medicine; 28, 3670-3682. Statistics in Medicine, 29, 1958-1960. This analysis returned similar results to the NCI’s Joinpoint Regression Program by estimating a changepoint at year=1999 - and, more precisely, that the start of that decreasing trend in smoking prevalence happened at an APC of -3.92 percent. That is, slope2 from the output above. Make statistically defensible statements about linear trends with complex survey data After identifying the change point for smoking prevalence, we can create two regression models (one for each time segment). (If we had two joinpoints, we would need three regression models.) The first model covers the years leading up to (and including) the changepoint (i.e., 1991 to 1999). The second model includes the years from the changepoint forward (i.e., 1999 to 2011). So start with 1991, 1993, 1995, 1997, 1999, the five year-points before (and including 1999). # calculate a five-timepoint linear contrast vector c3l &lt;- contr.poly( 3 )[ , 1 ] # tack the five-timepoint linear contrast vectors onto the current survey design object des_ns &lt;- update( des_ns , t3l = c3l[ match( year , seq( 2005 , 2009 , 2 ) ) ] ) pre_91_99 &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t3l , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 &amp; year &lt;= 2009 ) , family = quasibinomial ) summary( pre_91_99 ) Reproduce the sentence on pdf page 6 of the original document. In this example, T5L_L had a p-value=0.52261 and beta=0.03704. Therefore, there was “no significant change in the prevalence of ever smoking a cigarette during 1991-1999.” Then move on to 1999, 2001, 2003, 2005, 2007, 2009, and 2011, the seven year-points after (and including 1999). # calculate a seven-timepoint linear contrast vector c4l &lt;- contr.poly( 4 )[ , 1 ] # tack the seven-timepoint linear contrast vectors onto the current survey design object des_ns &lt;- update( des_ns , t4l = c4l[ match( year , seq( 2009 , 2015 , 2 ) ) ] ) post_99_11 &lt;- svyglm( I( rode_with_drunk_driver == 1 ) ~ sex + raceeth + grade + t4l , design = subset( des_ns , rode_with_drunk_driver %in% 1:2 &amp; year &gt;= 2009 ) , family = quasibinomial ) summary( post_99_11 ) Reproduce the sentence on pdf page 6 of the original document. In this example, T7L_R had a p-value&lt;0.0001 and beta=-0.99165. Therefore, there was a “significant linear decrease in the prevalence of ever smoking a cigarette during 1999-2011.” Note also that the 1999-2011 time period saw a linear decrease, which supports the APC estimate in step #8. Here’s everything displayed as a single coherent table. htmlreg(list(pre_91_99, post_99_11), doctype = F, html.tag = F, inline.css = T, head.tag = F, body.tag = F, center = F, single.row = T, caption = &quot;Table 2. Linear trends pre-post changepoint&quot;) This analysis may complement qualitative evaluation on prevalence changes observed from surveillance data by providing quantitative evidence, such as when a change point occurred. This analysis does not explain why or how changes in trends occur. "],["structural-equation-models-sem-with-complex-survey-data.html", "Structural Equation Models (SEM) with Complex Survey Data Load the 2008 Wave of the European Social Survey German and Spanish Microdata Two-factor CFA of attitudes toward the welfare state Invariance testing on Schwarz human values while accounting for the survey design. An example with a Latent Variable Regression", " Structural Equation Models (SEM) with Complex Survey Data Contributed by Dr. Daniel Oberski &lt;daniel.oberski@gmail.com&gt; The R lavaan.survey package by Dr. Daniel Oberski fits structural equation models to complex survey microdata, described in his JSS article. install.packages( &quot;lavaan.survey&quot; , repos = &quot;http://cran.rstudio.com/&quot; ) Load the 2008 Wave of the European Social Survey German and Spanish Microdata library(lodown) library(lavaan.survey) options( survey.lonely.psu = &quot;adjust&quot; ) # retrieve a listing of all available extracts for the european social survey ess_cat &lt;- get_catalog( &quot;ess&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; ) ) # limit the catalog to only wave #4 for germany and spain ess_cat &lt;- subset( ess_cat , wave == 4 &amp; grepl( &quot;c=DE|c=ES&quot; , full_url ) ) # download the ess microdata lodown( &quot;ess&quot; , ess_cat , your_email = &quot;email@address.com&quot; ) Immediately pull the German files into the workspace: # load Germany&#39;s round four main data file.. ess4.de &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4DE.rds&quot; ) ) # load Germany&#39;s round four sample design data file (sddf).. ess4.de.sddf &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4_DE_SDDF.rds&quot; ) ) The stratify variable is not literally equal to the actual strata but contains more information (which we don’t need here). Create a new variable that only uses the actual stratification namely, East v. West Germany, by using a regular expression / string substitution function to take the data.frame object’s stratify variable, convert it to a character variable, search for a dash, and keep only the text before the dash. Then, convert that resultant vector of ones and twos into a factor variable, labeled East versus West Germany. ess4.de.sddf$stratify &lt;- factor( gsub( &quot;(\\\\d+)-.+&quot; , &quot;\\\\1&quot; , as.character( ess4.de.sddf$stratify ) ) ) levels(ess4.de.sddf$stratify) &lt;- c(&quot;West Germany&quot;, &quot;East Germany&quot;) Check against ESS documentation statement that “The number of sampling points is 109 in the West, and 59 in the East”: stopifnot(tapply(ess4.de.sddf$psu, ess4.de.sddf$stratify, function(x) length(unique(x))) == c(109, 59)) Merge these two files together, creating a single table: ess4.de.m &lt;- merge( ess4.de , ess4.de.sddf) stopifnot( nrow( ess4.de ) == nrow( ess4.de.m ) &amp; nrow( ess4.de.sddf ) == nrow( ess4.de.m ) ) Create a survey design object: ess4.de.design &lt;- svydesign( ids = ~psu , strata = ~stratify , probs = ~prob , data = ess4.de.m ) Two-factor CFA of attitudes toward the welfare state This analysis uses the model of the below article. Please see the article for more information. Roosma, F., Gelissen, J., &amp; van Oorschot, W. (2013). The multidimensionality of welfare state attitudes: a European cross-national study. Social indicators research, 113(1), 235-255. Formulate the two-factor CFA using lavaan syntax: model.cfa &lt;- &quot;range =~ gvjbevn + gvhlthc + gvslvol + gvslvue + gvcldcr + gvpdlwk goals =~ sbprvpv + sbeqsoc + sbcwkfm&quot; Fit the model using lavaan, accounting for possible nonnormality using the MLM estimator: fit.cfa.ml &lt;- lavaan( model.cfa , data = ess4.de.m , estimator = &quot;MLM&quot; , int.ov.free = TRUE , auto.var = TRUE , auto.fix.first = TRUE , auto.cov.lv.x = TRUE ) Show some fit measure results, note the “scaling correction” which accounts for nonnormality: fit.cfa.ml Fit the two-factor model while taking the survey design into account: fit.cfa.surv &lt;- lavaan.survey( fit.cfa.ml , survey.design = ess4.de.design ) Show some fit measure results, “scaling correction” now accounts for both nonnormality and survey design. fit.cfa.surv Display parameter estimates and standard errors accounting for survey design: summary( fit.cfa.surv , standardized = TRUE ) Invariance testing on Schwarz human values while accounting for the survey design. For more information on this analysis, see: Davidov, E., Schmidt, P., &amp; Schwartz, S. H. (2008). “Bringing values back in: The adequacy of the European Social Survey to measure values in 20 countries”. Public opinion quarterly, 72(3), 420-445. Test the measurement equivalence of Schwarz human values from round 4 of the ESS, comparing Germany with Spain. First load the Spanish data so these can be merged: # load Spain&#39;s round four main data file.. ess4.es &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4ES.rds&quot; ) ) # load Spain&#39;s round four sample design data file (sddf).. ess4.es.sddf &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2008/ESS4_ES_SDDF.rds&quot; ) ) Merge these two files together, creating a single table: ess4.es.m &lt;- merge( ess4.es , ess4.es.sddf) stopifnot( nrow( ess4.es ) == nrow( ess4.es.m ) &amp; nrow( ess4.es.sddf ) == nrow( ess4.es.m ) ) Make sure PSU names are unique between the two countries. Paste on a “de-” to the German PSUs, and by pasting an “es-” to the front of the Spanish PSUs. ess4.de.m$psu &lt;- paste( &quot;de&quot; , ess4.de.m$psu , sep=&quot;-&quot; ) ess4.es.m$psu &lt;- paste( &quot;es&quot; , ess4.es.m$psu , sep=&quot;-&quot; ) Stack the two countries into a single table, then construct a survey design: ess4.m &lt;- rbind( ess4.de.m , ess4.es.m ) ess4.design &lt;- svydesign( ids = ~psu, strata = ~stratify , probs = ~prob , data = ess4.m ) Model based on Schwarz human value theory. Note that this is the basic starting model, not the final model used by Davidov et al. They merge certain values and allow cross-loadings: free.values.model.syntax &lt;- &quot; Universalism =~ ipeqopt + ipudrst + impenv Benevolence =~ iphlppl + iplylfr Tradition =~ ipmodst + imptrad Conformity =~ ipfrule + ipbhprp Security =~ impsafe + ipstrgv &quot; Fit two-group configural invariance model: free.values.fit &lt;- lavaan( free.values.model.syntax , data = ess4.m , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE , estimator = &quot;MLM&quot; , group = &quot;cntry&quot; ) summary( free.values.fit , standardized = TRUE ) Fit a two-group metric invariance model: free.values.fit.eq &lt;- lavaan( free.values.model.syntax , data = ess4.m , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE , estimator = &quot;MLM&quot; , group = &quot;cntry&quot; , group.equal = &quot;loadings&quot; ) summary( free.values.fit.eq , standardized = TRUE ) Metric invariance test (anova() would work here too, but not below): lavTestLRT( free.values.fit , free.values.fit.eq , SB.classic = TRUE ) Compare chisquares of the survey and non-survey SEM analyses for the configural invariance model: free.values.fit.surv &lt;- lavaan.survey( free.values.fit , ess4.design ) free.values.fit free.values.fit.surv Compare chisquares of the survey and non-survey SEM analyses for the metric invariance model: free.values.fit.eq.surv &lt;- lavaan.survey( free.values.fit.eq , ess4.design ) free.values.fit.eq free.values.fit.eq.surv Perform metric invariance test accounting for the survey design: lavTestLRT(free.values.fit.surv, free.values.fit.eq.surv, SB.classic = TRUE) The two models are more dissimilar after survey design is accounted for. An example with a Latent Variable Regression See Davidov, E., Meuleman, B., Billiet, J., &amp; Schmidt, P. (2008). Values and support for immigration: A cross-country comparison. European Sociological Review, 24(5), 583-599. The human values scale again, but this time: only two value dimensions are modeled. the two latent value dimensions are used to predict anti-immigration attitudes in the two countries. a test is performed on the difference between countries in latent regression coefficients. reg.syntax &lt;- &quot; SelfTranscendence =~ ipeqopt + ipudrst + impenv + iphlppl + iplylfr Conservation =~ ipmodst + imptrad + ipfrule + ipbhprp + impsafe + ipstrgv ALLOW =~ imdfetn + impcntr ALLOW ~ SelfTranscendence + Conservation &quot; reg.vals.fit &lt;- lavaan( reg.syntax , data = ess4.m , group = &quot;cntry&quot; , estimator = &quot;MLM&quot; , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE ) reg.vals.fit.eq &lt;- lavaan( reg.syntax , data = ess4.m , group = &quot;cntry&quot; , group.equal = &quot;regressions&quot; , estimator = &quot;MLM&quot; , auto.cov.lv.x = TRUE , auto.fix.first = TRUE , auto.var = TRUE , int.ov.free = TRUE ) summary( reg.vals.fit.eq , standardize = TRUE ) Test whether the relationship between values and anti-immigration attitudes is equal in Germany and Spain: lavTestLRT( reg.vals.fit , reg.vals.fit.eq , SB.classic = TRUE) Now do the same but accounting for the sampling design: reg.vals.fit.surv &lt;- lavaan.survey( reg.vals.fit , ess4.design ) reg.vals.fit.eq.surv &lt;- lavaan.survey( reg.vals.fit.eq , ess4.design ) lavTestLRT(reg.vals.fit.surv, reg.vals.fit.eq.surv, SB.classic = TRUE) The two models are less dissimilar after survey design is accounted for. "],["balancing-respondent-confidentiality-with-variance-estimation-precision.html", "Balancing Respondent Confidentiality with Variance Estimation Precision", " Balancing Respondent Confidentiality with Variance Estimation Precision # analyze survey data for free (http://asdfree.com) with the r language # how to create de-identified replicate weights # in less than ten steps # anthony joseph damico # ajdamico@gmail.com # the institute for digital research and education at ucla # hosts a very readable explanation of replicate weights, # how they protect confidentiality, and why they&#39;re generally awesome # http://www.ats.ucla.edu/stat/stata/library/replicate_weights.htm # remove the # in order to run this install.packages line only once # install.packages( c( &quot;survey&quot; , &quot;sdcMicro&quot; ) ) # load the r survey package library(survey) # load the sdcMicro package library(sdcMicro) # load some sample complex sample survey data data(api) # look at the first six records of the `apistrat` data.frame object # so you have a sense of what you&#39;re working with in this example # this particular example is student performance in california schools # http://r-survey.r-forge.r-project.org/survey/html/api.html head( apistrat ) ########################################### # # # # # # # # # # # # # # # # # # # # # # # part one. create the replicate weights # # # # # # # # # # # # # # # # # # # # # # # ########################################### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # step one: construct the taylor-series linearized design # # that you use on your internal, confidential microdata # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # there are many permutations of linearized designs. # you can find well-documented examples of tsl design setups # by searching for the text `svydesign` here: # https://github.com/ajdamico/asdfree/search?q=svydesign&amp;ref=cmdform # does your tsl design have a strata argument? # # this design does not have a `strata=` parameter api.tsl.without.strata &lt;- svydesign( id = ~dnum , data = apistrat , weights = ~pw ) # this design does have a `strata=` parameter api.tsl.with.strata &lt;- svydesign( id = ~dnum , strata = ~stype , data = apistrat , weights = ~pw , nest = TRUE ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # step two: convert this taylor-series linearized design # # to one of a few choices of replication-based designs # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # for unstratified designs, # use a &quot;jackknife delete one&quot; method api.jk1 &lt;- as.svrepdesign( api.tsl.without.strata , type = &quot;JK1&quot; , fay.rho = 0.5 , mse = TRUE , compress = FALSE # note: compressed replicate weights require less RAM but complicate the weight-extraction in step three ) # for stratified designs, # you might first attempt to use a # balanced repeated replication method # with a fay&#39;s adjustment # (this is the most common setup from the united states census bureau) try( { api.fay &lt;- as.svrepdesign( api.tsl.with.strata , type = &quot;Fay&quot; , fay.rho = 0.5 , mse = TRUE , compress = FALSE # note: compressed replicate weights require less RAM but complicate the weight-extraction in step three ) } , silent = TRUE ) # however, if the sampling plan contains an # odd number of clusters within any stratum # you will hit this error # Error in brrweights(design$strata[, 1], design$cluster[, 1], ..., fay.rho = fay.rho, : # Can&#39;t split with odd numbers of PSUs in a stratum # for stratified designs with an # odd number of clusters in any stratum, # use a &quot;jackknife delete n&quot; method api.jkn &lt;- as.svrepdesign( api.tsl.with.strata , type = &quot;JKn&quot; , mse = TRUE , compress = FALSE # note: compressed replicate weights require less RAM but complicate the weight-extraction in step three ) # now you have a matrix of replicate weights # stored in your replication-based survey-design # the purpose of this exercise is to produce comparable # variance estimates without compromising confidentiality # therefore, run a few standard error calculations # using the original linearized designs, # compared to the newly-created jackknife design # calculate the mean 1999 and 2000 academic performance index scores # using the original stratified taylor-series design.. svymean( ~ api99 + api00 , api.tsl.with.strata ) # ..and the newly-created jackknife replication-based design svymean( ~ api99 + api00 , api.jkn ) # the standard errors for these estimates are nearly identical # # run the same commands as above, broken down by award program eligibility svyby( ~ api99 + api00 , ~ awards , api.tsl.with.strata , svymean ) svyby( ~ api99 + api00 , ~ awards , api.jkn , svymean ) # in each case, the replication-based design # (that we created off of the linearized design) # produced a comparable variance estimate # # # # # # # # # # # # # # # # # # # # # # # # # # step three: extract the replication weights # # from your newly created survey design object # # # # # # # # # # # # # # # # # # # # # # # # # # # note that this example shows how to extract # the weights from the `api.jkn` survey object # however, this method would be identical # for the `api.jk1` and `api.fay` objects # displayed above as well # despite `api.fay` throwing an error, # because of an uneven number of clusters within strata # look at your survey design api.jkn # look at the contents of your replication-based survey design names( api.jkn ) # look at the first six replicate weight records within your survey design head( api.jkn$repweights ) # note that these weights are not *combined* by default # in other words, they still need to be multiplied by the original weight # you can confirm this by looking at the flag that indicates: # &quot;have replicate weights been combined?&quot; api.jkn$combined.weights # no. they have not been combined # therefore, these replication weights are `uncombined` # and will need to be analyzed by the user as such your.replicate.weights &lt;- data.frame( unclass( api.jkn$repweights ) ) # # # # # # # # # # # # # # # # # # # # # # # # # you&#39;ve created the set of replicate-weights # ############################################# # # # # # # # # # # # # # # # # # # # # # # # # part two. mask the strata from evil users # # # # # # # # # # # # # # # # # # # # # # # # ############################################# # now prevent users from reidentifying strata # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # step four: understand how a malicious user might easily # # identify clustering variables on un-obfuscated data # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # start with the replicate weights object from the script transposed.rw &lt;- data.frame( t( your.replicate.weights ) ) # the first record of apistrat has dnum==401 and stype==&#39;E&#39; # going back to the original microdata set, # eleven records are in this cluster x stratum which( apistrat$dnum == 401 &amp; apistrat$stype == &#39;E&#39; ) # without massaging your replicate weights at all, # a malicious user could easily view the correlations # between each record&#39;s replicate weights # in order to determine what other records # land in the same cluster x strata # for example, here are all replicate-weight records that # perfectly (after rounding) correlate with # the first record. which( round( cor( transposed.rw )[ 1 , ] ) == 1 ) # same numbers! # (wikipedia has a great definition: http://en.wikipedia.org/wiki/Obfuscation) # if you do not obfuscate your data, a malicious user could # identify unique clusters and strata even if only given replicate weights # # # # # # # # # # # # # # # # # step five: BRING THE NOISE # # # # # # # # # # # # # # # # # # set a random seed. this allows you to # go back to this code and reproduce the # exact same results in the future. # the following steps include a random process # setting a seed enforces the same random process every time sum( utf8ToInt( &quot;anthony is cool&quot; ) ) # my current favorite number is 1482, so let&#39;s go with that. set.seed( 1482 ) # figure out how much noise to add. noisy.transposed.rw &lt;- addNoise( transposed.rw , noise = 1 )$xm # remember, on the original replicate weight objects, # the correlations between the first record and # other records within the same clusters and strata # were perfect. which( sapply( cor( transposed.rw )[ 1 , ] , function( z ) isTRUE( all.equal( z , 1 ) ) ) ) # run the same test on noisified weights which( sapply( cor( noisy.transposed.rw )[ 1 , ] , function( z ) isTRUE( all.equal( z , 1 ) ) ) ) # and suddenly none of the other records are perfectly correlated # but even moderately correlations might allow evildoers # to identify clusters and strata # these records have a 0.1 or higher correlation coefficient which( cor( noisy.transposed.rw )[ 1 , ] &gt; 0.1 ) # these records have a 0.2 or higher correlation coefficient which( cor( noisy.transposed.rw )[ 1 , ] &gt; 0.2 ) # whoops. we did not add enough noise. # records in the same cluster x strata still have # too high of a correlation coefficient, # relative to other records # okay. this will make a big difference # on the size of the standard errors that # your users will actually see. # make a clutch decision: # how much noise can you tolerate? # hmncyt &lt;- 1 hmncyt &lt;- 3 # hmncyt &lt;- 10 # you need to run this script a few times # because the size of your standard errors # are going to change, depending on your data set. # essentially, choose the lowest noise value # that does not lead you to uncomfortable levels # of correlation within cluster/strata # in your replicate weights columns # crank up the random noise percentage to three noisy.transposed.rw &lt;- addNoise( transposed.rw , noise = hmncyt )$xm # and suddenly.. # records 29, 73 and 158 have a correlation coefficient # that&#39;s greater than zero point two. which( cor( noisy.transposed.rw )[ 1 , ] &gt; 0.2 ) # and of those, only record #29 is in the same cluster x strata intersect( which( cor( noisy.transposed.rw )[ 1 , ] &gt; 0.2 ) , which( round( cor( transposed.rw )[ 1 , ] ) == 1 ) ) # that&#39;s awesome, because you have two false-positives here. # the &quot;73&quot; and &quot;158&quot; will throw off a malicious user. # these records have a 0.1 or higher correlation coefficient which( cor( noisy.transposed.rw )[ 1 , ] &gt; 0.1 ) # and that looks very good. # because less than half of those records # are actually in the same cluster x strata intersect( which( cor( noisy.transposed.rw )[ 1 , ] &gt; 0.1 ) , which( round( cor( transposed.rw )[ 1 , ] ) == 1 ) ) # and there are lots of other records with high correlations (false positives) # that in fact are not in the same strata. great. perf. magnifique! # this object `noisy.transposed.rw` is the set of replicate weights # that you might now feel comfortable disclosing to your users. # bee tee dubs # you should un-transpose the weights rightaboutnow noisy.rw &lt;- t( noisy.transposed.rw ) # # # # # # # # # # # # # # # # # # # # # # step six: check with your legal dept. # # # # # # # # # # # # # # # # # # # # # # # brilliant malicious users might still be able to identify certain records # if they work really really really hard and have access to other information # included in your survey microdata set. # for example: # if your technical documentation says that memphis was one of your sampled clusters, and # if your microdata does have a state identifier, and # if the content of your survey was about barbeque consumption # it&#39;s possible that no amount of masking, obfuscating, massaging, noising, whathaveyouing # will prevent malicious users from determining the geography of some of the records # included in your public use file. so don&#39;t mindlessly follow this example. # consider exactly what you&#39;re disclosing, consider how someone might use it improperly. # # # # # # # # # # # # # # # # # # # # # # # # you&#39;ve protected cluster confidentiality # ############################################# # # # # # # # # # # # # # # # # # # # # # # # # part three. share these replicate weights # # # # # # # # # # # # # # # # # # # # # # # # ############################################# # now share these weights with your users. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # step seven: remove the confidential fields from your data # # and tack on the replicate weight columns created above. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # go back to our original data.frame object `apistrat` # and store that entire table into `x` x &lt;- apistrat # remove the columns that were deemed too confidential to release x$dnum &lt;- x$stype &lt;- NULL # in other words, delete the cluster and strata variables # from this data.frame object so you have a &quot;safe-for-the-public&quot; data set # look at the first six records # to confirm they have been removed head( x ) # merge on the replicate weight data.frame # that we just created, which contains # obfuscated-confidential information for users y &lt;- cbind( x , noisy.rw ) # look at the first six records # to confirm the weights have been tacked on head( y ) # the replicate weights are now stored # as `X1` through `X162` # this data.frame object `y` # contains all of the information that # a user needs to correctly calculate a # variance, standard error, confidence interval # uncomment this line to # export `y` to a csv file # write.csv( y , &quot;C:/My Directory/your microdata.csv&quot; ) # this csv file contains your full microdata set, # except for the cluster and strata variables # that you had determined to be confidential information # in other words, now you&#39;ve got a public-use file (puf) # that no longer contains identifiable geographic information # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # step eight: determine the `svrepdesign` specification to # # match the survey object above, but without cluster info # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # once users have a copy of this de-identified # microdata file, they will need the # replication-based complex sample design # there are many permutations of replication-based designs. # you can find well-documented examples of replicate weighted design setups # by searching for the text `svrepdesign` here: # https://github.com/ajdamico/asdfree/search?q=svrepdesign&amp;type=Code # for the object `y` built above, # construct the replication-based # &quot;jackknife delete n&quot; method # complex sample survey design object z &lt;- svrepdesign( data = y , type = &quot;JKn&quot; , repweights = &quot;X[1-9]+&quot; , weights = ~pw , scale = 1 , combined.weights = FALSE , mse = TRUE ) # is it giving you a warning about calculating the rscales by itself? # good. then you&#39;re doing right by me. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # step nine: confirm this new replication survey object # # matches the standard errors derived from as.svrepdesign # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # this `z` object can now be analyzed # and the statistics but not the standard errors # will match the `api.jkn` shown above. svymean( ~ api99 + api00 , z ) svymean( ~ api99 + api00 , api.jkn ) svyby( ~ api99 + api00 , ~ awards , z , svymean ) svyby( ~ api99 + api00 , ~ awards , api.jkn , svymean ) # see how the standard errors have a-little-more-than-doubled? # that&#39;s because we had to BRING THE NOISE to the replicate weights # if you re-run this script but lower the value of `hmncyt` # your standard errors will *decrease* and get closer to # what they actually are when you have the confidential information. # that&#39;s the trade-off. re-run this entire script but set `hmncyt &lt;- 0` # and you&#39;ll see some almost-perfect standard errors.. # but doing that would allow malicious users to identify clusters easily # then re-run it again and set `hmncyt &lt;- 10` and suddenly # no way in hell can a malicious user identify clustering information # but your standard errors (and subsequent confidence intervals) are ginormous # obfuscating your replicate weights is going to make it harder for users # to detect statistically significant differences when analyzing your microdata. # no way around that. # just do your best to minimize the amount of obfuscation. # dooooo it. public use microdata are an indisputable good. "],["american-community-survey-acs.html", "American Community Survey (ACS) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " American Community Survey (ACS) The US Census Bureau’s annual replacement for the long-form decennial census. One table with one row per household and a second table with one row per individual within each household. The civilian population of the United States. Released annually since 2005. Administered and financed by the US Census Bureau. Simplified Download and Importation The R lodown package easily downloads and imports all available ACS microdata by simply specifying \"acs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;acs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ACS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the ACS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available ACS microdata files acs_cat &lt;- get_catalog( &quot;acs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ACS&quot; ) ) # 2016 alabama single-year only. remove ` &amp; stateab == &#39;al&#39;` for a nationwide table acs_cat &lt;- subset( acs_cat , year == 2016 &amp; time_period == &#39;1-Year&#39; &amp; stateab == &#39;al&#39; ) # download the microdata to your local computer acs_cat &lt;- lodown( &quot;acs&quot; , acs_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: # # alternative subsets: # # nationwide merged table including puerto rico # acs_cat &lt;- subset( acs_cat , year == 2016 &amp; time_period == &#39;1-Year&#39; ) # acs_cat &lt;- lodown( &quot;acs&quot; , acs_cat ) # # nationwide merged table excluding puerto rico # acs_cat &lt;- subset( acs_cat , year == 2016 &amp; time_period == &#39;1-Year&#39; &amp; stateab != &#39;pr&#39; ) # acs_cat &lt;- lodown( &quot;acs&quot; , acs_cat ) library(survey) acs_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ACS&quot; , &quot;acs2016_1yr.rds&quot; ) ) # because of the catalog subset above # the `merged.rds` file is alabama only acs_design &lt;- svrepdesign( weight = ~pwgtp , repweights = &#39;pwgtp[0-9]+&#39; , scale = 4 / 80 , rscales = rep( 1 , 80 ) , mse = TRUE , type = &#39;JK1&#39; , data = acs_df ) Variable Recoding Add new columns to the data set: acs_design &lt;- update( acs_design , relp = as.numeric( relp ) , state_name = factor( as.numeric( st ) , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L, 72L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;Puerto Rico&quot;) ) , cit = factor( cit , levels = 1:5 , labels = c( &#39;born in the u.s.&#39; , &#39;born in the territories&#39; , &#39;born abroad to american parents&#39; , &#39;naturalized citizen&#39; , &#39;non-citizen&#39; ) ) , poverty_level = as.numeric( povpip ) , married = as.numeric( mar %in% 1 ) , sex = factor( sex , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( acs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ cit , acs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , acs_design ) svyby( ~ one , ~ cit , acs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ poverty_level , acs_design , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , acs_design ) svyby( ~ sex , ~ cit , acs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ poverty_level , acs_design , 0.5 , na.rm = TRUE ) svyby( ~ poverty_level , ~ cit , acs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ssip , denominator = ~ pincp , acs_design , na.rm = TRUE ) Subsetting Restrict the survey design to senior citizens: sub_acs_design &lt;- subset( acs_design , agep &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ poverty_level , sub_acs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ poverty_level , acs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ poverty_level , ~ cit , acs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( acs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ poverty_level , acs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ poverty_level , acs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ married , acs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( poverty_level ~ married , acs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ married + sex , acs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( poverty_level ~ married + sex , acs_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for ACS users, this code calculates the gini coefficient on complex sample survey data: library(convey) acs_design &lt;- convey_prep( acs_design ) svygini( ~ hincp , acs_design , na.rm = TRUE ) Replication Example The example below matches statistics, standard errors, and margin of errors from this table pulled from the tallies of 2016 PUMS: Match the sum of the weights: stopifnot( round( coef( svytotal( ~ one , acs_design ) ) , 0 ) == 4863300 ) Compute the population by age: pums_estimate &lt;- c( 285681 , 314701 , 300814 , 334318 , 327896 , 629329 , 599719 , 644212 , 342205 , 300254 , 464893 , 231293 , 87985 ) pums_standard_error &lt;- c( 2888 , 5168 , 5009 , 3673 , 3521 , 4825 , 4088 , 4398 , 5329 , 5389 , 1938 , 3214 , 2950 ) pums_margin_of_error &lt;- c( 4751 , 8501 , 8240 , 6043 , 5792 , 7937 , 6725 , 7234 , 8767 , 8865 , 3188 , 5287 , 4853 ) results &lt;- svytotal( ~ as.numeric( agep %in% 0:4 ) + as.numeric( agep %in% 5:9 ) + as.numeric( agep %in% 10:14 ) + as.numeric( agep %in% 15:19 ) + as.numeric( agep %in% 20:24 ) + as.numeric( agep %in% 25:34 ) + as.numeric( agep %in% 35:44 ) + as.numeric( agep %in% 45:54 ) + as.numeric( agep %in% 55:59 ) + as.numeric( agep %in% 60:64 ) + as.numeric( agep %in% 65:74 ) + as.numeric( agep %in% 75:84 ) + as.numeric( agep %in% 85:100 ) , acs_design ) stopifnot( all( round( coef( results ) , 0 ) == pums_estimate ) ) stopifnot( all( round( SE( results ) , 0 ) == pums_standard_error ) ) stopifnot( all( round( SE( results ) * 1.645 , 0 ) == pums_margin_of_error ) ) "],["national-longitudinal-study-of-adolescent-to-adult-health-addhealth.html", "National Longitudinal Study of Adolescent to Adult Health (ADDHEALTH) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " National Longitudinal Study of Adolescent to Adult Health (ADDHEALTH) The National Longitudinal Study of Adolescent to Adult Health follows a cohort of teenagers from the 1990s into adulthood. Many tables, most with one row per sampled youth respondent. A complex sample survey designed to generalize to adolescents in grades 7-12 in the United States during the 1994-95 school year. Released at irregular intervals, with 1994-1995, 1996, 2001-2002, and 2008-2009 available and 2016-2018 forthcoming. Administered by the Carolina Population Center and funded by a consortium. Simplified Download and Importation The R lodown package easily downloads and imports all available ADDHEALTH microdata by simply specifying \"addhealth\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;addhealth&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ADDHEALTH&quot; ) , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the ADDHEALTH catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available ADDHEALTH microdata files addhealth_cat &lt;- get_catalog( &quot;addhealth&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ADDHEALTH&quot; ) , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; ) # wave i only addhealth_cat &lt;- subset( addhealth_cat , wave == &quot;wave i&quot; ) # download the microdata to your local computer addhealth_cat &lt;- lodown( &quot;addhealth&quot; , addhealth_cat , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) addhealth_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ADDHEALTH&quot; , &quot;wave i consolidated.rds&quot; ) ) addhealth_design &lt;- svydesign( id = ~cluster2 , data = addhealth_df , weights = ~ gswgt1 , nest = TRUE ) Variable Recoding Add new columns to the data set: addhealth_design &lt;- update( addhealth_design , one = 1 , male = as.numeric( as.numeric( bio_sex ) == 1 ) , hours_of_computer_games = ifelse( h1da10 &gt; 99 , NA , h1da10 ) , hours_of_television = ifelse( h1da8 &gt; 99 , NA , h1da8 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( addhealth_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ h1gh25 , addhealth_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , addhealth_design ) svyby( ~ one , ~ h1gh25 , addhealth_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ hours_of_computer_games , addhealth_design , na.rm = TRUE ) svyby( ~ hours_of_computer_games , ~ h1gh25 , addhealth_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ h1gh24 , addhealth_design , na.rm = TRUE ) svyby( ~ h1gh24 , ~ h1gh25 , addhealth_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ hours_of_computer_games , addhealth_design , na.rm = TRUE ) svyby( ~ hours_of_computer_games , ~ h1gh25 , addhealth_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ h1gh24 , addhealth_design , na.rm = TRUE ) svyby( ~ h1gh24 , ~ h1gh25 , addhealth_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ hours_of_computer_games , addhealth_design , 0.5 , na.rm = TRUE ) svyby( ~ hours_of_computer_games , ~ h1gh25 , addhealth_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ hours_of_computer_games , denominator = ~ hours_of_television , addhealth_design , na.rm = TRUE ) Subsetting Restrict the survey design to self-reported fair or poor health: sub_addhealth_design &lt;- subset( addhealth_design , as.numeric( h1gh1 ) %in% c( 4 , 5 ) ) Calculate the mean (average) of this subset: svymean( ~ hours_of_computer_games , sub_addhealth_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ hours_of_computer_games , addhealth_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ hours_of_computer_games , ~ h1gh25 , addhealth_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( addhealth_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ hours_of_computer_games , addhealth_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ hours_of_computer_games , addhealth_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ hours_of_computer_games , addhealth_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , addhealth_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( hours_of_computer_games ~ male , addhealth_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + h1gh24 , addhealth_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( hours_of_computer_games ~ male + h1gh24 , addhealth_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ADDHEALTH users, this code replicates previously-presented examples: library(srvyr) addhealth_srvyr_design &lt;- as_survey( addhealth_design ) Calculate the mean (average) of a linear variable, overall and by groups: addhealth_srvyr_design %&gt;% summarize( mean = survey_mean( hours_of_computer_games , na.rm = TRUE ) ) addhealth_srvyr_design %&gt;% group_by( h1gh25 ) %&gt;% summarize( mean = survey_mean( hours_of_computer_games , na.rm = TRUE ) ) Replication Example A member of the AddHealth staff at UNC ran the mean hours of television using the wave one public use file with stata to confirm that the calculations presented on this page follow the correct methodology. svyset cluster2 [pweight=gswgt1] svy: mean w1hrtv Survey: Mean estimation Number of strata = 1 Number of obs = 6477 Number of PSUs = 132 Population size = 22159516 Design df = 131 -------------------------------------------------------------- | Linearized | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ w1hrtv | 15.64193 .3902066 14.87 16.41385 -------------------------------------------------------------- result &lt;- svymean( ~ hours_of_television , addhealth_design , na.rm = TRUE ) stopifnot( round( coef( result ) , 3 ) == 15.642 ) stopifnot( round( SE( result ) , 4 ) == 0.3902 ) "],["area-health-resource-file-ahrf.html", "Area Health Resource File (AHRF) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " Area Health Resource File (AHRF) Though not a survey data set itself, useful to merge onto other microdata. One table with one row per county and a second table with one row per state. Replaced annually with the latest available county- and state-level statistics. Compiled by the United States Health Services and Resources Administration (HRSA). Simplified Download and Importation The R lodown package easily downloads and imports all available AHRF microdata by simply specifying \"ahrf\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;ahrf&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;AHRF&quot; ) ) Analysis Examples with base R   Load a data frame: ahrf_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;AHRF&quot; , &quot;county/AHRF_2016-2017.rds&quot; ) ) Variable Recoding Add new columns to the data set: ahrf_df &lt;- transform( ahrf_df , cbsa_indicator_code = factor( 1 + as.numeric( f1406715 ) , labels = c( &quot;not metro&quot; , &quot;metro&quot; , &quot;micro&quot; ) ) , mhi_2014 = f1322614 , whole_county_hpsa_2016 = as.numeric( f0978716 == 1 ) , census_region = factor( as.numeric( f04439 ) , labels = c( &quot;northeast&quot; , &quot;midwest&quot; , &quot;south&quot; , &quot;west&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( ahrf_df ) table( ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( ahrf_df[ , &quot;mhi_2014&quot; ] , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2014&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( ahrf_df[ , &quot;census_region&quot; ] ) ) prop.table( table( ahrf_df[ , c( &quot;census_region&quot; , &quot;cbsa_indicator_code&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( ahrf_df[ , &quot;mhi_2014&quot; ] , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2014&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( ahrf_df[ , &quot;mhi_2014&quot; ] , 0.5 , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2014&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to California: sub_ahrf_df &lt;- subset( ahrf_df , f12424 == &quot;CA&quot; ) Calculate the mean (average) of this subset: mean( sub_ahrf_df[ , &quot;mhi_2014&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( ahrf_df[ , &quot;mhi_2014&quot; ] , na.rm = TRUE ) tapply( ahrf_df[ , &quot;mhi_2014&quot; ] , ahrf_df[ , &quot;cbsa_indicator_code&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( mhi_2014 ~ whole_county_hpsa_2016 , ahrf_df ) Perform a chi-squared test of association: this_table &lt;- table( ahrf_df[ , c( &quot;whole_county_hpsa_2016&quot; , &quot;census_region&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( mhi_2014 ~ whole_county_hpsa_2016 + census_region , data = ahrf_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for AHRF users, this code replicates previously-presented examples: library(dplyr) ahrf_tbl &lt;- tbl_df( ahrf_df ) Calculate the mean (average) of a linear variable, overall and by groups: ahrf_tbl %&gt;% summarize( mean = mean( mhi_2014 , na.rm = TRUE ) ) ahrf_tbl %&gt;% group_by( cbsa_indicator_code ) %&gt;% summarize( mean = mean( mhi_2014 , na.rm = TRUE ) ) "],["american-housing-survey-ahs.html", "American Housing Survey (AHS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " American Housing Survey (AHS) The American Housing Survey tracks housing structures across the United States. A collection of tables, most with one row per housing unit. A complex sample survey designed to generalize to both occupied and vacant housing units across the United States and also for about twenty-five metropolitan areas. Released more or less biennially since 1973. Sponsored by the Department of Housing and Urban Development (HUD) and conducted by the U.S. Census Bureau. Simplified Download and Importation The R lodown package easily downloads and imports all available AHS microdata by simply specifying \"ahs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;ahs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;AHS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the AHS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available AHS microdata files ahs_cat &lt;- get_catalog( &quot;ahs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;AHS&quot; ) ) # 2013 only ahs_cat &lt;- subset( ahs_cat , year == 2013 ) # download the microdata to your local computer ahs_cat &lt;- lodown( &quot;ahs&quot; , ahs_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) ahs_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;AHS&quot; , &quot;2013/national_v1.2/newhouse_repwgt.rds&quot; ) ) ahs_design &lt;- svrepdesign( weights = ~ wgt90geo , repweights = &quot;repwgt[1-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , mse = TRUE , data = ahs_df ) Variable Recoding Add new columns to the data set: ahs_design &lt;- update( ahs_design , tenure = factor( ifelse( is.na( tenure ) , 4 , tenure ) , levels = 1:4 , labels = c( &#39;Owned or being bought&#39; , &#39;Rented for cash rent&#39; , &#39;Occupied without payment of cash rent&#39; , &#39;Not occupied&#39; ) ) , lotsize = factor( 1 + findInterval( lot , c( 5500 , 11000 , 22000 , 44000 , 220000 , 440000 ) ) , levels = 1:7 , labels = c( &quot;Less then 1/8 acre&quot; , &quot;1/8 up to 1/4 acre&quot; , &quot;1/4 up to 1/2 acre&quot; , &quot;1/2 up to 1 acre&quot; , &quot;1 up to 5 acres&quot; , &quot;5 up to 10 acres&quot; , &quot;10 acres or more&quot; ) ) , below_poverty = as.numeric( poor &lt; 100 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( ahs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ tenure , ahs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , ahs_design ) svyby( ~ one , ~ tenure , ahs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ rooms , ahs_design , na.rm = TRUE ) svyby( ~ rooms , ~ tenure , ahs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ lotsize , ahs_design , na.rm = TRUE ) svyby( ~ lotsize , ~ tenure , ahs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ rooms , ahs_design , na.rm = TRUE ) svyby( ~ rooms , ~ tenure , ahs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ lotsize , ahs_design , na.rm = TRUE ) svyby( ~ lotsize , ~ tenure , ahs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ rooms , ahs_design , 0.5 , na.rm = TRUE ) svyby( ~ rooms , ~ tenure , ahs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ rooms , denominator = ~ rent , ahs_design , na.rm = TRUE ) Subsetting Restrict the survey design to homes with a garage or carport: sub_ahs_design &lt;- subset( ahs_design , garage == 1 ) Calculate the mean (average) of this subset: svymean( ~ rooms , sub_ahs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ rooms , ahs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ rooms , ~ tenure , ahs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ahs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ rooms , ahs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ rooms , ahs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ rooms , ahs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ below_poverty , ahs_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( rooms ~ below_poverty , ahs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ below_poverty + lotsize , ahs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( rooms ~ below_poverty + lotsize , ahs_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for AHS users, this code replicates previously-presented examples: library(srvyr) ahs_srvyr_design &lt;- as_survey( ahs_design ) Calculate the mean (average) of a linear variable, overall and by groups: ahs_srvyr_design %&gt;% summarize( mean = survey_mean( rooms , na.rm = TRUE ) ) ahs_srvyr_design %&gt;% group_by( tenure ) %&gt;% summarize( mean = survey_mean( rooms , na.rm = TRUE ) ) Replication Example The example below matches statistics and standard errors from this table pulled from the US Census Bureau’s Quick Guide to Estimating Variance Using Replicate Weights: Compute the statistics and standard errors for monthly housing costs by owner/renter status of the unit: means &lt;- c( 1241.8890 , 972.6051 , 170.0121 ) std_err &lt;- c( 7.3613 , 5.6956 , 6.1586 ) ci_lb &lt;- c( 1227.3511 , 961.3569 , 157.8495 ) ci_ub &lt;- c( 1256.4270 , 983.8532 , 182.1747 ) results &lt;- svyby( ~ zsmhc , ~ tenure , ahs_design , svymean , na.rm = TRUE , na.rm.all = TRUE ) ci_res &lt;- confint( results , df = degf( ahs_design ) + 1 ) stopifnot( all( round( coef( results ) , 4 ) == means ) ) stopifnot( all( round( SE( results ) , 4 ) == std_err ) ) stopifnot( all( round( ci_res[ , 1 ] , 4 ) == ci_lb ) ) stopifnot( all( round( ci_res[ , 2 ] , 4 ) == ci_ub ) ) "],["american-national-election-study-anes.html", "American National Election Study (ANES) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " American National Election Study (ANES) The American National Election Study (ANES) collects information on political belief and behavior from eligible voters in the United States. Most tables contain one row per sampled eligible voter. A complex sample survey designed to generalize to eligible voters in the United States. Time series studies released biennially. Administered by Stanford University and the University of Michigan and funded by the National Science Foundation. Simplified Download and Importation The R lodown package easily downloads and imports all available ANES microdata by simply specifying \"anes\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;anes&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ANES&quot; ) , your_email = &quot;email@address.com&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the ANES catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available ANES microdata files anes_cat &lt;- get_catalog( &quot;anes&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ANES&quot; ) , your_email = &quot;email@address.com&quot; ) # 2016 only anes_cat &lt;- subset( anes_cat , directory == &quot;2016 Time Series Study&quot; ) # download the microdata to your local computer anes_cat &lt;- lodown( &quot;anes&quot; , anes_cat , your_email = &quot;email@address.com&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) anes_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ANES&quot; , &quot;2016 Time Series Study/anes_timeseries_2016_.rds&quot; ) ) anes_design &lt;- svydesign( ~v160202 , strata = ~v160201 , data = anes_df , weights = ~v160102 , nest = TRUE ) Variable Recoding Add new columns to the data set: anes_design &lt;- update( anes_design , one = 1 , pope_francis_score = ifelse( v162094 %in% 0:100 , v162094 , NA ) , christian_fundamentalist_score = ifelse( v162095 %in% 0:100 , v162095 , NA ) , primary_voter = ifelse( v161021 %in% 1:2 , as.numeric( v161021 == 1 ) , NA ) , think_gov_spend = factor( v161514 , levels = 1:4 , labels = c( &#39;foreign aid&#39; , &#39;medicare&#39; , &#39;national defense&#39; , &#39;social security&#39; ) ) , undoc_kids = factor( v161195x , levels = 1:6 , labels = c( &#39;should sent back - favor a great deal&#39; , &#39;should sent back - favor a moderate amount&#39; , &#39;should sent back - favor a little&#39; , &#39;should allow to stay - favor a little&#39; , &#39;should allow to stay - favor a moderate amount&#39; , &#39;should allow to stay - favor a great deal&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( anes_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ undoc_kids , anes_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , anes_design ) svyby( ~ one , ~ undoc_kids , anes_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ pope_francis_score , anes_design , na.rm = TRUE ) svyby( ~ pope_francis_score , ~ undoc_kids , anes_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ think_gov_spend , anes_design , na.rm = TRUE ) svyby( ~ think_gov_spend , ~ undoc_kids , anes_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ pope_francis_score , anes_design , na.rm = TRUE ) svyby( ~ pope_francis_score , ~ undoc_kids , anes_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ think_gov_spend , anes_design , na.rm = TRUE ) svyby( ~ think_gov_spend , ~ undoc_kids , anes_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ pope_francis_score , anes_design , 0.5 , na.rm = TRUE ) svyby( ~ pope_francis_score , ~ undoc_kids , anes_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ christian_fundamentalist_score , denominator = ~ pope_francis_score , anes_design , na.rm = TRUE ) Subsetting Restrict the survey design to party id: independent: sub_anes_design &lt;- subset( anes_design , v161158x == 4 ) Calculate the mean (average) of this subset: svymean( ~ pope_francis_score , sub_anes_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ pope_francis_score , anes_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ pope_francis_score , ~ undoc_kids , anes_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( anes_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ pope_francis_score , anes_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ pope_francis_score , anes_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ pope_francis_score , anes_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ primary_voter , anes_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( pope_francis_score ~ primary_voter , anes_design ) Perform a chi-squared test of association for survey data: svychisq( ~ primary_voter + think_gov_spend , anes_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( pope_francis_score ~ primary_voter + think_gov_spend , anes_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ANES users, this code replicates previously-presented examples: library(srvyr) anes_srvyr_design &lt;- as_survey( anes_design ) Calculate the mean (average) of a linear variable, overall and by groups: anes_srvyr_design %&gt;% summarize( mean = survey_mean( pope_francis_score , na.rm = TRUE ) ) anes_srvyr_design %&gt;% group_by( undoc_kids ) %&gt;% summarize( mean = survey_mean( pope_francis_score , na.rm = TRUE ) ) Replication Example "],["american-time-use-survey-atus.html", "American Time Use Survey (ATUS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " American Time Use Survey (ATUS) The American Time Use Survey (ATUS) collects information about how we spend our time. Sampled individuals write down everything they do for a single twenty-four hour period, in ten minute intervals. Many economists use ATUS to study uncompensated work (chores and childcare), but you can use it to learn that even in the dead of night, one-twentieth of us are awake. Many tables with different structures described in the user guide. A complex sample survey designed to generalize to the number of person-hours in the civilian non-institutional population of the United States aged older than fourteen. Released annually since 2003. Administered by the Bureau of Labor Statistics. Simplified Download and Importation The R lodown package easily downloads and imports all available ATUS microdata by simply specifying \"atus\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;atus&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the ATUS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available ATUS microdata files atus_cat &lt;- get_catalog( &quot;atus&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; ) ) # 2015 only atus_cat &lt;- subset( atus_cat , directory == 2015 ) # download the microdata to your local computer atus_cat &lt;- lodown( &quot;atus&quot; , atus_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) atusact &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; , &quot;2015/atusact.rds&quot; ) ) atusact &lt;- atusact[ c( &#39;tucaseid&#39; , &#39;tutier1code&#39; , &#39;tutier2code&#39; , &#39;tuactdur24&#39; ) ] atusresp &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; , &quot;2015/atusresp.rds&quot; ) ) atusresp &lt;- atusresp[ c( &#39;tucaseid&#39; , &#39;tufinlwgt&#39; , &#39;tulineno&#39; ) ] atusrost &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; , &quot;2015/atusrost.rds&quot; ) ) atusrost &lt;- atusrost[ , c( &#39;tucaseid&#39; , &#39;tulineno&#39; , &#39;teage&#39; , &#39;tesex&#39; ) ] atuswgts &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ATUS&quot; , &quot;2015/atuswgts.rds&quot; ) ) atuswgts &lt;- atuswgts[ , c( 1 , grep( &#39;finlwgt&#39; , names( atuswgts ) ) ) ] # looking at the 2012 lexicon, travel-related activities # have a tier 1 code of 18 -- # http://www.bls.gov/tus/lexiconnoex2012.pdf#page=22 # for all records where the tier 1 code is 18 (travel) # replace that tier 1 of 18 with whatever&#39;s stored in tier 2 atusact[ atusact$tutier1code == 18 , &#39;tutier1code&#39; ] &lt;- atusact[ atusact$tutier1code == 18 , &#39;tutier2code&#39; ] # this will distribute all travel-related activities # to the appropriate tier 1 category, which matches # the structure of the 2012 bls table available at # http://www.bls.gov/tus/tables/a1_2012.pdf # sum up activity duration at the respondent-level # *and* also the tier 1 code level # (using tucaseid as the unique identifier) # from the activities file x &lt;- aggregate( tuactdur24 ~ tucaseid + tutier1code , data = atusact , sum ) # now table `x` contains # one record per person per major activity category # reshape this data from &quot;long&quot; to &quot;wide&quot; format, # creating a one-record-per-person table y &lt;- reshape( x , idvar = &#39;tucaseid&#39; , timevar = &#39;tutier1code&#39; , direction = &#39;wide&#39; ) y[ is.na( y ) ] &lt;- 0 # convert all missings to zeroes, # since those individuals simply did not # engage in those activities during their interview day # (meaning they should have zero minutes of time) # except for the first column (the unique identifier, # replace each column by the quotient of itself and sixty y[ , -1 ] &lt;- y[ , -1 ] / 60 # now you&#39;ve got an activity file `y` # with one record per respondent # merge together the data.frame objects with all needed columns # in order to create a replicate-weighted survey design object # merge the respondent file with the newly-created activity file # (which, remember, is also one-record-per-respondent) resp_y &lt;- merge( atusresp , y ) # confirm that the result of the merge has the same number of records # as the original bls atus respondent file. (this is a worthwhile check) stopifnot( nrow( resp_y ) == nrow( atusresp ) ) # merge that result with the roster file # note that the roster file has multiple records per `tucaseid` # but only the `tulineno` columns equal to 1 will match # records in the original respondent file, this merge works. resp_y_rost &lt;- merge( resp_y , atusrost ) # confirm that the result of the merge has the same number of records stopifnot( nrow( resp_y_rost ) == nrow( atusresp ) ) # merge that result with the replicate weights file z &lt;- merge( resp_y_rost , atuswgts ) # confirm that the result of the merge has the same number of records stopifnot( nrow( z ) == nrow( atusresp ) ) # remove dots from column names names( z ) &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , names( z ) ) # add a column of ones z$one &lt;- 1 atus_design &lt;- svrepdesign( weights = ~tufinlwgt , repweights = &quot;finlwgt[1-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , mse = TRUE , data = z ) Variable Recoding Add new columns to the data set: atus_design &lt;- update( atus_design , any_care = as.numeric( tuactdur24_3 &gt; 0 ) , age_category = factor( 1 + findInterval( teage , c( 18 , 35 , 65 ) ) , labels = c( &quot;under 18&quot; , &quot;18 - 34&quot; , &quot;35 - 64&quot; , &quot;65 or older&quot; ) ) ) # caring for and helping household members row # which we know is top level 03 from # http://www.bls.gov/tus/lexiconnoex2012.pdf Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( atus_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_category , atus_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , atus_design ) svyby( ~ one , ~ age_category , atus_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ tuactdur24_1 , atus_design ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ tesex , atus_design ) svyby( ~ tesex , ~ age_category , atus_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ tuactdur24_1 , atus_design ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ tesex , atus_design ) svyby( ~ tesex , ~ age_category , atus_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ tuactdur24_1 , atus_design , 0.5 ) svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ tuactdur24_5 , denominator = ~ tuactdur24_12 , atus_design ) Subsetting Restrict the survey design to any time volunteering: sub_atus_design &lt;- subset( atus_design , tuactdur24_15 &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ tuactdur24_1 , sub_atus_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ tuactdur24_1 , atus_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ tuactdur24_1 , ~ age_category , atus_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( atus_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ tuactdur24_1 , atus_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ tuactdur24_1 , atus_design , deff = TRUE ) # SRS with replacement svymean( ~ tuactdur24_1 , atus_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ any_care , atus_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( tuactdur24_1 ~ any_care , atus_design ) Perform a chi-squared test of association for survey data: svychisq( ~ any_care + tesex , atus_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( tuactdur24_1 ~ any_care + tesex , atus_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ATUS users, this code replicates previously-presented examples: library(srvyr) atus_srvyr_design &lt;- as_survey( atus_design ) Calculate the mean (average) of a linear variable, overall and by groups: atus_srvyr_design %&gt;% summarize( mean = survey_mean( tuactdur24_1 ) ) atus_srvyr_design %&gt;% group_by( age_category ) %&gt;% summarize( mean = survey_mean( tuactdur24_1 ) ) Replication Example "],["behavioral-risk-factor-surveillance-system-brfss.html", "Behavioral Risk Factor Surveillance System (BRFSS) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " Behavioral Risk Factor Surveillance System (BRFSS) A health behavior telephone interview survey with enough sample size to examine all fifty states. One table with one row per telephone respondent. A complex sample survey designed to generalize to the civilian non-institutional adult population of the United States. Released annually since 1984 but all states did not participate until 1994. Administered by the Centers for Disease Control and Prevention. Simplified Download and Importation The R lodown package easily downloads and imports all available BRFSS microdata by simply specifying \"brfss\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;brfss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;BRFSS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the BRFSS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available BRFSS microdata files brfss_cat &lt;- get_catalog( &quot;brfss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;BRFSS&quot; ) ) # 2016 only brfss_cat &lt;- subset( brfss_cat , year == 2016 ) # download the microdata to your local computer brfss_cat &lt;- lodown( &quot;brfss&quot; , brfss_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) brfss_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;BRFSS&quot; , &quot;2016 main.rds&quot; ) ) variables_to_keep &lt;- c( &#39;one&#39; , &#39;xpsu&#39; , &#39;xststr&#39; , &#39;xllcpwt&#39; , &#39;genhlth&#39; , &#39;medcost&#39; , &#39;xstate&#39; , &#39;xage80&#39; , &#39;nummen&#39; , &#39;numadult&#39; , &#39;hlthpln1&#39; ) brfss_df &lt;- brfss_df[ variables_to_keep ] ; gc() brfss_design &lt;- svydesign( id = ~ xpsu , strata = ~ xststr , data = brfss_df , weight = ~ xllcpwt , nest = TRUE ) Variable Recoding Add new columns to the data set: brfss_design &lt;- update( brfss_design , fair_or_poor_health = ifelse( genhlth %in% 1:5 , as.numeric( genhlth &gt; 3 ) , NA ) , couldnt_see_doc_due_to_cost = factor( medcost , levels = c( 1 , 2 , 7 , 9 ) , labels = c( &quot;yes&quot; , &quot;no&quot; , &quot;dk&quot; , &quot;rf&quot; ) ) , state_name = factor( xstate , levels = c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 66, 72, 78) , labels = c(&quot;ALABAMA&quot;, &quot;ALASKA&quot;, &quot;ARIZONA&quot;, &quot;ARKANSAS&quot;, &quot;CALIFORNIA&quot;, &quot;COLORADO&quot;, &quot;CONNECTICUT&quot;, &quot;DELAWARE&quot;, &quot;DISTRICT OF COLUMBIA&quot;, &quot;FLORIDA&quot;, &quot;GEORGIA&quot;, &quot;HAWAII&quot;, &quot;IDAHO&quot;, &quot;ILLINOIS&quot;, &quot;INDIANA&quot;, &quot;IOWA&quot;, &quot;KANSAS&quot;, &quot;KENTUCKY&quot;, &quot;LOUISIANA&quot;, &quot;MAINE&quot;, &quot;MARYLAND&quot;, &quot;MASSACHUSETTS&quot;, &quot;MICHIGAN&quot;, &quot;MINNESOTA&quot;, &quot;MISSISSIPPI&quot;, &quot;MISSOURI&quot;, &quot;MONTANA&quot;, &quot;NEBRASKA&quot;, &quot;NEVADA&quot;, &quot;NEW HAMPSHIRE&quot;, &quot;NEW JERSEY&quot;, &quot;NEW MEXICO&quot;, &quot;NEW YORK&quot;, &quot;NORTH CAROLINA&quot;, &quot;NORTH DAKOTA&quot;, &quot;OHIO&quot;, &quot;OKLAHOMA&quot;, &quot;OREGON&quot;, &quot;PENNSYLVANIA&quot;, &quot;RHODE ISLAND&quot;, &quot;SOUTH CAROLINA&quot;, &quot;SOUTH DAKOTA&quot;, &quot;TENNESSEE&quot;, &quot;TEXAS&quot;, &quot;UTAH&quot;, &quot;VERMONT&quot;, &quot;VIRGINIA&quot;, &quot;WASHINGTON&quot;, &quot;WEST VIRGINIA&quot;, &quot;WISCONSIN&quot;, &quot;WYOMING&quot;, &quot;GUAM&quot;, &quot;PUERTO RICO&quot;, &quot;U.S. VIRGIN ISLANDS&quot;) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( brfss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , brfss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , brfss_design ) svyby( ~ one , ~ state_name , brfss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ xage80 , brfss_design ) svyby( ~ xage80 , ~ state_name , brfss_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ couldnt_see_doc_due_to_cost , brfss_design , na.rm = TRUE ) svyby( ~ couldnt_see_doc_due_to_cost , ~ state_name , brfss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ xage80 , brfss_design ) svyby( ~ xage80 , ~ state_name , brfss_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ couldnt_see_doc_due_to_cost , brfss_design , na.rm = TRUE ) svyby( ~ couldnt_see_doc_due_to_cost , ~ state_name , brfss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ xage80 , brfss_design , 0.5 ) svyby( ~ xage80 , ~ state_name , brfss_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ nummen , denominator = ~ numadult , brfss_design , na.rm = TRUE ) Subsetting Restrict the survey design to persons without health insurance: sub_brfss_design &lt;- subset( brfss_design , hlthpln1 == 2 ) Calculate the mean (average) of this subset: svymean( ~ xage80 , sub_brfss_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ xage80 , brfss_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ xage80 , ~ state_name , brfss_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( brfss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ xage80 , brfss_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ xage80 , brfss_design , deff = TRUE ) # SRS with replacement svymean( ~ xage80 , brfss_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ fair_or_poor_health , brfss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( xage80 ~ fair_or_poor_health , brfss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ fair_or_poor_health + couldnt_see_doc_due_to_cost , brfss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( xage80 ~ fair_or_poor_health + couldnt_see_doc_due_to_cost , brfss_design ) summary( glm_result ) The example below matches confidence intervals from this table pulled from the BRFSS Web Enabled Analysis Tool: Match the C.I. for Row %: Replication Example result &lt;- svymean( ~ couldnt_see_doc_due_to_cost , subset( brfss_design , couldnt_see_doc_due_to_cost %in% c( &#39;yes&#39; , &#39;no&#39; ) ) , na.rm = TRUE ) stopifnot( round( confint( result )[ 1 , 1 ] , 3 ) == 0.128 ) stopifnot( round( confint( result )[ 1 , 2 ] , 3 ) == 0.133 ) stopifnot( round( confint( result )[ 2 , 1 ] , 3 ) == 0.867 ) stopifnot( round( confint( result )[ 2 , 2 ] , 3 ) == 0.872 ) "],["basic-stand-alone-public-use-file-bsapuf.html", "Basic Stand Alone Public Use File (BSAPUF) Simplified Download and Importation Analysis Examples with SQL and RSQLite   Analysis Examples with dplyr  ", " Basic Stand Alone Public Use File (BSAPUF) The CMS Basic Stand Alone Public Use File (BSAPUF) contains a five percent sample of Medicare beneficiary spending and utilization in the enrolled population. Multiple non-linkable tables, each with one row per beneficiary event. The population of elderly and disabled individuals covered by fee-for-service Medicare in the United States. No listed update frequency. Maintained by the United States Centers for Medicare &amp; Medicaid Services (CMS) Simplified Download and Importation The R lodown package easily downloads and imports all available BSAPUF microdata by simply specifying \"bsapuf\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;bsapuf&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;BSAPUF&quot; ) ) Analysis Examples with SQL and RSQLite   Connect to a database: library(DBI) dbdir &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;BSAPUF&quot; , &quot;SQLite.db&quot; ) db &lt;- dbConnect( RSQLite::SQLite() , dbdir ) Variable Recoding Add new columns to the data set: dbSendQuery( db , &quot;ALTER TABLE bsa_partd_events_2008 ADD COLUMN brand_name_drug INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE bsa_partd_events_2008 SET brand_name_drug = CASE WHEN pde_drug_type_cd = 1 THEN 1 WHEN pde_drug_type_cd = 2 THEN 0 ELSE NULL END&quot; ) Unweighted Counts Count the unweighted number of records in the SQL table, overall and by groups: dbGetQuery( db , &quot;SELECT COUNT(*) FROM bsa_partd_events_2008&quot; ) dbGetQuery( db , &quot;SELECT bene_sex_ident_cd , COUNT(*) FROM bsa_partd_events_2008 GROUP BY bene_sex_ident_cd&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT AVG( pde_drug_cost ) FROM bsa_partd_events_2008&quot; ) dbGetQuery( db , &quot;SELECT bene_sex_ident_cd , AVG( pde_drug_cost ) AS mean_pde_drug_cost FROM bsa_partd_events_2008 GROUP BY bene_sex_ident_cd&quot; ) Calculate the distribution of a categorical variable: dbGetQuery( db , &quot;SELECT bene_age_cat_cd , COUNT(*) / ( SELECT COUNT(*) FROM bsa_partd_events_2008 ) AS share_bene_age_cat_cd FROM bsa_partd_events_2008 GROUP BY bene_age_cat_cd&quot; ) Calculate the sum of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT SUM( pde_drug_cost ) FROM bsa_partd_events_2008&quot; ) dbGetQuery( db , &quot;SELECT bene_sex_ident_cd , SUM( pde_drug_cost ) AS sum_pde_drug_cost FROM bsa_partd_events_2008 GROUP BY bene_sex_ident_cd&quot; ) Calculate the 25th, median, and 75th percentiles of a linear variable, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT LOWER_QUARTILE( pde_drug_cost ) , MEDIAN( pde_drug_cost ) , UPPER_QUARTILE( pde_drug_cost ) FROM bsa_partd_events_2008&quot; ) dbGetQuery( db , &quot;SELECT bene_sex_ident_cd , LOWER_QUARTILE( pde_drug_cost ) AS lower_quartile_pde_drug_cost , MEDIAN( pde_drug_cost ) AS median_pde_drug_cost , UPPER_QUARTILE( pde_drug_cost ) AS upper_quartile_pde_drug_cost FROM bsa_partd_events_2008 GROUP BY bene_sex_ident_cd&quot; ) Subsetting Limit your SQL analysis to events where patient paid 100% of drug’s cost with WHERE: dbGetQuery( db , &quot;SELECT AVG( pde_drug_cost ) FROM bsa_partd_events_2008 WHERE pde_drug_pat_pay_cd = 3&quot; ) Measures of Uncertainty Calculate the variance and standard deviation, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT VARIANCE( pde_drug_cost ) , STDEV( pde_drug_cost ) FROM bsa_partd_events_2008&quot; ) dbGetQuery( db , &quot;SELECT bene_sex_ident_cd , VARIANCE( pde_drug_cost ) AS var_pde_drug_cost , STDEV( pde_drug_cost ) AS stddev_pde_drug_cost FROM bsa_partd_events_2008 GROUP BY bene_sex_ident_cd&quot; ) Regression Models and Tests of Association Perform a t-test: bsapuf_slim_df &lt;- dbGetQuery( db , &quot;SELECT pde_drug_cost , brand_name_drug , bene_age_cat_cd FROM bsa_partd_events_2008&quot; ) t.test( pde_drug_cost ~ brand_name_drug , bsapuf_slim_df ) Perform a chi-squared test of association: this_table &lt;- table( bsapuf_slim_df[ , c( &quot;brand_name_drug&quot; , &quot;bene_age_cat_cd&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( pde_drug_cost ~ brand_name_drug + bene_age_cat_cd , data = bsapuf_slim_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for BSAPUF users, this code replicates previously-presented examples: library(dplyr) library(dbplyr) dplyr_db &lt;- dplyr::src_sqlite( dbdir ) bsapuf_tbl &lt;- tbl( dplyr_db , &#39;bsa_partd_events_2008&#39; ) Calculate the mean (average) of a linear variable, overall and by groups: bsapuf_tbl %&gt;% summarize( mean = mean( pde_drug_cost ) ) bsapuf_tbl %&gt;% group_by( bene_sex_ident_cd ) %&gt;% summarize( mean = mean( pde_drug_cost ) ) "],["brazilian-censo-demografico-censo.html", "Brazilian Censo Demografico (CENSO) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " Brazilian Censo Demografico (CENSO) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; Brazil’s decennial census. One table with one row per household and a second table with one row per individual within each household. The 2000 Censo also includes a table with one record per family inside each household. An enumeration of the civilian non-institutional population of Brazil. Released decennially by IBGE since 2000, however earlier extracts are available from IPUMS International. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available CENSO microdata by simply specifying \"censo\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;censo&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CENSO&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the CENSO catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available CENSO microdata files censo_cat &lt;- get_catalog( &quot;censo&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CENSO&quot; ) ) # 2010 only censo_cat &lt;- subset( censo_cat , year == 2010 ) # download the microdata to your local computer censo_cat &lt;- lodown( &quot;censo&quot; , censo_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) # choose columns to import from both household and person files columns_to_import &lt;- c( &#39;v6531&#39; , &#39;v6033&#39; , &#39;v0640&#39; , &#39;v0001&#39; , &#39;v0601&#39; ) # initiate a data.frame to stack all downloaded censo states censo_df &lt;- data.frame( NULL ) # only construct one censo design at a time (2000 and 2010 should not be stacked) stopifnot( length( unique( censo_cat[ , &#39;year&#39; ] ) ) == 1 ) # loop through all downloaded censo states for( this_state in seq( nrow( censo_cat ) ) ){ # add the design information to the columns to import these_columns_to_import &lt;- unique( c( columns_to_import , as.character( censo_cat[ this_state , c( &#39;weight&#39; , paste0( &#39;fpc&#39; , 1:5 ) ) ] ) ) ) # remove NAs these_columns_to_import &lt;- these_columns_to_import[ !is.na( these_columns_to_import ) ] # load structure files, lowercase variable names, set unwanted columns to missing dom_stru &lt;- SAScii::parse.SAScii( censo_cat[ this_state , &#39;dom_sas&#39; ] ) dom_stru$varname &lt;- tolower( dom_stru$varname ) pes_stru &lt;- SAScii::parse.SAScii( censo_cat[ this_state , &#39;pes_sas&#39; ] ) pes_stru$varname &lt;- tolower( pes_stru$varname ) # import fixed-width files this_censo_dom_df &lt;- data.frame( readr::read_fwf( censo_cat[ this_state , &#39;dom_file&#39; ] , readr::fwf_widths( abs( dom_stru$width ) , col_names = dom_stru[ , &#39;varname&#39; ] ) , col_types = paste0( ifelse( !( dom_stru$varname %in% these_columns_to_import ) , &quot;_&quot; , ifelse( dom_stru$char , &quot;c&quot; , &quot;d&quot; ) ) , collapse = &quot;&quot; ) ) ) this_censo_pes_df &lt;- data.frame( readr::read_fwf( censo_cat[ this_state , &#39;pes_file&#39; ] , readr::fwf_widths( abs( pes_stru$width ) , col_names = pes_stru[ , &#39;varname&#39; ] ) , col_types = paste0( ifelse( !( pes_stru$varname %in% these_columns_to_import ) , &quot;_&quot; , ifelse( pes_stru$char , &quot;c&quot; , &quot;d&quot; ) ) , collapse = &quot;&quot; ) ) ) # add decimals for( this_variable in these_columns_to_import ) { if( ( this_variable %in% names( this_censo_dom_df ) ) &amp; !isTRUE( all.equal( 1 , dom_stru[ dom_stru$varname == this_variable , &#39;divisor&#39; ] ) ) ){ this_censo_dom_df[ , this_variable ] &lt;- dom_stru[ dom_stru$varname == this_variable , &#39;divisor&#39; ] * this_censo_dom_df[ , this_variable ] } if( ( this_variable %in% names( this_censo_pes_df ) ) &amp; !isTRUE( all.equal( 1 , pes_stru[ pes_stru$varname == this_variable , &#39;divisor&#39; ] ) ) ){ this_censo_pes_df[ , this_variable ] &lt;- pes_stru[ pes_stru$varname == this_variable , &#39;divisor&#39; ] * this_censo_pes_df[ , this_variable ] } } # merge household and person tables var_names_diff &lt;- setdiff(names(this_censo_pes_df),names(this_censo_dom_df) ) this_censo_df &lt;- merge( this_censo_dom_df , this_censo_pes_df[, c(&quot;v0001&quot;,&quot;v0300&quot;,var_names_diff)], by.x = c(&quot;v0001&quot;,&quot;v0300&quot;) , by.y = c(&quot;v0001&quot;, &quot;v0300&quot; )) # confirm one record per person, with household information merged on stopifnot( nrow( this_censo_df ) == nrow( this_censo_pes_df ) ) rm( this_censo_dom_df , this_censo_pes_df ) ; gc() # stack the merged tables censo_df &lt;- rbind( censo_df , this_censo_df ) rm( this_censo_df ) ; gc() } # add a column of ones censo_df[ , &#39;one&#39; ] &lt;- 1 # calculate the finite population correction for each stratum to construct a # sampling design with weighting areas as strata and households as psu # the real censo design is stratified with &quot;setor censitarios&quot; rather than # &quot;area de ponderacao&quot; but those are not disclosed due to confidentiality # v0010 is the person or household weight # v0011 is the weighting area identifier # both of these are specified inside `censo_cat[ c( &#39;fpc1&#39; , &#39;weight&#39; ) ]` fpc_sums &lt;- aggregate( v0010 ~ v0011 , data = censo_df , sum ) names( fpc_sums )[ 2 ] &lt;- &#39;fpc&#39; censo_df &lt;- merge( censo_df , fpc_sums ) ; gc() censo_wgts &lt;- survey::bootweights( strata = censo_df[ , censo_cat[ 1 , &#39;fpc1&#39; ] ] , psu = censo_df[ , censo_cat[ 1 , &#39;fpc4&#39; ] ] , replicates = 80 , fpc = censo_df[ , &#39;fpc&#39; ] ) # construct a complex survey design object censo_design &lt;- survey::svrepdesign( weight = ~ v0010 , repweights = censo_wgts$repweights , type = &quot;bootstrap&quot;, combined.weights = FALSE , scale = censo_wgts$scale , rscales = censo_wgts$rscales , data = censo_df ) rm( censo_df , censo_wgts , fpc_sums ) ; gc() Variable Recoding Add new columns to the data set: censo_design &lt;- update( censo_design , nmorpob1 = ifelse( v6531 &gt;= 0 , as.numeric( v6531 &lt; 70 ) , NA ) , nmorpob2 = ifelse( v6531 &gt;= 0 , as.numeric( v6531 &lt; 80 ) , NA ) , nmorpob3 = ifelse( v6531 &gt;= 0 , as.numeric( v6531 &lt; 90 ) , NA ) , nmorpob4 = ifelse( v6531 &gt;= 0 , as.numeric( v6531 &lt; 100 ) , NA ) , nmorpob5 = ifelse( v6531 &gt;= 0 , as.numeric( v6531 &lt; 140 ) , NA ) , nmorpob6 = ifelse( v6531 &gt;= 0 , as.numeric( v6531 &lt; 272.50 ) , NA ) , sexo = factor( v0601 , labels = c( &quot;masculino&quot; , &quot;feminino&quot; ) ) , state_name = factor( v0001 , levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) , labels = c( &quot;Rondonia&quot; , &quot;Acre&quot; , &quot;Amazonas&quot; , &quot;Roraima&quot; , &quot;Para&quot; , &quot;Amapa&quot; , &quot;Tocantins&quot; , &quot;Maranhao&quot; , &quot;Piaui&quot; , &quot;Ceara&quot; , &quot;Rio Grande do Norte&quot; , &quot;Paraiba&quot; , &quot;Pernambuco&quot; , &quot;Alagoas&quot; , &quot;Sergipe&quot; , &quot;Bahia&quot; , &quot;Minas Gerais&quot; , &quot;Espirito Santo&quot; , &quot;Rio de Janeiro&quot; , &quot;Sao Paulo&quot; , &quot;Parana&quot; , &quot;Santa Catarina&quot; , &quot;Rio Grande do Sul&quot; , &quot;Mato Grosso do Sul&quot; , &quot;Mato Grosso&quot; , &quot;Goias&quot; , &quot;Distrito Federal&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( censo_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , censo_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , censo_design ) svyby( ~ one , ~ state_name , censo_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ v6033 , censo_design ) svyby( ~ v6033 , ~ state_name , censo_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sexo , censo_design ) svyby( ~ sexo , ~ state_name , censo_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ v6033 , censo_design ) svyby( ~ v6033 , ~ state_name , censo_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sexo , censo_design ) svyby( ~ sexo , ~ state_name , censo_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ v6033 , censo_design , 0.5 ) svyby( ~ v6033 , ~ state_name , censo_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ nmorpob1 , denominator = ~ nmorpob1 + one , censo_design , na.rm = TRUE ) Subsetting Restrict the survey design to married persons: sub_censo_design &lt;- subset( censo_design , v0640 == 1 ) Calculate the mean (average) of this subset: svymean( ~ v6033 , sub_censo_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ v6033 , censo_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ v6033 , ~ state_name , censo_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( censo_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ v6033 , censo_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ v6033 , censo_design , deff = TRUE ) # SRS with replacement svymean( ~ v6033 , censo_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ nmorpob6 , censo_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( v6033 ~ nmorpob6 , censo_design ) Perform a chi-squared test of association for survey data: svychisq( ~ nmorpob6 + sexo , censo_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( v6033 ~ nmorpob6 + sexo , censo_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for CENSO users, this code calculates the gini coefficient on complex sample survey data: library(convey) censo_design &lt;- convey_prep( censo_design ) sub_censo_design &lt;- subset( censo_design , v6531 &gt;= 0 ) svygini( ~ v6531 , sub_censo_design , na.rm = TRUE ) Replication Example "],["consumer-expenditure-survey-ces.html", "Consumer Expenditure Survey (CES) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " Consumer Expenditure Survey (CES) The Consumer Expenditure Survey (CES) is the authoritative data source to understand how Americans spend money. Participating households keep a running diary about every purchase over fifteen months. Those diaries are then summed up into precise expenditure categories. One table of survey responses per quarter with one row per sampled household (consumer unit). Additional tables containing one record per expenditure A complex sample survey designed to generalize to the civilian non-institutional population of the United States. Released annually since 1996. Administered by the Bureau of Labor Statistics. Simplified Download and Importation The R lodown package easily downloads and imports all available CES microdata by simply specifying \"ces\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;ces&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the CES catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available CES microdata files ces_cat &lt;- get_catalog( &quot;ces&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; ) ) # 2016 only ces_cat &lt;- subset( ces_cat , year == 2016 ) # download the microdata to your local computer ces_cat &lt;- lodown( &quot;ces&quot; , ces_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) # read in the five quarters of family data files (fmli) fmli161x &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; , &quot;2016/fmli161x.rds&quot; ) ) fmli162 &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; , &quot;2016/fmli162.rds&quot; ) ) fmli163 &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; , &quot;2016/fmli163.rds&quot; ) ) fmli164 &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; , &quot;2016/fmli164.rds&quot; ) ) fmli171 &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CES&quot; , &quot;2016/fmli171.rds&quot; ) ) fmli161x$qtr &lt;- 1 fmli162$qtr &lt;- 2 fmli163$qtr &lt;- 3 fmli164$qtr &lt;- 4 fmli171$qtr &lt;- 5 fmli171 &lt;- fmli171[ , names( fmli161x ) ] fmly &lt;- rbind( fmli161x , fmli162 , fmli163 , fmli164 , fmli171 ) rm( fmli161x , fmli162 , fmli163 , fmli164 , fmli171 ) wtrep &lt;- c( paste0( &quot;wtrep&quot; , stringr::str_pad( 1:44 , 2 , pad = &quot;0&quot; ) ) , &quot;finlwt21&quot; ) for ( i in wtrep ) fmly[ is.na( fmly[ , i ] ) , i ] &lt;- 0 # create a new variable in the fmly data table called &#39;totalexp&#39; # that contains the sum of the total expenditure from the current and previous quarters fmly$totalexp &lt;- rowSums( fmly[ , c( &quot;totexppq&quot; , &quot;totexpcq&quot; ) ] , na.rm = TRUE ) # immediately convert missing values (NA) to zeroes fmly[ is.na( fmly$totalexp ) , &quot;totalexp&quot; ] &lt;- 0 # annualize the total expenditure by multiplying the total expenditure by four, # creating a new variable &#39;annexp&#39; in the fmly data table fmly &lt;- transform( fmly , annexp = totalexp * 4 ) # add a column of ones fmly$one &lt;- 1 # create a vector containing all of the multiply-imputed variables # (leaving the numbers off the end) mi_vars &lt;- gsub( &quot;5$&quot; , &quot;&quot; , grep( &quot;[a-z]5$&quot; , names( fmly ) , value = TRUE ) ) # loop through each of the five variables.. for ( i in 1:5 ){ # copy the &#39;fmly&#39; table over to a new temporary data frame &#39;x&#39; x &lt;- fmly # loop through each of the multiply-imputed variables.. for ( j in mi_vars ){ # copy the contents of the current column (for example &#39;welfare1&#39;) # over to a new column ending in &#39;mi&#39; (for example &#39;welfaremi&#39;) x[ , paste0( j , &#39;mi&#39; ) ] &lt;- x[ , paste0( j , i ) ] # delete the all five of the imputed variable columns x &lt;- x[ , !( names( x ) %in% paste0( j , 1:5 ) ) ] } # save the current table in the sqlite database as &#39;imp1&#39; &#39;imp2&#39; etc. assign( paste0( &#39;imp&#39; , i ) , x ) # remove the temporary table rm( x ) } # containing the five multiply-imputed data tables - imp1 through imp5 ces_design &lt;- svrepdesign( weights = ~finlwt21 , repweights = &quot;wtrep[0-9]+&quot; , data = imputationList( list( imp1 , imp2 , imp3 , imp4 , imp5 ) ) , type = &quot;BRR&quot; , combined.weights = TRUE , mse = TRUE ) rm( imp1 , imp2 , imp3 , imp4 , imp5 ) Variable Recoding Add new columns to the data set: ces_design &lt;- update( ces_design , any_food_stamp = as.numeric( jfs_amtmi &gt; 0 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( ces_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( ces_design , svyby( ~ one , ~ bls_urbn , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( ces_design , svytotal( ~ one ) ) ) MIcombine( with( ces_design , svyby( ~ one , ~ bls_urbn , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( ces_design , svymean( ~ annexp ) ) ) MIcombine( with( ces_design , svyby( ~ annexp , ~ bls_urbn , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( ces_design , svymean( ~ sex_ref ) ) ) MIcombine( with( ces_design , svyby( ~ sex_ref , ~ bls_urbn , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( ces_design , svytotal( ~ annexp ) ) ) MIcombine( with( ces_design , svyby( ~ annexp , ~ bls_urbn , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( ces_design , svytotal( ~ sex_ref ) ) ) MIcombine( with( ces_design , svyby( ~ sex_ref , ~ bls_urbn , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( ces_design , svyquantile( ~ annexp , 0.5 , se = TRUE ) ) ) MIcombine( with( ces_design , svyby( ~ annexp , ~ bls_urbn , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( ces_design , svyratio( numerator = ~ annexp , denominator = ~ fincbtxmi ) ) ) Subsetting Restrict the survey design to california residents: sub_ces_design &lt;- subset( ces_design , state == &#39;06&#39; ) Calculate the mean (average) of this subset: MIcombine( with( sub_ces_design , svymean( ~ annexp ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( ces_design , svymean( ~ annexp ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( ces_design , svyby( ~ annexp , ~ bls_urbn , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ces_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( ces_design , svyvar( ~ annexp ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( ces_design , svymean( ~ annexp , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( ces_design , svymean( ~ annexp , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ any_food_stamp , ces_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( annexp ~ any_food_stamp , ces_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ any_food_stamp + sex_ref , ces_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( ces_design , svyglm( annexp ~ any_food_stamp + sex_ref ) ) ) summary( glm_result ) Replication Example "],["california-health-interview-survey-chis.html", "California Health Interview Survey (CHIS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " California Health Interview Survey (CHIS) Contributed by Carl Ganz &lt;carlganz@gmail.com&gt; The State of California’s edition of the National Health Interview Survey (NHIS), a regional healthcare survey for the nation’s largest state. One adult, one teenage, and one child table, each with one row per sampled respondent. A complex sample survey designed to generalize to the civilian non-institutionalized population of California. Released annually since 2011, and biennially since 2001. Administered by the UCLA Center for Health Policy Research. Simplified Download and Importation The R lodown package easily downloads and imports all available CHIS microdata by simply specifying \"chis\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;chis&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CHIS&quot; ) , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the CHIS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available CHIS microdata files chis_cat &lt;- get_catalog( &quot;chis&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CHIS&quot; ) , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) # 2014 only chis_cat &lt;- subset( chis_cat , year == 2014 ) # download the microdata to your local computer chis_cat &lt;- lodown( &quot;chis&quot; , chis_cat , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) child &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CHIS&quot; , &quot;2014 child.rds&quot; ) ) child$ak7_p1 &lt;- child$ak10_p &lt;- NA child$agecat &lt;- &quot;1 - child&quot; child$no_usual_source_of_care &lt;- as.numeric( child$cd1 == 2 ) # four-category srhs (excellent / very good / good / fair+poor) child$hlthcat &lt;- child$ca6_p1 # load adolescents ages 12-17 teen &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CHIS&quot; , &quot;2014 teen.rds&quot; ) ) teen$ak7_p1 &lt;- teen$ak10_p &lt;- NA teen$agecat &lt;- &quot;2 - adolescent&quot; teen$no_usual_source_of_care &lt;- as.numeric( teen$tf1 == 2 ) # four-category srhs (excellent / very good / good / fair+poor) teen$hlthcat &lt;- teen$tb1_p1 # load adults ages 18+ adult &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CHIS&quot; , &quot;2014 adult.rds&quot; ) ) adult$agecat &lt;- ifelse( adult$srage_p1 &gt;= 65 , &quot;4 - senior&quot; , &quot;3 - adult&quot; ) adult$no_usual_source_of_care &lt;- as.numeric( adult$ah1 == 2 ) # four-category srhs (excellent / very good / good / fair+poor) adult$hlthcat &lt;- c( 1 , 2 , 3 , 4 , 4 )[ adult$ab1 ] # construct a character vector with only the variables needed for the analysis vars_to_keep &lt;- c( grep( &quot;rakedw&quot; , names( adult ) , value = TRUE ) , &#39;hlthcat&#39; , &#39;agecat&#39; , &#39;ak7_p1&#39; , &#39;ak10_p&#39; , &#39;povgwd_p&#39; , &#39;no_usual_source_of_care&#39; ) chis_df &lt;- rbind( child[ vars_to_keep ] , teen[ vars_to_keep ] , adult[ vars_to_keep ] ) # remove labelled classes labelled_cols &lt;- sapply( chis_df , function( w ) class( w ) == &#39;labelled&#39; ) chis_df[ labelled_cols ] &lt;- sapply( chis_df[ labelled_cols ] , as.numeric ) chis_design &lt;- svrepdesign( data = chis_df , weights = ~ rakedw0 , repweights = &quot;rakedw[1-9]&quot; , type = &quot;other&quot; , scale = 1 , rscales = 1 , mse = TRUE ) Variable Recoding Add new columns to the data set: chis_design &lt;- update( chis_design , one = 1 , hlthcat = factor( hlthcat , labels = c( &#39;excellent&#39; , &#39;very good&#39; , &#39;good&#39; , &#39;fair or poor&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( chis_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ hlthcat , chis_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , chis_design ) svyby( ~ one , ~ hlthcat , chis_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ povgwd_p , chis_design ) svyby( ~ povgwd_p , ~ hlthcat , chis_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ agecat , chis_design ) svyby( ~ agecat , ~ hlthcat , chis_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ povgwd_p , chis_design ) svyby( ~ povgwd_p , ~ hlthcat , chis_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ agecat , chis_design ) svyby( ~ agecat , ~ hlthcat , chis_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ povgwd_p , chis_design , 0.5 ) svyby( ~ povgwd_p , ~ hlthcat , chis_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ ak10_p , denominator = ~ ak7_p1 , chis_design , na.rm = TRUE ) Subsetting Restrict the survey design to seniors: sub_chis_design &lt;- subset( chis_design , agecat == &quot;4 - senior&quot; ) Calculate the mean (average) of this subset: svymean( ~ povgwd_p , sub_chis_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ povgwd_p , chis_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ povgwd_p , ~ hlthcat , chis_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( chis_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ povgwd_p , chis_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ povgwd_p , chis_design , deff = TRUE ) # SRS with replacement svymean( ~ povgwd_p , chis_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ no_usual_source_of_care , chis_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( povgwd_p ~ no_usual_source_of_care , chis_design ) Perform a chi-squared test of association for survey data: svychisq( ~ no_usual_source_of_care + agecat , chis_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( povgwd_p ~ no_usual_source_of_care + agecat , chis_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for CHIS users, this code replicates previously-presented examples: library(srvyr) chis_srvyr_design &lt;- as_survey( chis_design ) Calculate the mean (average) of a linear variable, overall and by groups: chis_srvyr_design %&gt;% summarize( mean = survey_mean( povgwd_p ) ) chis_srvyr_design %&gt;% group_by( hlthcat ) %&gt;% summarize( mean = survey_mean( povgwd_p ) ) Replication Example The example below matches statistics and confidence intervals from this table pulled from the AskCHIS online table creator: Match the bottom right weighted count: stopifnot( round( coef( svytotal( ~ one , chis_design ) ) , -3 ) == 37582000 ) Compute the statistics and standard errors for excellent, very good, and good in the rightmost column: ( total_population_ex_vg_good &lt;- svymean( ~ hlthcat , chis_design ) ) # confirm these match stopifnot( identical( as.numeric( round( coef( total_population_ex_vg_good ) * 100 , 1 )[ 1:3 ] ) , c( 23.2 , 31.4 , 28.4 ) ) ) Compute the confidence intervals in the rightmost column: ( total_pop_ci &lt;- confint( total_population_ex_vg_good , df = degf( chis_design ) ) ) # confirm these match stopifnot( identical( as.numeric( round( total_pop_ci * 100 , 1 )[ 1:3 , ] ) , c( 22.1 , 30.1 , 27.1 , 24.2 , 32.7 , 29.6 ) ) ) "],["current-population-survey---annual-social-and-economic-supplement-cpsasec.html", "Current Population Survey - Annual Social and Economic Supplement (CPSASEC) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " Current Population Survey - Annual Social and Economic Supplement (CPSASEC) The March Current Population Survey Annual Social and Economic Supplement has supplied the statistics for the US Census Bureau’s report on income, poverty, and health insurance coverage since 1948. One table with one row per sampled household, a second table with one row per family within each sampled household, and a third table with one row per individual within each of those families. A complex sample survey designed to generalize to the civilian non-institutional population of the United States Released annually since 1998. Administered jointly by the US Census Bureau and the Bureau of Labor Statistics. Simplified Download and Importation The R lodown package easily downloads and imports all available CPSASEC microdata by simply specifying \"cpsasec\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;cpsasec&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CPSASEC&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the CPSASEC catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available CPSASEC microdata files cpsasec_cat &lt;- get_catalog( &quot;cpsasec&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CPSASEC&quot; ) ) # 2016 only cpsasec_cat &lt;- subset( cpsasec_cat , year == 2016 ) # download the microdata to your local computer cpsasec_cat &lt;- lodown( &quot;cpsasec&quot; , cpsasec_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) cpsasec_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CPSASEC&quot; , &quot;2016 cps asec.rds&quot; ) ) variables_to_keep &lt;- c( &#39;a_maritl&#39; , &#39;gestfips&#39; , &#39;a_sex&#39; , &#39;ptotval&#39; , &#39;moop&#39; , &#39;a_age&#39; , &#39;htotval&#39; , &#39;one&#39; , &#39;a_exprrp&#39; , &#39;marsupwt&#39; , grep( &quot;pwwgt&quot; , names( cpsasec_df ) , value = TRUE ) ) cpsasec_df &lt;- cpsasec_df[ variables_to_keep ] ; gc() cpsasec_design &lt;- svrepdesign( weights = ~ marsupwt , repweights = &quot;pwwgt[1-9]&quot; , type = &quot;Fay&quot; , rho = ( 1 - 1 / sqrt( 4 ) ) , data = cpsasec_df , combined.weights = TRUE , mse = TRUE ) Variable Recoding Add new columns to the data set: cpsasec_design &lt;- update( cpsasec_design , a_maritl = factor( a_maritl , labels = c( &quot;married - civilian spouse present&quot; , &quot;married - AF spouse present&quot; , &quot;married - spouse absent&quot; , &quot;widowed&quot; , &quot;divorced&quot; , &quot;separated&quot; , &quot;never married&quot; ) ) , state_name = factor( gestfips , levels = c(1L, 2L, 4L, 5L, 6L, 8L, 9L, 10L, 11L, 12L, 13L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 53L, 54L, 55L, 56L) , labels = c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;District of Columbia&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;) ) , male = as.numeric( a_sex == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( cpsasec_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , cpsasec_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , cpsasec_design ) svyby( ~ one , ~ state_name , cpsasec_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ ptotval , cpsasec_design ) svyby( ~ ptotval , ~ state_name , cpsasec_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ a_maritl , cpsasec_design ) svyby( ~ a_maritl , ~ state_name , cpsasec_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ ptotval , cpsasec_design ) svyby( ~ ptotval , ~ state_name , cpsasec_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ a_maritl , cpsasec_design ) svyby( ~ a_maritl , ~ state_name , cpsasec_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ ptotval , cpsasec_design , 0.5 ) svyby( ~ ptotval , ~ state_name , cpsasec_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ moop , denominator = ~ ptotval , cpsasec_design ) Subsetting Restrict the survey design to persons aged 18-64: sub_cpsasec_design &lt;- subset( cpsasec_design , a_age %in% 18:64 ) Calculate the mean (average) of this subset: svymean( ~ ptotval , sub_cpsasec_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ ptotval , cpsasec_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ ptotval , ~ state_name , cpsasec_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( cpsasec_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ ptotval , cpsasec_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ ptotval , cpsasec_design , deff = TRUE ) # SRS with replacement svymean( ~ ptotval , cpsasec_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , cpsasec_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( ptotval ~ male , cpsasec_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + a_maritl , cpsasec_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( ptotval ~ male + a_maritl , cpsasec_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for CPSASEC users, this code calculates the gini coefficient on complex sample survey data: library(convey) cpsasec_design &lt;- convey_prep( cpsasec_design ) sub_cpsasec_design &lt;- subset( cpsasec_design , a_exprrp %in% 1:2 ) svygini( ~ htotval , sub_cpsasec_design ) Replication Example "],["current-population-survey---basic-monthly-cpsbasic.html", "Current Population Survey - Basic Monthly (CPSBASIC) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Current Population Survey - Basic Monthly (CPSBASIC) The Current Population Survey - Basic Monthly is the monthly labor force survey of the United States. One table with one row per sampled youth respondent. A complex sample survey designed to generalize to the civilian non-institutional population of the United States Released monthly since 1994. Administered jointly by the US Census Bureau and the Bureau of Labor Statistics. Simplified Download and Importation The R lodown package easily downloads and imports all available CPSBASIC microdata by simply specifying \"cpsbasic\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;cpsbasic&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CPSBASIC&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the CPSBASIC catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available CPSBASIC microdata files cpsbasic_cat &lt;- get_catalog( &quot;cpsbasic&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;CPSBASIC&quot; ) ) # march 2017 only cpsbasic_cat &lt;- subset( cpsbasic_cat , year == 2017 &amp; month == 3 ) # download the microdata to your local computer cpsbasic_cat &lt;- lodown( &quot;cpsbasic&quot; , cpsbasic_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) cpsbasic_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;CPSBASIC&quot; , &quot;2017 03 cps basic.rds&quot; ) ) # construct a fake survey design warning( &quot;this survey design produces correct point estimates but incorrect standard errors.&quot; ) cpsbasic_design &lt;- svydesign( ~ 1 , data = cpsbasic_df , weights = ~ pwsswgt ) Variable Recoding Add new columns to the data set: cpsbasic_design &lt;- update( cpsbasic_design , one = 1 , pesex = factor( pesex , levels = 1:2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , weekly_earnings = ifelse( prernwa == -.01 , NA , prernwa ) , # exclude anyone whose hours vary weekly_hours = ifelse( pehrusl1 &lt; 0 , NA , pehrusl1 ) , class_of_worker = factor( peio1cow , levels = 1:8 , labels = c( &quot;government - federal&quot; , &quot;government - state&quot; , &quot;government - local&quot; , &quot;private, for profit&quot; , &quot;private, nonprofit&quot; , &quot;self-employed, incorporated&quot; , &quot;self-employed, unincorporated&quot; , &quot;without pay&quot; ) ) , part_time = ifelse( pemlr == 1 , as.numeric( pehruslt &lt; 35 ) , NA ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( cpsbasic_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ pesex , cpsbasic_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , cpsbasic_design ) svyby( ~ one , ~ pesex , cpsbasic_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE ) svyby( ~ weekly_earnings , ~ pesex , cpsbasic_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ class_of_worker , cpsbasic_design , na.rm = TRUE ) svyby( ~ class_of_worker , ~ pesex , cpsbasic_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE ) svyby( ~ weekly_earnings , ~ pesex , cpsbasic_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ class_of_worker , cpsbasic_design , na.rm = TRUE ) svyby( ~ class_of_worker , ~ pesex , cpsbasic_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ weekly_earnings , cpsbasic_design , 0.5 , na.rm = TRUE ) svyby( ~ weekly_earnings , ~ pesex , cpsbasic_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ weekly_earnings , denominator = ~ weekly_hours , cpsbasic_design , na.rm = TRUE ) Subsetting Restrict the survey design to california residents: sub_cpsbasic_design &lt;- subset( cpsbasic_design , gestfips == 6 ) Calculate the mean (average) of this subset: svymean( ~ weekly_earnings , sub_cpsbasic_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ weekly_earnings , ~ pesex , cpsbasic_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( cpsbasic_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ weekly_earnings , cpsbasic_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ part_time , cpsbasic_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( weekly_earnings ~ part_time , cpsbasic_design ) Perform a chi-squared test of association for survey data: svychisq( ~ part_time + class_of_worker , cpsbasic_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( weekly_earnings ~ part_time + class_of_worker , cpsbasic_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for CPSBASIC users, this code replicates previously-presented examples: library(srvyr) cpsbasic_srvyr_design &lt;- as_survey( cpsbasic_design ) Calculate the mean (average) of a linear variable, overall and by groups: cpsbasic_srvyr_design %&gt;% summarize( mean = survey_mean( weekly_earnings , na.rm = TRUE ) ) cpsbasic_srvyr_design %&gt;% group_by( pesex ) %&gt;% summarize( mean = survey_mean( weekly_earnings , na.rm = TRUE ) ) Replication Example "],["demographic-and-health-surveys-dhs.html", "Demographic and Health Surveys (DHS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Demographic and Health Surveys (DHS) The Demographic and Health Surveys collect data on population, health, HIV, and nutrition in over 90 countries. Many tables, often with one row per male, per female, or per responding household. A complex sample survey designed to generalize to the residents of various countries. Many releases for different countries annually, since 1984. Administered by the ICF International and funded by the US Agency for International Development. Simplified Download and Importation The R lodown package easily downloads and imports all available DHS microdata by simply specifying \"dhs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;dhs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;DHS&quot; ) , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; , your_project = &quot;project&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the DHS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available DHS microdata files dhs_cat &lt;- get_catalog( &quot;dhs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;DHS&quot; ) , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; , your_project = &quot;project&quot; ) # malawi 2004 only dhs_cat &lt;- subset( dhs_cat , country == &#39;Malawi&#39; &amp; year == 2004 ) # download the microdata to your local computer dhs_cat &lt;- lodown( &quot;dhs&quot; , dhs_cat , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; , your_project = &quot;project&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) dhs_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;DHS&quot; , &quot;Malawi/Standard DHS 2004/MWIR4EFL.rds&quot; ) ) # convert the weight column to a numeric type dhs_df$weight &lt;- as.numeric( dhs_df$v005 ) # paste the `sdist` and `v025` columns together # into a single strata variable dhs_df$strata &lt;- do.call( paste , dhs_df[ , c( &#39;sdist&#39; , &#39;v025&#39; ) ] ) # as shown at # http://userforum.dhsprogram.com/index.php?t=rview&amp;goto=2154#msg_2154 dhs_design &lt;- svydesign( ~ v021 , strata = ~strata , data = dhs_df , weights = ~weight ) Variable Recoding Add new columns to the data set: dhs_design &lt;- update( dhs_design , one = 1 , total_children_ever_born = v201 , surviving_children = v201 - v206 - v207 , urban_rural = factor( v025 , labels = c( &#39;urban&#39; , &#39;rural&#39; ) ) , ethnicity = factor( v131 , levels = c( 1:8 , 96 ) , labels = c( &quot;Chewa&quot; , &quot;Tumbuka&quot; , &quot;Lomwe&quot; , &quot;Tonga&quot; , &quot;Yao&quot; , &quot;Sena&quot; , &quot;Nkonde&quot; , &quot;Ngoni&quot; , &quot;Other&quot; ) ) , no_formal_education = as.numeric( v149 == 0 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( dhs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ urban_rural , dhs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , dhs_design ) svyby( ~ one , ~ urban_rural , dhs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ surviving_children , dhs_design ) svyby( ~ surviving_children , ~ urban_rural , dhs_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ ethnicity , dhs_design , na.rm = TRUE ) svyby( ~ ethnicity , ~ urban_rural , dhs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ surviving_children , dhs_design ) svyby( ~ surviving_children , ~ urban_rural , dhs_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ ethnicity , dhs_design , na.rm = TRUE ) svyby( ~ ethnicity , ~ urban_rural , dhs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ surviving_children , dhs_design , 0.5 ) svyby( ~ surviving_children , ~ urban_rural , dhs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ surviving_children , denominator = ~ total_children_ever_born , dhs_design ) Subsetting Restrict the survey design to 40-49 year old females only: sub_dhs_design &lt;- subset( dhs_design , v447a %in% 40:49 ) Calculate the mean (average) of this subset: svymean( ~ surviving_children , sub_dhs_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ surviving_children , dhs_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ surviving_children , ~ urban_rural , dhs_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( dhs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ surviving_children , dhs_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ surviving_children , dhs_design , deff = TRUE ) # SRS with replacement svymean( ~ surviving_children , dhs_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ no_formal_education , dhs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( surviving_children ~ no_formal_education , dhs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ no_formal_education + ethnicity , dhs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( surviving_children ~ no_formal_education + ethnicity , dhs_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for DHS users, this code replicates previously-presented examples: library(srvyr) dhs_srvyr_design &lt;- as_survey( dhs_design ) Calculate the mean (average) of a linear variable, overall and by groups: dhs_srvyr_design %&gt;% summarize( mean = survey_mean( surviving_children ) ) dhs_srvyr_design %&gt;% group_by( urban_rural ) %&gt;% summarize( mean = survey_mean( surviving_children ) ) Replication Example "],["exame-nacional-de-desempenho-de-estudantes-enade.html", "Exame Nacional de Desempenho de Estudantes (ENADE) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " Exame Nacional de Desempenho de Estudantes (ENADE) The Exame Nacional de Desempenho de Estudantes (ENADE) evaluates the performance of undergraduate students in relation to the program content, skills and competences acquired in their training. The exam is mandatory and the student’s regularity in the exam must be included in his or her school record. One table with one row per individual undergraduate student in Brazil. An enumeration of undergraduate students in Brazil. Released annually since 2004. Compiled by the Brazilian Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP). Simplified Download and Importation The R lodown package easily downloads and imports all available ENADE microdata by simply specifying \"enade\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;enade&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ENADE&quot; ) ) Analysis Examples with base R   Load a data frame: enade_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ENADE&quot; , &quot;2016 main.rds&quot; ) ) Variable Recoding Add new columns to the data set: enade_df &lt;- transform( enade_df , # qual foi o tempo gasto por voce para concluir a prova? less_than_two_hours = as.numeric( qp_i9 %in% c( &#39;A&#39; , &#39;B&#39; ) ) , state_name = factor( co_uf_curso , levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) , labels = c( &quot;Rondonia&quot; , &quot;Acre&quot; , &quot;Amazonas&quot; , &quot;Roraima&quot; , &quot;Para&quot; , &quot;Amapa&quot; , &quot;Tocantins&quot; , &quot;Maranhao&quot; , &quot;Piaui&quot; , &quot;Ceara&quot; , &quot;Rio Grande do Norte&quot; , &quot;Paraiba&quot; , &quot;Pernambuco&quot; , &quot;Alagoas&quot; , &quot;Sergipe&quot; , &quot;Bahia&quot; , &quot;Minas Gerais&quot; , &quot;Espirito Santo&quot; , &quot;Rio de Janeiro&quot; , &quot;Sao Paulo&quot; , &quot;Parana&quot; , &quot;Santa Catarina&quot; , &quot;Rio Grande do Sul&quot; , &quot;Mato Grosso do Sul&quot; , &quot;Mato Grosso&quot; , &quot;Goias&quot; , &quot;Distrito Federal&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( enade_df ) table( enade_df[ , &quot;tp_sexo&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;tp_sexo&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( enade_df[ , &quot;state_name&quot; ] ) ) prop.table( table( enade_df[ , c( &quot;state_name&quot; , &quot;tp_sexo&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;tp_sexo&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( enade_df[ , &quot;nt_obj_fg&quot; ] , 0.5 , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;tp_sexo&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to Students reporting that the general training section of the test was easy or very easy: sub_enade_df &lt;- subset( enade_df , qp_i1 %in% c( &quot;A&quot; , &quot;B&quot; ) ) Calculate the mean (average) of this subset: mean( sub_enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( enade_df[ , &quot;nt_obj_fg&quot; ] , na.rm = TRUE ) tapply( enade_df[ , &quot;nt_obj_fg&quot; ] , enade_df[ , &quot;tp_sexo&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( nt_obj_fg ~ less_than_two_hours , enade_df ) Perform a chi-squared test of association: this_table &lt;- table( enade_df[ , c( &quot;less_than_two_hours&quot; , &quot;state_name&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( nt_obj_fg ~ less_than_two_hours + state_name , data = enade_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for ENADE users, this code replicates previously-presented examples: library(dplyr) enade_tbl &lt;- tbl_df( enade_df ) Calculate the mean (average) of a linear variable, overall and by groups: enade_tbl %&gt;% summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) ) enade_tbl %&gt;% group_by( tp_sexo ) %&gt;% summarize( mean = mean( nt_obj_fg , na.rm = TRUE ) ) "],["exame-nacional-do-ensino-medio-enem.html", "Exame Nacional do Ensino Medio (ENEM) Simplified Download and Importation Analysis Examples with SQL and RSQLite   Analysis Examples with dplyr   Replication Example", " Exame Nacional do Ensino Medio (ENEM) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; The Exame Nacional do Ensino Medio (ENEM) contains the standardized test results of most Brazilian high school students. An annual table with one row per student. Updated annually since 1998. Maintained by the Brazil’s Instituto Nacional de Estudos e Pesquisas Educacionais Anisio Teixeira (INEP) Simplified Download and Importation The R lodown package easily downloads and imports all available ENEM microdata by simply specifying \"enem\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;enem&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ENEM&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the ENEM catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available ENEM microdata files enem_cat &lt;- get_catalog( &quot;enem&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ENEM&quot; ) ) # 2015 only enem_cat &lt;- subset( enem_cat , year == 2015 ) # download the microdata to your local computer enem_cat &lt;- lodown( &quot;enem&quot; , enem_cat ) Analysis Examples with SQL and RSQLite   Connect to a database: library(DBI) dbdir &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;ENEM&quot; , &quot;SQLite.db&quot; ) db &lt;- dbConnect( RSQLite::SQLite() , dbdir ) Variable Recoding Add new columns to the data set: dbSendQuery( db , &quot;ALTER TABLE microdados_enem_2015 ADD COLUMN female INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE microdados_enem_2015 SET female = CASE WHEN tp_sexo = 2 THEN 1 ELSE 0 END&quot; ) dbSendQuery( db , &quot;ALTER TABLE microdados_enem_2015 ADD COLUMN fathers_education INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE microdados_enem_2015 SET fathers_education = CASE WHEN q001 = 1 THEN &#39;01 - nao estudou&#39; WHEN q001 = 2 THEN &#39;02 - 1 a 4 serie&#39; WHEN q001 = 3 THEN &#39;03 - 5 a 8 serie&#39; WHEN q001 = 4 THEN &#39;04 - ensino medio incompleto&#39; WHEN q001 = 5 THEN &#39;05 - ensino medio&#39; WHEN q001 = 6 THEN &#39;06 - ensino superior incompleto&#39; WHEN q001 = 7 THEN &#39;07 - ensino superior&#39; WHEN q001 = 8 THEN &#39;08 - pos-graduacao&#39; WHEN q001 = 9 THEN &#39;09 - nao estudou&#39; ELSE NULL END&quot; ) Unweighted Counts Count the unweighted number of records in the SQL table, overall and by groups: dbGetQuery( db , &quot;SELECT COUNT(*) FROM microdados_enem_2015&quot; ) dbGetQuery( db , &quot;SELECT fathers_education , COUNT(*) FROM microdados_enem_2015 GROUP BY fathers_education&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT AVG( nota_mt ) FROM microdados_enem_2015&quot; ) dbGetQuery( db , &quot;SELECT fathers_education , AVG( nota_mt ) AS mean_nota_mt FROM microdados_enem_2015 GROUP BY fathers_education&quot; ) Calculate the distribution of a categorical variable: dbGetQuery( db , &quot;SELECT uf_residencia , COUNT(*) / ( SELECT COUNT(*) FROM microdados_enem_2015 ) AS share_uf_residencia FROM microdados_enem_2015 GROUP BY uf_residencia&quot; ) Calculate the sum of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT SUM( nota_mt ) FROM microdados_enem_2015&quot; ) dbGetQuery( db , &quot;SELECT fathers_education , SUM( nota_mt ) AS sum_nota_mt FROM microdados_enem_2015 GROUP BY fathers_education&quot; ) Calculate the 25th, median, and 75th percentiles of a linear variable, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT LOWER_QUARTILE( nota_mt ) , MEDIAN( nota_mt ) , UPPER_QUARTILE( nota_mt ) FROM microdados_enem_2015&quot; ) dbGetQuery( db , &quot;SELECT fathers_education , LOWER_QUARTILE( nota_mt ) AS lower_quartile_nota_mt , MEDIAN( nota_mt ) AS median_nota_mt , UPPER_QUARTILE( nota_mt ) AS upper_quartile_nota_mt FROM microdados_enem_2015 GROUP BY fathers_education&quot; ) Subsetting Limit your SQL analysis to took mathematics exam with WHERE: dbGetQuery( db , &quot;SELECT AVG( nota_mt ) FROM microdados_enem_2015 WHERE in_presenca_mt = 1&quot; ) Measures of Uncertainty Calculate the variance and standard deviation, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT VARIANCE( nota_mt ) , STDEV( nota_mt ) FROM microdados_enem_2015&quot; ) dbGetQuery( db , &quot;SELECT fathers_education , VARIANCE( nota_mt ) AS var_nota_mt , STDEV( nota_mt ) AS stddev_nota_mt FROM microdados_enem_2015 GROUP BY fathers_education&quot; ) Regression Models and Tests of Association Perform a t-test: enem_slim_df &lt;- dbGetQuery( db , &quot;SELECT nota_mt , female , uf_residencia FROM microdados_enem_2015&quot; ) t.test( nota_mt ~ female , enem_slim_df ) Perform a chi-squared test of association: this_table &lt;- table( enem_slim_df[ , c( &quot;female&quot; , &quot;uf_residencia&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( nota_mt ~ female + uf_residencia , data = enem_slim_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for ENEM users, this code replicates previously-presented examples: library(dplyr) library(dbplyr) dplyr_db &lt;- dplyr::src_sqlite( dbdir ) enem_tbl &lt;- tbl( dplyr_db , &#39;microdados_enem_2015&#39; ) Calculate the mean (average) of a linear variable, overall and by groups: enem_tbl %&gt;% summarize( mean = mean( nota_mt ) ) enem_tbl %&gt;% group_by( fathers_education ) %&gt;% summarize( mean = mean( nota_mt ) ) Replication Example dbGetQuery( db , &quot;SELECT COUNT(*) FROM microdados_enem_2015&quot; ) "],["european-social-survey-ess.html", "European Social Survey (ESS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " European Social Survey (ESS) Contributed by Dr. Daniel Oberski &lt;daniel.oberski@gmail.com&gt; The European Social Survey measures political opinion and behavior across the continent. One table per country with one row per sampled respondent. A complex sample survey designed to generalize to residents aged 15 and older in participating nations. Released biennially since 2002. Headquartered at City, University of London and governed by a scientific team across Europe. Simplified Download and Importation The R lodown package easily downloads and imports all available ESS microdata by simply specifying \"ess\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;ess&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; ) , your_email = &quot;email@address.com&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the ESS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available ESS microdata files ess_cat &lt;- get_catalog( &quot;ess&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; ) , your_email = &quot;email@address.com&quot; ) # 2014 only ess_cat &lt;- subset( ess_cat , year == 2014 ) # download the microdata to your local computer ess_cat &lt;- lodown( &quot;ess&quot; , ess_cat , your_email = &quot;email@address.com&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) ess_be_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2014/ESS7BE.rds&quot; ) ) ess_sddf_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;ESS&quot; , &quot;2014/ESS7SDDFe01_1.rds&quot; ) ) ess_df &lt;- merge( ess_be_df , ess_sddf_df , by = c( &#39;cntry&#39; , &#39;idno&#39; ) ) stopifnot( nrow( ess_df ) == nrow( ess_be_df ) ) ess_design &lt;- svydesign( ids = ~psu , strata = ~stratify , probs = ~prob , data = ess_df ) Variable Recoding Add new columns to the data set: ess_design &lt;- update( ess_design , one = 1 , non_european_immigrants = factor( impcntr , labels = c( &#39;Allow many to come and live here&#39; , &#39;Allow some&#39; , &#39;Allow a few&#39; , &#39;Allow none&#39; ) ) , sex = factor( icgndra , labels = c( &#39;male&#39; , &#39;female&#39; ) ) , more_than_one_hour_tv_daily = as.numeric( tvtot &gt;= 3 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( ess_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ non_european_immigrants , ess_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , ess_design ) svyby( ~ one , ~ non_european_immigrants , ess_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ ppltrst , ess_design ) svyby( ~ ppltrst , ~ non_european_immigrants , ess_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , ess_design , na.rm = TRUE ) svyby( ~ sex , ~ non_european_immigrants , ess_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ ppltrst , ess_design ) svyby( ~ ppltrst , ~ non_european_immigrants , ess_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , ess_design , na.rm = TRUE ) svyby( ~ sex , ~ non_european_immigrants , ess_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ ppltrst , ess_design , 0.5 ) svyby( ~ ppltrst , ~ non_european_immigrants , ess_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ ppltrst , denominator = ~ pplfair , ess_design ) Subsetting Restrict the survey design to voters: sub_ess_design &lt;- subset( ess_design , vote == 1 ) Calculate the mean (average) of this subset: svymean( ~ ppltrst , sub_ess_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ ppltrst , ess_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ ppltrst , ~ non_european_immigrants , ess_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ess_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ ppltrst , ess_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ ppltrst , ess_design , deff = TRUE ) # SRS with replacement svymean( ~ ppltrst , ess_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ more_than_one_hour_tv_daily , ess_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( ppltrst ~ more_than_one_hour_tv_daily , ess_design ) Perform a chi-squared test of association for survey data: svychisq( ~ more_than_one_hour_tv_daily + sex , ess_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( ppltrst ~ more_than_one_hour_tv_daily + sex , ess_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for ESS users, this code replicates previously-presented examples: library(srvyr) ess_srvyr_design &lt;- as_survey( ess_design ) Calculate the mean (average) of a linear variable, overall and by groups: ess_srvyr_design %&gt;% summarize( mean = survey_mean( ppltrst ) ) ess_srvyr_design %&gt;% group_by( non_european_immigrants ) %&gt;% summarize( mean = survey_mean( ppltrst ) ) Replication Example "],["fda-adverse-event-reporting-system-faers.html", "FDA Adverse Event Reporting System (FAERS) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " FDA Adverse Event Reporting System (FAERS) The FDA Adverse Event Reporting System (FAERS) compiles all prescription drug-related side-effects reported by either physicians or patients in the United States. Either party can make a (voluntary) submission to the FDA or the manufacturer (who then must report that event). This is the post-marketing safety surveillance program for drug and therapeutic biological products. Multiple tables linkable by the primaryid field with patient demographics, drug/biologic information, patient outcomes, reporting source, drug start and end dates. Published quarterly with the latest events reported to the FDA since 2004, with a revised system beginning in the fourth quarter of 2012. Maintained by the United States Food and Drug Administration (FDA). Simplified Download and Importation The R lodown package easily downloads and imports all available FAERS microdata by simply specifying \"faers\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;faers&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;FAERS&quot; ) ) Analysis Examples with base R   Load a data frame: faers_drug_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;FAERS&quot; , &quot;2016 q4/drug16q4.rds&quot; ) ) faers_outcome_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;FAERS&quot; , &quot;2016 q4/outc16q4.rds&quot; ) ) faers_demo_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;FAERS&quot; , &quot;2016 q4/demo16q4.rds&quot; ) ) faers_df &lt;- merge( faers_drug_df , faers_outcome_df ) faers_df &lt;- merge( faers_df , faers_demo_df , all.x = TRUE ) Variable Recoding Add new columns to the data set: faers_df &lt;- transform( faers_df , physician_reported = as.numeric( occp_cod == &quot;MD&quot; ) , init_fda_year = as.numeric( substr( init_fda_dt , 1 , 4 ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( faers_df ) table( faers_df[ , &quot;outc_code&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;outc_code&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( faers_df[ , &quot;sex&quot; ] ) ) prop.table( table( faers_df[ , c( &quot;sex&quot; , &quot;outc_code&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;outc_code&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( faers_df[ , &quot;init_fda_year&quot; ] , 0.5 , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;outc_code&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to elderly persons: sub_faers_df &lt;- subset( faers_df , age_grp == &quot;E&quot; ) Calculate the mean (average) of this subset: mean( sub_faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( faers_df[ , &quot;init_fda_year&quot; ] , na.rm = TRUE ) tapply( faers_df[ , &quot;init_fda_year&quot; ] , faers_df[ , &quot;outc_code&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( init_fda_year ~ physician_reported , faers_df ) Perform a chi-squared test of association: this_table &lt;- table( faers_df[ , c( &quot;physician_reported&quot; , &quot;sex&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( init_fda_year ~ physician_reported + sex , data = faers_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for FAERS users, this code replicates previously-presented examples: library(dplyr) faers_tbl &lt;- tbl_df( faers_df ) Calculate the mean (average) of a linear variable, overall and by groups: faers_tbl %&gt;% summarize( mean = mean( init_fda_year , na.rm = TRUE ) ) faers_tbl %&gt;% group_by( outc_code ) %&gt;% summarize( mean = mean( init_fda_year , na.rm = TRUE ) ) "],["general-social-survey-gss.html", "General Social Survey (GSS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr  ", " General Social Survey (GSS) The General Social Survey (GSS) has captured political beliefs and social attitudes since 1972. In contrast to non-trendable tracking polls that capture newspaper headlines, the GSS has sustained a set of questions over four decades. One table with one row per sampled respondent. A complex sample survey designed to generalize to the non-institutional population of adults (18+) in the United States. Updated biennially since 1972. Funded by the National Science Foundation and administered by the National Opinion Research Center. Simplified Download and Importation The R lodown package easily downloads and imports all available GSS microdata by simply specifying \"gss\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;gss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;GSS&quot; ) ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) gss_files &lt;- list.files( file.path( path.expand( &quot;~&quot; ) , &quot;GSS&quot; ) , full.names = TRUE ) gss_rds &lt;- grep( &quot;cross sectional cumulative(.*)([0-9][0-9][0-9][0-9])\\\\.rds$&quot; , gss_files , value = TRUE ) gss_df &lt;- readRDS( gss_rds ) # keep only the variables you need keep_vars &lt;- c( &quot;vpsu&quot; , &quot;vstrat&quot; , &quot;compwt&quot; , &quot;polviews&quot; , &quot;born&quot; , &quot;adults&quot; , &quot;hompop&quot; , &quot;race&quot; , &quot;region&quot; , &quot;age&quot; , &quot;sex&quot; , &quot;one&quot; ) gss_df &lt;- gss_df[ keep_vars ] ; gc() # this step conserves RAM # https://gssdataexplorer.norc.org/pages/show?page=gss%2Fstandard_error gss_design &lt;- svydesign( ~ vpsu , strata = ~ vstrat , data = gss_df , weights = ~ wtssall , nest = TRUE ) Variable Recoding Add new columns to the data set: gss_design &lt;- update( gss_design , polviews = factor( polviews , labels = c( &quot;Extremely liberal&quot; , &quot;Liberal&quot; , &quot;Slightly liberal&quot; , &quot;Moderate, middle of the road&quot; , &quot;Slightly conservative&quot; , &quot;Conservative&quot; , &quot;Extremely conservative&quot; ) ) , born_in_usa = ifelse( born %in% 1:2 , as.numeric( born == 1 ) , NA ) , adults_in_hh = ifelse( adults &gt; 8 , NA , adults ) , persons_in_hh = ifelse( hompop &gt; 11 , NA , hompop ) , race = factor( race , labels = c( &quot;white&quot; , &quot;black&quot; , &quot;other&quot; ) ) , region = factor( region , labels = c( &quot;New England&quot; , &quot;Middle Atlantic&quot; , &quot;East North Central&quot; , &quot;West North Central&quot; , &quot;South Atlantic&quot; , &quot;East South Central&quot; , &quot;West South Central&quot; , &quot;Mountain&quot; , &quot;Pacific&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( gss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region , gss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , gss_design ) svyby( ~ one , ~ region , gss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ age , gss_design , na.rm = TRUE ) svyby( ~ age , ~ region , gss_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ race , gss_design , na.rm = TRUE ) svyby( ~ race , ~ region , gss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ age , gss_design , na.rm = TRUE ) svyby( ~ age , ~ region , gss_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ race , gss_design , na.rm = TRUE ) svyby( ~ race , ~ region , gss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ age , gss_design , 0.5 , na.rm = TRUE ) svyby( ~ age , ~ region , gss_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ adults_in_hh , denominator = ~ persons_in_hh , gss_design , na.rm = TRUE ) Subsetting Restrict the survey design to females: sub_gss_design &lt;- subset( gss_design , sex == 2 ) Calculate the mean (average) of this subset: svymean( ~ age , sub_gss_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ age , gss_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ age , ~ region , gss_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( gss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ age , gss_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ age , gss_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ age , gss_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ born_in_usa , gss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( age ~ born_in_usa , gss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ born_in_usa + race , gss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( age ~ born_in_usa + race , gss_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for GSS users, this code replicates previously-presented examples: library(srvyr) gss_srvyr_design &lt;- as_survey( gss_design ) Calculate the mean (average) of a linear variable, overall and by groups: gss_srvyr_design %&gt;% summarize( mean = survey_mean( age , na.rm = TRUE ) ) gss_srvyr_design %&gt;% group_by( region ) %&gt;% summarize( mean = survey_mean( age , na.rm = TRUE ) ) "],["home-mortgage-disclosure-act-hmda.html", "Home Mortgage Disclosure Act (HMDA) Simplified Download and Importation Analysis Examples with SQL and RSQLite   Analysis Examples with dplyr   Replication Example", " Home Mortgage Disclosure Act (HMDA) Contributed by Max Weselcouch &lt;mweselco@gmail.com&gt; Responding to discriminatory lending practices, the United States Congress mandated that financial organizations originating home mortgages report some basic operational statistics. The Home Mortgage Disclosure Act (HMDA) increased the transparency of home-lending activity across the country. A loan application record (LAR) table with one record per public loan application, with secondary tables containing both private loan applications (PMIC) and one record per institution tables (INS). A public compilation of more than ninety percent of all Federal Housing Authority (FHA) loans in the United States. Updated every September with a new year of microdata. Data prior to 2006 require a special order from the United States National Archives. Maintained by the United States Federal Financial Institutions Examination Council (FEIEC) Simplified Download and Importation The R lodown package easily downloads and imports all available HMDA microdata by simply specifying \"hmda\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;hmda&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;HMDA&quot; ) ) Analysis Examples with SQL and RSQLite   Connect to a database: library(DBI) dbdir &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;HMDA&quot; , &quot;SQLite.db&quot; ) db &lt;- dbConnect( RSQLite::SQLite() , dbdir ) Variable Recoding Add new columns to the data set: dbSendQuery( db , &quot;ALTER TABLE hmda_2015 ADD COLUMN multifamily_home INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE hmda_2015 SET multifamily_home = CASE WHEN ( propertytype = 3 ) THEN 1 ELSE 0 END&quot; ) Unweighted Counts Count the unweighted number of records in the SQL table, overall and by groups: dbGetQuery( db , &quot;SELECT COUNT(*) FROM hmda_2015&quot; ) dbGetQuery( db , &quot;SELECT loanpurpose , COUNT(*) FROM hmda_2015 GROUP BY loanpurpose&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT AVG( loanamount ) FROM hmda_2015&quot; ) dbGetQuery( db , &quot;SELECT loanpurpose , AVG( loanamount ) AS mean_loanamount FROM hmda_2015 GROUP BY loanpurpose&quot; ) Calculate the distribution of a categorical variable: dbGetQuery( db , &quot;SELECT actiontype , COUNT(*) / ( SELECT COUNT(*) FROM hmda_2015 ) AS share_actiontype FROM hmda_2015 GROUP BY actiontype&quot; ) Calculate the sum of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT SUM( loanamount ) FROM hmda_2015&quot; ) dbGetQuery( db , &quot;SELECT loanpurpose , SUM( loanamount ) AS sum_loanamount FROM hmda_2015 GROUP BY loanpurpose&quot; ) Calculate the 25th, median, and 75th percentiles of a linear variable, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT LOWER_QUARTILE( loanamount ) , MEDIAN( loanamount ) , UPPER_QUARTILE( loanamount ) FROM hmda_2015&quot; ) dbGetQuery( db , &quot;SELECT loanpurpose , LOWER_QUARTILE( loanamount ) AS lower_quartile_loanamount , MEDIAN( loanamount ) AS median_loanamount , UPPER_QUARTILE( loanamount ) AS upper_quartile_loanamount FROM hmda_2015 GROUP BY loanpurpose&quot; ) Subsetting Limit your SQL analysis to non-Hispanic White persons with WHERE: dbGetQuery( db , &quot;SELECT AVG( loanamount ) FROM hmda_2015 WHERE race = 5 AND ethnicity = 2&quot; ) Measures of Uncertainty Calculate the variance and standard deviation, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT VARIANCE( loanamount ) , STDEV( loanamount ) FROM hmda_2015&quot; ) dbGetQuery( db , &quot;SELECT loanpurpose , VARIANCE( loanamount ) AS var_loanamount , STDEV( loanamount ) AS stddev_loanamount FROM hmda_2015 GROUP BY loanpurpose&quot; ) Regression Models and Tests of Association Perform a t-test: hmda_slim_df &lt;- dbGetQuery( db , &quot;SELECT loanamount , multifamily_home , actiontype FROM hmda_2015&quot; ) t.test( loanamount ~ multifamily_home , hmda_slim_df ) Perform a chi-squared test of association: this_table &lt;- table( hmda_slim_df[ , c( &quot;multifamily_home&quot; , &quot;actiontype&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( loanamount ~ multifamily_home + actiontype , data = hmda_slim_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for HMDA users, this code replicates previously-presented examples: library(dplyr) library(dbplyr) dplyr_db &lt;- dplyr::src_sqlite( dbdir ) hmda_tbl &lt;- tbl( dplyr_db , &#39;hmda_2015&#39; ) Calculate the mean (average) of a linear variable, overall and by groups: hmda_tbl %&gt;% summarize( mean = mean( loanamount ) ) hmda_tbl %&gt;% group_by( loanpurpose ) %&gt;% summarize( mean = mean( loanamount ) ) Replication Example dbGetQuery( db , &quot;SELECT COUNT(*) FROM hmda_2015&quot; ) "],["health-and-retirement-study-hrs.html", "Health and Retirement Study (HRS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Health and Retirement Study (HRS) The Health and Retirement Study interviews Americans aged 50+ for their entire life. Allows for findings like, “Among Americans who were 50-74 years old in 1998, X% lived in nursing homes by 2010.” Many tables, most with one row per sampled respondent and linkable over time. Use the RAND HRS data file for a cleaner, cross-wave data set. A complex sample survey designed to generalize to Americans aged 50+ at each interview, but longitudinal analysts can observe outcomes. Released biennially since 1992. Administered by the University of Michigan’s Institute for Social Research with data management by the RAND Corporation. Funded by the National Institute on Aging and the Social Security Administration. Simplified Download and Importation The R lodown package easily downloads and imports all available HRS microdata by simply specifying \"hrs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;hrs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;HRS&quot; ) , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the HRS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available HRS microdata files hrs_cat &lt;- get_catalog( &quot;hrs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;HRS&quot; ) , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) # RAND consolidated file only hrs_cat &lt;- subset( hrs_cat , grepl( &#39;rand([a-z]+)stata\\\\.zip&#39; , file_name ) ) # download the microdata to your local computer hrs_cat &lt;- lodown( &quot;hrs&quot; , hrs_cat , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) hrs_df &lt;- readRDS( grep( &#39;rand([a-z]+)stata(.*)rds$&#39; , list.files( hrs_cat$output_folder , full.names = TRUE , recursive = TRUE ) , value = TRUE ) ) # RAM cleanup keep_vars &lt;- c( &quot;raehsamp&quot; , &quot;raestrat&quot; , &quot;r3wtresp&quot; , &quot;r3work&quot; , &quot;r12work&quot; , &quot;h12ahous&quot; , &quot;r3mstat&quot; , &quot;r12mstat&quot; , &quot;h4ahous&quot; ) hrs_df &lt;- hrs_df[ keep_vars ] # community residents aged 50+ in 1996 hrs_design &lt;- svydesign( id = ~ raehsamp , strata = ~ raestrat , weights = ~ r3wtresp , nest = TRUE , data = subset( hrs_df , r3wtresp &gt; 0 ) ) Variable Recoding Add new columns to the data set: hrs_design &lt;- update( hrs_design , one = 1 , working_in_1996 = r3work , working_in_2014 = r12work , marital_stat_1996 = factor( r3mstat , levels = 1:8 , labels = c( &quot;Married&quot; , &quot;Married, spouse absent&quot; , &quot;Partnered&quot; , &quot;Separated&quot; , &quot;Divorced&quot; , &quot;Separated/divorced&quot; , &quot;Widowed&quot; , &quot;Never married&quot; ) ) , marital_stat_2014 = factor( r12mstat , levels = 1:8 , labels = c( &quot;Married&quot; , &quot;Married, spouse absent&quot; , &quot;Partnered&quot; , &quot;Separated&quot; , &quot;Divorced&quot; , &quot;Separated/divorced&quot; , &quot;Widowed&quot; , &quot;Never married&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( hrs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ marital_stat_1996 , hrs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , hrs_design ) svyby( ~ one , ~ marital_stat_1996 , hrs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ h12ahous , hrs_design , na.rm = TRUE ) svyby( ~ h12ahous , ~ marital_stat_1996 , hrs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ marital_stat_2014 , hrs_design , na.rm = TRUE ) svyby( ~ marital_stat_2014 , ~ marital_stat_1996 , hrs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ h12ahous , hrs_design , na.rm = TRUE ) svyby( ~ h12ahous , ~ marital_stat_1996 , hrs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ marital_stat_2014 , hrs_design , na.rm = TRUE ) svyby( ~ marital_stat_2014 , ~ marital_stat_1996 , hrs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ h12ahous , hrs_design , 0.5 , na.rm = TRUE ) svyby( ~ h12ahous , ~ marital_stat_1996 , hrs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ h4ahous , denominator = ~ h12ahous , hrs_design , na.rm = TRUE ) Subsetting Restrict the survey design to : sub_hrs_design &lt;- subset( hrs_design , working_in_1996 == 1 ) Calculate the mean (average) of this subset: svymean( ~ h12ahous , sub_hrs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ h12ahous , hrs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ h12ahous , ~ marital_stat_1996 , hrs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( hrs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ h12ahous , hrs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ h12ahous , hrs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ h12ahous , hrs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ working_in_2014 , hrs_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( h12ahous ~ working_in_2014 , hrs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ working_in_2014 + marital_stat_2014 , hrs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( h12ahous ~ working_in_2014 + marital_stat_2014 , hrs_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for HRS users, this code replicates previously-presented examples: library(srvyr) hrs_srvyr_design &lt;- as_survey( hrs_design ) Calculate the mean (average) of a linear variable, overall and by groups: hrs_srvyr_design %&gt;% summarize( mean = survey_mean( h12ahous , na.rm = TRUE ) ) hrs_srvyr_design %&gt;% group_by( marital_stat_1996 ) %&gt;% summarize( mean = survey_mean( h12ahous , na.rm = TRUE ) ) Replication Example "],["medical-expenditure-panel-survey-meps.html", "Medical Expenditure Panel Survey (MEPS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Medical Expenditure Panel Survey (MEPS) The Medical Expenditure Panel Survey’s Household Component (MEPS-HC) captures person-level medical expenditures by payor and type of service with more detail than any other publicly-available data set. The annual consolidated file contains one row per individual within each sampled household. Other available mergeable tables contain one record per medical event, one record per job, one record per insurance held. A complex sample survey designed to generalize to the civilian non-institutionalized population of the United States. Released annually since 1996. Administered by the Agency for Healthcare Research and Quality. Simplified Download and Importation The R lodown package easily downloads and imports all available MEPS microdata by simply specifying \"meps\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;meps&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;MEPS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the MEPS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available MEPS microdata files meps_cat &lt;- get_catalog( &quot;meps&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;MEPS&quot; ) ) # 2015 only meps_cat &lt;- subset( meps_cat , year == 2015 ) # download the microdata to your local computer meps_cat &lt;- lodown( &quot;meps&quot; , meps_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) meps_cons_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;MEPS&quot; , &quot;2015/full year consolidated.rds&quot; ) ) meps_brr &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;MEPS&quot; , &quot;meps 1996-2015 replicates for variance estimation.rds&quot; ) ) meps_brr &lt;- meps_brr[ , c( &quot;dupersid&quot; , &quot;panel&quot; , names( meps_brr )[ !( names( meps_brr ) %in% names( meps_cons_df ) ) ] ) ] meps_df &lt;- merge( meps_cons_df , meps_brr ) stopifnot( nrow( meps_df ) == nrow( meps_cons_df ) ) meps_design &lt;- svrepdesign( data = meps_df , weights = ~ perwt15f , type = &quot;BRR&quot; , combined.weights = FALSE , repweights = &quot;brr[1-9]+&quot; , mse = TRUE ) Variable Recoding Add new columns to the data set: meps_design &lt;- update( meps_design , one = 1 , insured_december_31st = ifelse( ins15x %in% 1:2 , as.numeric( ins15x == 1 ) , NA ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( meps_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region15 , meps_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , meps_design ) svyby( ~ one , ~ region15 , meps_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ totexp15 , meps_design ) svyby( ~ totexp15 , ~ region15 , meps_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , meps_design ) svyby( ~ sex , ~ region15 , meps_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ totexp15 , meps_design ) svyby( ~ totexp15 , ~ region15 , meps_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , meps_design ) svyby( ~ sex , ~ region15 , meps_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ totexp15 , meps_design , 0.5 ) svyby( ~ totexp15 , ~ region15 , meps_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ totmcd15 , denominator = ~ totexp15 , meps_design ) Subsetting Restrict the survey design to seniors: sub_meps_design &lt;- subset( meps_design , agelast &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ totexp15 , sub_meps_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ totexp15 , meps_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ totexp15 , ~ region15 , meps_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( meps_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ totexp15 , meps_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ totexp15 , meps_design , deff = TRUE ) # SRS with replacement svymean( ~ totexp15 , meps_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ insured_december_31st , meps_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( totexp15 ~ insured_december_31st , meps_design ) Perform a chi-squared test of association for survey data: svychisq( ~ insured_december_31st + sex , meps_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( totexp15 ~ insured_december_31st + sex , meps_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for MEPS users, this code replicates previously-presented examples: library(srvyr) meps_srvyr_design &lt;- as_survey( meps_design ) Calculate the mean (average) of a linear variable, overall and by groups: meps_srvyr_design %&gt;% summarize( mean = survey_mean( totexp15 ) ) meps_srvyr_design %&gt;% group_by( region15 ) %&gt;% summarize( mean = survey_mean( totexp15 ) ) Replication Example "],["medical-large-claims-experience-study-mlces.html", "Medical Large Claims Experience Study (MLCES) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " Medical Large Claims Experience Study (MLCES) The Medical Large Claims Experience Study (MLCES) might be the best private health insurance claims data available to the public. This data should be used to calibrate other data sets, and probably nothing more. One table with one row per individual with nonzero total paid charges. A convenience sample of group (employer-sponsored) health insurance claims from seven private health insurers in the United States. 1997 thru 1999 with no expected updates in the future. Provided by the Society of Actuaries (SOA). Simplified Download and Importation The R lodown package easily downloads and imports all available MLCES microdata by simply specifying \"mlces\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;mlces&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;MLCES&quot; ) ) Analysis Examples with base R   Load a data frame: mlces_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;MLCES&quot; , &quot;mlces1997.rds&quot; ) ) Variable Recoding Add new columns to the data set: mlces_df &lt;- transform( mlces_df , claimant_relationship_to_policyholder = ifelse( relation == &quot;E&quot; , &quot;covered employee&quot; , ifelse( relation == &quot;S&quot; , &quot;spouse of covered employee&quot; , ifelse( relation == &quot;D&quot; , &quot;dependent of covered employee&quot; , NA ) ) ) , ppo_plan = as.numeric( ppo == &#39;Y&#39; ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( mlces_df ) table( mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( mlces_df[ , &quot;patsex&quot; ] ) ) prop.table( table( mlces_df[ , c( &quot;patsex&quot; , &quot;claimant_relationship_to_policyholder&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( mlces_df[ , &quot;totpdchg&quot; ] , 0.5 ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to persons under 18: sub_mlces_df &lt;- subset( mlces_df , ( ( claimyr - patbrtyr ) &lt; 18 ) ) Calculate the mean (average) of this subset: mean( sub_mlces_df[ , &quot;totpdchg&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( mlces_df[ , &quot;totpdchg&quot; ] ) tapply( mlces_df[ , &quot;totpdchg&quot; ] , mlces_df[ , &quot;claimant_relationship_to_policyholder&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( totpdchg ~ ppo_plan , mlces_df ) Perform a chi-squared test of association: this_table &lt;- table( mlces_df[ , c( &quot;ppo_plan&quot; , &quot;patsex&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( totpdchg ~ ppo_plan + patsex , data = mlces_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for MLCES users, this code replicates previously-presented examples: library(dplyr) mlces_tbl &lt;- tbl_df( mlces_df ) Calculate the mean (average) of a linear variable, overall and by groups: mlces_tbl %&gt;% summarize( mean = mean( totpdchg ) ) mlces_tbl %&gt;% group_by( claimant_relationship_to_policyholder ) %&gt;% summarize( mean = mean( totpdchg ) ) "],["national-beneficiary-survey-nbs.html", "National Beneficiary Survey (NBS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " National Beneficiary Survey (NBS) The National Beneficiary Survey (NBS) is the principal microdata for disability researchers in the United States interested in Social Security program performance. One table with one row per sampled youth respondent. A complex sample survey designed to generalize to Americans covered by either Social Security Disability Insurance (SSDI) or Supplemental Security Income (SSI). Note that the public use files do not include individuals sampled for ticket-to-work (TTW) programs. Released at irregular intervals, with 2004, 2005, 2006, and 2010 available and 2015, 2017, and 2019 forthcoming. Administered by the Social Security Administration. Simplified Download and Importation The R lodown package easily downloads and imports all available NBS microdata by simply specifying \"nbs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nbs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NBS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NBS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NBS microdata files nbs_cat &lt;- get_catalog( &quot;nbs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NBS&quot; ) ) # 2010 only nbs_cat &lt;- subset( nbs_cat , this_round == 4 ) # download the microdata to your local computer nbs_cat &lt;- lodown( &quot;nbs&quot; , nbs_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) nbs_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NBS&quot; , &quot;round 04.rds&quot; ) ) nbs_design &lt;- svydesign( ~ a_psu_pub , strata = ~ a_strata , data = nbs_df , weights = ~ wtr4_ben ) Variable Recoding Add new columns to the data set: nbs_design &lt;- update( nbs_design , male = as.numeric( orgsampinfo_sex == 1 ) , age_categories = factor( c_intage_pub , labels = c( &quot;18-25&quot; , &quot;26-40&quot; , &quot;41-55&quot; , &quot;56 and older&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nbs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_categories , nbs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nbs_design ) svyby( ~ one , ~ age_categories , nbs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ n_totssbenlastmnth_pub , nbs_design ) svyby( ~ n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svymean ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ c_hhsize_pub , nbs_design ) svyby( ~ c_hhsize_pub , ~ age_categories , nbs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ n_totssbenlastmnth_pub , nbs_design ) svyby( ~ n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svytotal ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ c_hhsize_pub , nbs_design ) svyby( ~ c_hhsize_pub , ~ age_categories , nbs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ n_totssbenlastmnth_pub , nbs_design , 0.5 ) svyby( ~ n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE ) Estimate a ratio: svyratio( numerator = ~ n_ssilastmnth_pub , denominator = ~ n_totssbenlastmnth_pub , nbs_design ) Subsetting Restrict the survey design to currently covered by Medicare: sub_nbs_design &lt;- subset( nbs_design , c_curmedicare == 1 ) Calculate the mean (average) of this subset: svymean( ~ n_totssbenlastmnth_pub , sub_nbs_design ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ n_totssbenlastmnth_pub , nbs_design ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ n_totssbenlastmnth_pub , ~ age_categories , nbs_design , svymean ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nbs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ n_totssbenlastmnth_pub , nbs_design ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ n_totssbenlastmnth_pub , nbs_design , deff = TRUE ) # SRS with replacement svymean( ~ n_totssbenlastmnth_pub , nbs_design , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , nbs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( n_totssbenlastmnth_pub ~ male , nbs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + c_hhsize_pub , nbs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( n_totssbenlastmnth_pub ~ male + c_hhsize_pub , nbs_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NBS users, this code replicates previously-presented examples: library(srvyr) nbs_srvyr_design &lt;- as_survey( nbs_design ) Calculate the mean (average) of a linear variable, overall and by groups: nbs_srvyr_design %&gt;% summarize( mean = survey_mean( n_totssbenlastmnth_pub ) ) nbs_srvyr_design %&gt;% group_by( age_categories ) %&gt;% summarize( mean = survey_mean( n_totssbenlastmnth_pub ) ) Replication Example "],["youth-risk-behavior-surveillance-system-ncvs.html", "Youth Risk Behavior Surveillance System (NCVS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Youth Risk Behavior Surveillance System (NCVS) The Youth Risk Behavior Surveillance System is the high school edition of the Behavioral Risk Factor Surveillance System (BRFSS), a scientific study of good kids who do bad things. One table with one row per sampled youth respondent. A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States. Released biennially since 1993. Administered by the Centers for Disease Control and Prevention. Simplified Download and Importation The R lodown package easily downloads and imports all available NCVS microdata by simply specifying \"ncvs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;ncvs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NCVS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NCVS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NCVS microdata files ncvs_cat &lt;- get_catalog( &quot;ncvs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NCVS&quot; ) ) # 2015 only ncvs_cat &lt;- subset( ncvs_cat , year == 2015 ) # download the microdata to your local computer ncvs_cat &lt;- lodown( &quot;ncvs&quot; , ncvs_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) ncvs_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NCVS&quot; , &quot;2015 main.rds&quot; ) ) ncvs_design &lt;- svydesign( ~ psu , strata = ~ stratum , data = ncvs_df , weights = ~ weight , nest = TRUE ) Variable Recoding Add new columns to the data set: ncvs_design &lt;- update( ncvs_design , q2 = q2 , never_rarely_wore_bike_helmet = as.numeric( qn8 == 1 ) , ever_smoked_marijuana = as.numeric( qn47 == 1 ) , ever_tried_to_quit_cigarettes = as.numeric( q36 &gt; 2 ) , smoked_cigarettes_past_year = as.numeric( q36 &gt; 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( ncvs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ ever_smoked_marijuana , ncvs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , ncvs_design ) svyby( ~ one , ~ ever_smoked_marijuana , ncvs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ bmipct , ncvs_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_smoked_marijuana , ncvs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ q2 , ncvs_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_smoked_marijuana , ncvs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ bmipct , ncvs_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_smoked_marijuana , ncvs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ q2 , ncvs_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_smoked_marijuana , ncvs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ bmipct , ncvs_design , 0.5 , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_smoked_marijuana , ncvs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ever_tried_to_quit_cigarettes , denominator = ~ smoked_cigarettes_past_year , ncvs_design , na.rm = TRUE ) Subsetting Restrict the survey design to youths who ever drank alcohol: sub_ncvs_design &lt;- subset( ncvs_design , qn41 == 1 ) Calculate the mean (average) of this subset: svymean( ~ bmipct , sub_ncvs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ bmipct , ncvs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ bmipct , ~ ever_smoked_marijuana , ncvs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( ncvs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ bmipct , ncvs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ bmipct , ncvs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ bmipct , ncvs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ never_rarely_wore_bike_helmet , ncvs_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( bmipct ~ never_rarely_wore_bike_helmet , ncvs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ never_rarely_wore_bike_helmet + q2 , ncvs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( bmipct ~ never_rarely_wore_bike_helmet + q2 , ncvs_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NCVS users, this code replicates previously-presented examples: library(srvyr) ncvs_srvyr_design &lt;- as_survey( ncvs_design ) Calculate the mean (average) of a linear variable, overall and by groups: ncvs_srvyr_design %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) ncvs_srvyr_design %&gt;% group_by( ever_smoked_marijuana ) %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) Replication Example This snippet replicates the “never/rarely wore bicycle helmet” row of PDF page 29 of this CDC analysis software document. unwtd.count( ~ never_rarely_wore_bike_helmet , yrbss_design ) svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_bike_helmet ) ) ) svymean( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE ) svyciprop( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE , method = &quot;beta&quot; ) "],["national-health-and-nutrition-examination-survey-nhanes.html", "National Health and Nutrition Examination Survey (NHANES) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " National Health and Nutrition Examination Survey (NHANES) The National Health and Nutrition Examination Survey (NHANES) is this fascinating survey where doctors and dentists accompany survey interviewers in a little mobile medical center that drives around the country. While the survey methodologists interview people, the medical professionals administer laboratory tests and conduct a thorough physical examination. The blood work and medical exam allow researchers to answer tough questions like, “how many people have diabetes but don’t know they have diabetes?” Many tables containing information gathered from the various examinations, generally with one row per individual respondent. A complex sample survey designed to generalize to the civilian non-institutionalized population of the United States. Released biennially since 1999-2000. Administered by the Centers for Disease Control and Prevention. Simplified Download and Importation The R lodown package easily downloads and imports all available NHANES microdata by simply specifying \"nhanes\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nhanes&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NHANES&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NHANES catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NHANES microdata files nhanes_cat &lt;- get_catalog( &quot;nhanes&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NHANES&quot; ) ) # 2015-2016 only nhanes_cat &lt;- subset( nhanes_cat , years == &quot;2015-2016&quot; ) # download the microdata to your local computer nhanes_cat &lt;- lodown( &quot;nhanes&quot; , nhanes_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nhanes_demo_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NHANES&quot; , &quot;2015-2016/demo_i.rds&quot; ) ) nhanes_tchol_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NHANES&quot; , &quot;2015-2016/tchol_i.rds&quot; ) ) nhanes_df &lt;- merge( nhanes_demo_df , nhanes_tchol_df , all = TRUE ) stopifnot( nrow( nhanes_df ) == nrow( nhanes_demo_df ) ) # keep only individuals who took the &quot;mobile examination center&quot; component nhanes_df &lt;- subset( nhanes_df , ridstatr %in% 2 ) nhanes_design &lt;- svydesign( id = ~sdmvpsu , strata = ~sdmvstra , nest = TRUE , weights = ~wtmec2yr , data = nhanes_df ) Variable Recoding Add new columns to the data set: nhanes_design &lt;- update( nhanes_design , one = 1 , pregnant_at_interview = ifelse( ridexprg %in% 1:2 , as.numeric( ridexprg == 1 ) , NA ) , race_ethnicity = factor( c( 3 , 3 , 1 , 2 , 4 )[ ridreth1 ] , levels = 1:4 , labels = c( &#39;non-hispanic white&#39; , &#39;non-hispanic black&#39; , &#39;hispanic&#39; , &#39;other&#39; ) ) , age_category = factor( findInterval( ridageyr , c( 20 , 40 , 60 ) ) , labels = c( &quot;0-19&quot; , &quot;20-39&quot; , &quot;40-59&quot; , &quot;60+&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nhanes_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ race_ethnicity , nhanes_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nhanes_design ) svyby( ~ one , ~ race_ethnicity , nhanes_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ lbxtc , nhanes_design , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ riagendr , nhanes_design ) svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ lbxtc , nhanes_design , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ riagendr , nhanes_design ) svyby( ~ riagendr , ~ race_ethnicity , nhanes_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ lbxtc , nhanes_design , 0.5 , na.rm = TRUE ) svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ lbxtc , denominator = ~ ridageyr , nhanes_design , na.rm = TRUE ) Subsetting Restrict the survey design to respondents aged 60 or older: sub_nhanes_design &lt;- subset( nhanes_design , age_category == &quot;60+&quot; ) Calculate the mean (average) of this subset: svymean( ~ lbxtc , sub_nhanes_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ lbxtc , nhanes_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ lbxtc , ~ race_ethnicity , nhanes_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nhanes_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ lbxtc , nhanes_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ lbxtc , nhanes_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ pregnant_at_interview , nhanes_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( lbxtc ~ pregnant_at_interview , nhanes_design ) Perform a chi-squared test of association for survey data: svychisq( ~ pregnant_at_interview + riagendr , nhanes_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( lbxtc ~ pregnant_at_interview + riagendr , nhanes_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NHANES users, this code replicates previously-presented examples: library(srvyr) nhanes_srvyr_design &lt;- as_survey( nhanes_design ) Calculate the mean (average) of a linear variable, overall and by groups: nhanes_srvyr_design %&gt;% summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) ) nhanes_srvyr_design %&gt;% group_by( race_ethnicity ) %&gt;% summarize( mean = survey_mean( lbxtc , na.rm = TRUE ) ) Replication Example "],["national-health-interview-survey-nhis.html", "National Health Interview Survey (NHIS) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " National Health Interview Survey (NHIS) The National Health Interview Survey (NHIS) is America’s most detailed household survey of health status and medical experience. A main table with one row for each person within each sampled household, mergeable other tables like the sample child table with a more detailed questionnaire for only one child (when available) within each sampled household. A complex sample survey designed to generalize to the civilian non-institutionalized population of the United States. Released annually since 1963, the most recent major re-design in 1997. Administered by the Centers for Disease Control and Prevention. Simplified Download and Importation The R lodown package easily downloads and imports all available NHIS microdata by simply specifying \"nhis\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nhis&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NHIS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NHIS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NHIS microdata files nhis_cat &lt;- get_catalog( &quot;nhis&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NHIS&quot; ) ) # 2016 only nhis_cat &lt;- subset( nhis_cat , year == 2016 ) # download the microdata to your local computer nhis_cat &lt;- lodown( &quot;nhis&quot; , nhis_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) library(mitools) nhis_personsx_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NHIS&quot; , &quot;2016/personsx.rds&quot; ) ) nhis_income_list &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NHIS&quot; , &quot;2016/incmimp.rds&quot; ) ) merge_variables &lt;- c( &quot;hhx&quot; , &quot;fmx&quot; , &quot;fpx&quot; ) nhis_personsx_df[ merge_variables ] &lt;- sapply( nhis_personsx_df[ merge_variables ] , as.numeric ) inc_vars_to_keep &lt;- c( merge_variables , setdiff( names( nhis_income_list[[ 1 ]] ) , names( nhis_personsx_df ) ) ) # personsx variables to keep vars_to_keep &lt;- c( merge_variables , &quot;ppsu&quot; , &quot;pstrat&quot; , &quot;wtfa&quot; , &#39;phstat&#39; , &#39;sex&#39; , &#39;hospno&#39; , &#39;age_p&#39; , &#39;hinotmyr&#39; , &#39;notcov&#39; ) nhis_personsx_df &lt;- nhis_personsx_df[ vars_to_keep ] nhis_personsx_list &lt;- lapply( nhis_income_list , function( w ){ w &lt;- w[ inc_vars_to_keep ] w[ merge_variables ] &lt;- sapply( w[ merge_variables ] , as.numeric ) result &lt;- merge( nhis_personsx_df , w ) stopifnot( nrow( result ) == nrow( nhis_personsx_df ) ) result } ) # personsx design nhis_design &lt;- svydesign( id = ~ppsu , strata = ~pstrat , nest = TRUE , weights = ~wtfa , data = imputationList( nhis_personsx_list ) ) rm( nhis_personsx_list ) ; gc() nhis_samadult_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NHIS&quot; , &quot;2016/samadult.rds&quot; ) ) nhis_samadult_df[ merge_variables ] &lt;- sapply( nhis_samadult_df[ merge_variables ] , as.numeric ) samadult_vars_to_keep &lt;- c( merge_variables , setdiff( names( nhis_samadult_df ) , names( nhis_personsx_df ) ) ) nhis_personsx_samadult_df &lt;- merge( nhis_personsx_df , nhis_samadult_df[ samadult_vars_to_keep ] ) stopifnot( nrow( nhis_personsx_samadult_df ) == nrow( nhis_samadult_df ) ) rm( nhis_personsx_df , nhis_samadult_df ) ; gc() nhis_samadult_list &lt;- lapply( nhis_income_list , function( w ){ w &lt;- w[ inc_vars_to_keep ] w[ merge_variables ] &lt;- sapply( w[ merge_variables ] , as.numeric ) result &lt;- merge( nhis_personsx_samadult_df , w ) stopifnot( nrow( result ) == nrow( nhis_personsx_samadult_df ) ) result } ) rm( nhis_income_list , nhis_personsx_samadult_df ) ; gc() # sample adult design (commented out) # nhis_samadult_design &lt;- # svydesign( # id = ~ppsu , # strata = ~pstrat , # nest = TRUE , # weights = ~wtfa_sa , # data = imputationList( nhis_samadult_list ) # ) rm( nhis_samadult_list ) ; gc() Variable Recoding Add new columns to the data set: nhis_design &lt;- update( nhis_design , one = 1 , poverty_category = factor( findInterval( povrati3 , 1:4 ) , labels = c( &quot;below poverty&quot; , &quot;100-199%&quot; , &quot;200-299%&quot; , &quot;300-399%&quot; , &quot;400%+&quot; ) ) , fair_or_poor_reported_health = ifelse( phstat %in% 1:5 , as.numeric( phstat &gt;= 4 ) , NA ) , sex = factor( sex , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , hospno = ifelse( hospno &gt; 366 , NA , hospno ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( nhis_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( nhis_design , svyby( ~ one , ~ poverty_category , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( nhis_design , svytotal( ~ one ) ) ) MIcombine( with( nhis_design , svyby( ~ one , ~ poverty_category , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( nhis_design , svymean( ~ age_p ) ) ) MIcombine( with( nhis_design , svyby( ~ age_p , ~ poverty_category , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( nhis_design , svymean( ~ sex ) ) ) MIcombine( with( nhis_design , svyby( ~ sex , ~ poverty_category , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( nhis_design , svytotal( ~ age_p ) ) ) MIcombine( with( nhis_design , svyby( ~ age_p , ~ poverty_category , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( nhis_design , svytotal( ~ sex ) ) ) MIcombine( with( nhis_design , svyby( ~ sex , ~ poverty_category , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( nhis_design , svyquantile( ~ age_p , 0.5 , se = TRUE ) ) ) MIcombine( with( nhis_design , svyby( ~ age_p , ~ poverty_category , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( nhis_design , svyratio( numerator = ~ hinotmyr , denominator = ~ hospno , na.rm = TRUE ) ) ) Subsetting Restrict the survey design to uninsured: sub_nhis_design &lt;- subset( nhis_design , notcov == 1 ) Calculate the mean (average) of this subset: MIcombine( with( sub_nhis_design , svymean( ~ age_p ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( nhis_design , svymean( ~ age_p ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( nhis_design , svyby( ~ age_p , ~ poverty_category , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nhis_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( nhis_design , svyvar( ~ age_p ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( nhis_design , svymean( ~ age_p , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( nhis_design , svymean( ~ age_p , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ fair_or_poor_reported_health , nhis_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( age_p ~ fair_or_poor_reported_health , nhis_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ fair_or_poor_reported_health + sex , nhis_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( nhis_design , svyglm( age_p ~ fair_or_poor_reported_health + sex ) ) ) summary( glm_result ) Replication Example "],["pesquisa-nacional-por-amostra-de-domicilios-nhts.html", "Pesquisa Nacional por Amostra de Domicilios (NHTS) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " Pesquisa Nacional por Amostra de Domicilios (NHTS) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; Brazil’s previous principal household survey, the Pesquisa Nacional por Amostra de Domicilios (PNAD) measures general education, labor, income, and housing characteristics of the population. One table with one row per sampled household and a second table with one row per individual within each sampled household. A complex sample survey designed to generalize to the civilian non-institutional population of Brazil, although the rural north was not included prior to 2004. Released annually since 2001 except for years ending in zero, when the decennial census takes its place. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available NHTS microdata by simply specifying \"nhts\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nhts&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NHTS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NHTS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NHTS microdata files nhts_cat &lt;- get_catalog( &quot;nhts&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NHTS&quot; ) ) # 2011 only nhts_cat &lt;- subset( nhts_cat , year == 2011 ) # download the microdata to your local computer nhts_cat &lt;- lodown( &quot;nhts&quot; , nhts_cat ) Analysis Examples with the survey library   Construct a database-backed complex sample survey design: library(DBI) library(RSQLite) library(survey) options( survey.lonely.psu = &quot;adjust&quot; ) prestratified_design &lt;- svydesign( id = ~v4618 , strata = ~v4617 , data = nhts_cat[ 1 , &quot;db_tablename&quot; ] , weights = ~pre_wgt , nest = TRUE , dbtype = &quot;SQLite&quot; , dbname = nhts_cat[ 1 , &quot;dbfile&quot; ] ) nhts_design &lt;- lodown:::pnad_postStratify( design = prestratified_design , strata.col = &#39;v4609&#39; , oldwgt = &#39;pre_wgt&#39; ) Variable Recoding Add new columns to the data set: nhts_design &lt;- update( nhts_design , age_categories = factor( 1 + findInterval( v8005 , seq( 5 , 60 , 5 ) ) ) , male = as.numeric( v0302 == 2 ) , teenagers = as.numeric( v8005 &gt; 12 &amp; v8005 &lt; 20 ) , started_working_before_thirteen = as.numeric( v9892 &lt; 13 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nhts_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region , nhts_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nhts_design ) svyby( ~ one , ~ region , nhts_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ v4720 , nhts_design , na.rm = TRUE ) svyby( ~ v4720 , ~ region , nhts_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ age_categories , nhts_design ) svyby( ~ age_categories , ~ region , nhts_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ v4720 , nhts_design , na.rm = TRUE ) svyby( ~ v4720 , ~ region , nhts_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ age_categories , nhts_design ) svyby( ~ age_categories , ~ region , nhts_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ v4720 , nhts_design , 0.5 , na.rm = TRUE ) svyby( ~ v4720 , ~ region , nhts_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ started_working_before_thirteen , denominator = ~ teenagers , nhts_design , na.rm = TRUE ) Subsetting Restrict the survey design to married persons: sub_nhts_design &lt;- subset( nhts_design , v4011 == 1 ) Calculate the mean (average) of this subset: svymean( ~ v4720 , sub_nhts_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ v4720 , nhts_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ v4720 , ~ region , nhts_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nhts_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ v4720 , nhts_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ v4720 , nhts_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ v4720 , nhts_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , nhts_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( v4720 ~ male , nhts_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + age_categories , nhts_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( v4720 ~ male + age_categories , nhts_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for NHTS users, this code calculates the gini coefficient on complex sample survey data: library(convey) nhts_design &lt;- convey_prep( nhts_design ) sub_nhts_design &lt;- subset( nhts_design , !is.na( v4720 ) &amp; v4720 != 0 &amp; v8005 &gt;= 15 ) svygini( ~ v4720 , sub_nhts_design , na.rm = TRUE ) Replication Example svytotal( ~one , nhts_design ) svytotal( ~factor( v0302 ) , nhts_design ) cv( svytotal( ~factor( v0302 ) , nhts_design ) ) "],["national-immunization-survey-nis.html", "National Immunization Survey (NIS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " National Immunization Survey (NIS) Contributed by Joe Walsh &lt;jtwalsh@protonmail.com&gt; The National Immunization Survey tracks childhood vaccination rates at the state-level. One table with one row per sampled toddler. A complex sample survey designed to generalize to children aged 19-35 months in the United States. Released biennially since 1995. Administered by the Centers for Disease Control and Prevention. Simplified Download and Importation The R lodown package easily downloads and imports all available NIS microdata by simply specifying \"nis\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nis&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NIS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NIS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NIS microdata files nis_cat &lt;- get_catalog( &quot;nis&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NIS&quot; ) ) # 2015 only nis_cat &lt;- subset( nis_cat , year == 2015 ) # download the microdata to your local computer nis_cat &lt;- lodown( &quot;nis&quot; , nis_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nis_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NIS&quot; , &quot;2015 main.rds&quot; ) ) nis_design &lt;- svydesign( id = ~ seqnumhh , strata = ~ stratum , weights = ~ provwt_d , data = subset( nis_df , provwt_d &gt; 0 ) ) Variable Recoding Add new columns to the data set: nis_design &lt;- update( nis_design , state_name = factor( state , levels = c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 66, 72, 78) , labels = c(&quot;ALABAMA&quot;, &quot;ALASKA&quot;, &quot;ARIZONA&quot;, &quot;ARKANSAS&quot;, &quot;CALIFORNIA&quot;, &quot;COLORADO&quot;, &quot;CONNECTICUT&quot;, &quot;DELAWARE&quot;, &quot;DISTRICT OF COLUMBIA&quot;, &quot;FLORIDA&quot;, &quot;GEORGIA&quot;, &quot;HAWAII&quot;, &quot;IDAHO&quot;, &quot;ILLINOIS&quot;, &quot;INDIANA&quot;, &quot;IOWA&quot;, &quot;KANSAS&quot;, &quot;KENTUCKY&quot;, &quot;LOUISIANA&quot;, &quot;MAINE&quot;, &quot;MARYLAND&quot;, &quot;MASSACHUSETTS&quot;, &quot;MICHIGAN&quot;, &quot;MINNESOTA&quot;, &quot;MISSISSIPPI&quot;, &quot;MISSOURI&quot;, &quot;MONTANA&quot;, &quot;NEBRASKA&quot;, &quot;NEVADA&quot;, &quot;NEW HAMPSHIRE&quot;, &quot;NEW JERSEY&quot;, &quot;NEW MEXICO&quot;, &quot;NEW YORK&quot;, &quot;NORTH CAROLINA&quot;, &quot;NORTH DAKOTA&quot;, &quot;OHIO&quot;, &quot;OKLAHOMA&quot;, &quot;OREGON&quot;, &quot;PENNSYLVANIA&quot;, &quot;RHODE ISLAND&quot;, &quot;SOUTH CAROLINA&quot;, &quot;SOUTH DAKOTA&quot;, &quot;TENNESSEE&quot;, &quot;TEXAS&quot;, &quot;UTAH&quot;, &quot;VERMONT&quot;, &quot;VIRGINIA&quot;, &quot;WASHINGTON&quot;, &quot;WEST VIRGINIA&quot;, &quot;WISCONSIN&quot;, &quot;WYOMING&quot;, &quot;GUAM&quot;, &quot;PUERTO RICO&quot;, &quot;U.S. VIRGIN ISLANDS&quot;) ) , sex = factor( ifelse( sex %in% 1:2 , sex , NA ) , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , dtap_3p = as.numeric( ( p_numdah &gt;= 3 ) | ( p_numdhi &gt;= 3 ) | ( p_numdih &gt;= 3 ) | ( p_numdta &gt;= 3 ) | ( p_numdtp &gt;= 3 ) ) , dtap_4p = as.numeric( ( p_numdah &gt;= 4 ) | ( p_numdhi &gt;= 4 ) | ( p_numdih &gt;= 4 ) | ( p_numdta &gt;= 4 ) | ( p_numdtp &gt;= 4 ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nis_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , nis_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nis_design ) svyby( ~ one , ~ state_name , nis_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ p_nuhepx , nis_design , na.rm = TRUE ) svyby( ~ p_nuhepx , ~ state_name , nis_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sex , nis_design , na.rm = TRUE ) svyby( ~ sex , ~ state_name , nis_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ p_nuhepx , nis_design , na.rm = TRUE ) svyby( ~ p_nuhepx , ~ state_name , nis_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sex , nis_design , na.rm = TRUE ) svyby( ~ sex , ~ state_name , nis_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ p_nuhepx , nis_design , 0.5 , na.rm = TRUE ) svyby( ~ p_nuhepx , ~ state_name , nis_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ childnm , denominator = ~ bf_endr06 , nis_design , na.rm = TRUE ) Subsetting Restrict the survey design to toddlers up to date on polio shots: sub_nis_design &lt;- subset( nis_design , p_utdpol == 1 ) Calculate the mean (average) of this subset: svymean( ~ p_nuhepx , sub_nis_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ p_nuhepx , nis_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ p_nuhepx , ~ state_name , nis_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nis_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ p_nuhepx , nis_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ p_nuhepx , nis_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ p_nuhepx , nis_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ dtap_3p , nis_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( p_nuhepx ~ dtap_3p , nis_design ) Perform a chi-squared test of association for survey data: svychisq( ~ dtap_3p + sex , nis_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( p_nuhepx ~ dtap_3p + sex , nis_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NIS users, this code replicates previously-presented examples: library(srvyr) nis_srvyr_design &lt;- as_survey( nis_design ) Calculate the mean (average) of a linear variable, overall and by groups: nis_srvyr_design %&gt;% summarize( mean = survey_mean( p_nuhepx , na.rm = TRUE ) ) nis_srvyr_design %&gt;% group_by( state_name ) %&gt;% summarize( mean = survey_mean( p_nuhepx , na.rm = TRUE ) ) Replication Example "],["national-longitudinal-surveys-nls.html", "National Longitudinal Surveys (NLS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr  ", " National Longitudinal Surveys (NLS) The National Longitudinal Surveys follow the same sample of individuals from specific birth cohorts over time. The surveys collect data on labor market activity, schooling, fertility, program participation, and health. Multiple panels, each with one table with one row per sampled panel respondent. A series of complex sample surveys designed to generalize to various cohorts of Americans born during designated time periods. Updated biennally for most active panels. Administered by the Bureau of Labor Statistics. Simplified Download and Importation The R lodown package easily downloads and imports all available NLS microdata by simply specifying \"nls\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nls&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NLS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NLS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NLS microdata files nls_cat &lt;- get_catalog( &quot;nls&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NLS&quot; ) ) # National Longitudinal Survey of Youth, 1997 only nls_cat &lt;- subset( nls_cat , study_name == &#39;NLS Youth 1997 (NLSY97)&#39; ) # download the microdata to your local computer nls_cat &lt;- lodown( &quot;nls&quot; , nls_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nlsy_files &lt;- list.files( file.path( path.expand( &quot;~&quot; ) , &quot;NLS&quot; ) , full.names = TRUE , recursive = TRUE ) # read in the R loading script nlsy97_r &lt;- readLines( nlsy_files[ grepl( &quot;nlsy97(.*)R$&quot; , basename( nlsy_files ) ) ] ) # find all instances of &quot;data$&quot; in the r script data_dollar &lt;- grep( &quot;data\\\\$&quot; , nlsy97_r ) # extract the column names from the R script first_line &lt;- grep( &quot;names(new_data) &lt;-&quot; , nlsy97_r , fixed = TRUE ) close_parentheses &lt;- grep( &quot;)&quot; , nlsy97_r , fixed = TRUE ) last_line &lt;- min( close_parentheses[ close_parentheses &gt; first_line ] ) column_names_lines &lt;- nlsy97_r[ seq( first_line , last_line ) ] column_names_lines &lt;- gsub( &#39;names(new_data) &lt;-&#39; , &#39;column_names &lt;-&#39; , column_names_lines , fixed = TRUE ) eval( parse( text = column_names_lines ) ) # choose which columns to import columns_to_import &lt;- c( &#39;R0000100&#39; , &#39;T5206900&#39; , &#39;R9829600&#39; , &#39;R0536300&#39; , &#39;Z9061800&#39; , &#39;T6657200&#39; , &#39;R1205300&#39; , &#39;T7545600&#39; ) # for each column to import, look for a recoding block find_recoding_block &lt;- function( w ){ this_block_start &lt;- min( grep( paste0( &quot;data\\\\$&quot; , w ) , nlsy97_r ) ) recode_lines &lt;- seq( this_block_start , min( data_dollar[ data_dollar &gt; this_block_start ] ) - 1 ) paste( nlsy97_r[ recode_lines ] , collapse = &#39;&#39; ) } recodes_to_run &lt;- unlist( lapply( columns_to_import , find_recoding_block ) ) # readr::read_delim() columns must match their order in the csv file columns_to_import &lt;- columns_to_import[ order( match( columns_to_import , column_names ) ) ] # confirm all column names are available stopifnot( all( columns_to_import %in% column_names ) ) # identify the .dat file nlsy97_dat &lt;- nlsy_files[ grepl( &quot;nlsy97(.*)dat$&quot; , basename( nlsy_files ) ) ] nls_variables_df &lt;- data.frame( readr::read_delim( nlsy97_dat , col_names = columns_to_import , col_types = paste0( ifelse( column_names %in% columns_to_import , &#39;n&#39; , &#39;_&#39; ) , collapse = &quot;&quot; ) , delim = &#39; &#39; ) ) # remove all missings nls_variables_df[ nls_variables_df &lt; 0 ] &lt;- NA recodes_to_run &lt;- gsub( &quot;data\\\\$&quot; , &quot;nls_variables_df$&quot; , recodes_to_run ) # align the main variables with what the R script says for( this_recode in recodes_to_run ) eval( parse( text = this_recode ) ) # cluster and strata variables nls_psustr_df &lt;- readRDS( grep( &quot;strpsu\\\\.rds$&quot; , nlsy_files , value = TRUE ) ) # you can read more about longitudinal weights here # http://www.nlsinfo.org/weights # the get_nlsy_weights function returns a data.frame object # containing the unique person identifiers and also a column of weights. # view which points-in-time are available for a particular study # get_nlsy_selections( &quot;nlsy97&quot; ) # download weights for respondents in 1997 w &lt;- nls_get_weights( &quot;nlsy97&quot; , &#39;YES&#39; , &#39;SURV1997&#39; ) # download weights for respondents who were in any of the 1997, 2002, or 2007 surveys # w &lt;- # nls_get_weights( &quot;nlsy97&quot; , &#39;YES&#39; , c( &#39;SURV1997&#39; , &#39;SURV2002&#39; , &#39;SURV2007&#39; ) ) # download weights for respondents who were in all of the 1997, 2002, and 2007 surveys # w &lt;- # nls_get_weights( &quot;nlsy97&quot; , &#39;NO&#39; , c( &#39;SURV1997&#39; , &#39;SURV2002&#39; , &#39;SURV2007&#39; ) ) # download weights for respondents who are in all available surveys # w &lt;- # nls_get_weights( &quot;nlsy97&quot; , &quot;NO&quot; , nls_get_selections( &quot;nlsy97&quot; ) ) # merge weights with cluster and strata variables nls_survey_df &lt;- merge( nls_psustr_df , w ) # merge variables onto survey design nls_df &lt;- merge( nls_variables_df , nls_survey_df ) nls_design &lt;- svydesign( ~ R1489800 , strata = ~ R1489700 , data = nls_df , weights = ~ weight , nest = TRUE ) Variable Recoding Add new columns to the data set: nls_design &lt;- update( nls_design , one = 1 , bachelors_degree_or_higher = as.numeric( as.numeric( T6657200 ) &gt;= 5 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nls_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ R1205300 , nls_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nls_design ) svyby( ~ one , ~ R1205300 , nls_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ T7545600 , nls_design , na.rm = TRUE ) svyby( ~ T7545600 , ~ R1205300 , nls_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ T6657200 , nls_design , na.rm = TRUE ) svyby( ~ T6657200 , ~ R1205300 , nls_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ T7545600 , nls_design , na.rm = TRUE ) svyby( ~ T7545600 , ~ R1205300 , nls_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ T6657200 , nls_design , na.rm = TRUE ) svyby( ~ T6657200 , ~ R1205300 , nls_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ T7545600 , nls_design , 0.5 , na.rm = TRUE ) svyby( ~ T7545600 , ~ R1205300 , nls_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ R9829600 , denominator = ~ T7545600 , nls_design , na.rm = TRUE ) Subsetting Restrict the survey design to raised by only biological mother or father in 1997: sub_nls_design &lt;- subset( nls_design , as.numeric( R1205300 ) %in% 4:5 ) Calculate the mean (average) of this subset: svymean( ~ T7545600 , sub_nls_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ T7545600 , nls_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ T7545600 , ~ R1205300 , nls_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nls_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ T7545600 , nls_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ T7545600 , nls_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ T7545600 , nls_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ bachelors_degree_or_higher , nls_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( T7545600 ~ bachelors_degree_or_higher , nls_design ) Perform a chi-squared test of association for survey data: svychisq( ~ bachelors_degree_or_higher + T6657200 , nls_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( T7545600 ~ bachelors_degree_or_higher + T6657200 , nls_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NLS users, this code replicates previously-presented examples: library(srvyr) nls_srvyr_design &lt;- as_survey( nls_design ) Calculate the mean (average) of a linear variable, overall and by groups: nls_srvyr_design %&gt;% summarize( mean = survey_mean( T7545600 , na.rm = TRUE ) ) nls_srvyr_design %&gt;% group_by( R1205300 ) %&gt;% summarize( mean = survey_mean( T7545600 , na.rm = TRUE ) ) "],["national-plan-and-provider-enumeration-system-nppes.html", "National Plan and Provider Enumeration System (NPPES) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " National Plan and Provider Enumeration System (NPPES) The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry. A single large table with one row per enumerated health care provider. A census of individuals and organizations who bill for medical services in the United States. Updated monthly with new providers. Maintained by the United States Centers for Medicare &amp; Medicaid Services (CMS) Simplified Download and Importation The R lodown package easily downloads and imports all available NPPES microdata by simply specifying \"nppes\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nppes&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NPPES&quot; ) ) Analysis Examples with base R   Load a data frame: npi_filepath &lt;- grep( &quot;npidata_pfile_20050523-([0-9]+)\\\\.csv&quot; , list.files( file.path( path.expand( &quot;~&quot; ) , &quot;NPPES&quot; ) , full.names = TRUE ) , value = TRUE ) column_names &lt;- names( read.csv( npi_filepath , nrow = 1 )[ FALSE , , ] ) column_names &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , tolower( column_names ) ) column_types &lt;- ifelse( grepl( &quot;code&quot; , column_names ) &amp; !grepl( &quot;country|state|gender|taxonomy|postal&quot; , column_names ) , &#39;n&#39; , &#39;c&#39; ) columns_to_import &lt;- c( &quot;entity_type_code&quot; , &quot;provider_gender_code&quot; , &quot;provider_enumeration_date&quot; , &quot;is_sole_proprietor&quot; , &quot;provider_business_practice_location_address_state_name&quot; ) stopifnot( all( columns_to_import %in% column_names ) ) # readr::read_csv() columns must match their order in the csv file columns_to_import &lt;- columns_to_import[ order( match( columns_to_import , column_names ) ) ] nppes_df &lt;- data.frame( readr::read_csv( npi_filepath , col_names = columns_to_import , col_types = paste0( ifelse( column_names %in% columns_to_import , column_types , &#39;_&#39; ) , collapse = &quot;&quot; ) , skip = 1 ) ) Variable Recoding Add new columns to the data set: nppes_df &lt;- transform( nppes_df , individual = as.numeric( entity_type_code ) , provider_enumeration_year = as.numeric( substr( provider_enumeration_date , 7 , 10 ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( nppes_df ) table( nppes_df[ , &quot;provider_gender_code&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( nppes_df[ , &quot;is_sole_proprietor&quot; ] ) ) prop.table( table( nppes_df[ , c( &quot;is_sole_proprietor&quot; , &quot;provider_gender_code&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( nppes_df[ , &quot;provider_enumeration_year&quot; ] , 0.5 , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to California: sub_nppes_df &lt;- subset( nppes_df , provider_business_practice_location_address_state_name = &#39;CA&#39; ) Calculate the mean (average) of this subset: mean( sub_nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( nppes_df[ , &quot;provider_enumeration_year&quot; ] , na.rm = TRUE ) tapply( nppes_df[ , &quot;provider_enumeration_year&quot; ] , nppes_df[ , &quot;provider_gender_code&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( provider_enumeration_year ~ individual , nppes_df ) Perform a chi-squared test of association: this_table &lt;- table( nppes_df[ , c( &quot;individual&quot; , &quot;is_sole_proprietor&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( provider_enumeration_year ~ individual + is_sole_proprietor , data = nppes_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for NPPES users, this code replicates previously-presented examples: library(dplyr) nppes_tbl &lt;- tbl_df( nppes_df ) Calculate the mean (average) of a linear variable, overall and by groups: nppes_tbl %&gt;% summarize( mean = mean( provider_enumeration_year , na.rm = TRUE ) ) nppes_tbl %&gt;% group_by( provider_gender_code ) %&gt;% summarize( mean = mean( provider_enumeration_year , na.rm = TRUE ) ) "],["national-survey-of-oaa-participants-nps.html", "National Survey of OAA Participants (NPS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " National Survey of OAA Participants (NPS) The National Survey of OAA Participants measures program satisfaction with state agency community services for American seniors. One table with one row per sampled senior respondent. A complex sample survey designed to generalize to non-institutionalized beneficiaries of Area Agencies on Aging (AAA) within the United States. Released annually since 2003. Administered by the U.S. Administration on Aging. Simplified Download and Importation The R lodown package easily downloads and imports all available NPS microdata by simply specifying \"nps\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nps&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NPS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NPS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NPS microdata files nps_cat &lt;- get_catalog( &quot;nps&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NPS&quot; ) ) # 2015 only nps_cat &lt;- subset( nps_cat , year == 2015 ) # download the microdata to your local computer nps_cat &lt;- lodown( &quot;nps&quot; , nps_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) nps_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NPS&quot; , &quot;2015 transportation.rds&quot; ) ) nps_design &lt;- svrepdesign( data = nps_df , repweights = &quot;pstotwgt[0-9]&quot; , weights = ~ pstotwgt , type = &quot;Fay&quot; , rho = 0.29986 , mse = TRUE ) Variable Recoding Add new columns to the data set: nps_design &lt;- update( nps_design , age_category = factor( agec , levels = 2:5 , labels = c( &quot;60-64&quot; , &quot;65-74&quot; , &quot;75-84&quot; , &quot;85+&quot; ) ) , gender = factor( gender , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , trip_this_week = as.numeric( trdays %in% 1:2 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nps_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_category , nps_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nps_design ) svyby( ~ one , ~ age_category , nps_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ adlaoa6p , nps_design , na.rm = TRUE ) svyby( ~ adlaoa6p , ~ age_category , nps_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ gender , nps_design ) svyby( ~ gender , ~ age_category , nps_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ adlaoa6p , nps_design , na.rm = TRUE ) svyby( ~ adlaoa6p , ~ age_category , nps_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ gender , nps_design ) svyby( ~ gender , ~ age_category , nps_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ adlaoa6p , nps_design , 0.5 , na.rm = TRUE ) svyby( ~ adlaoa6p , ~ age_category , nps_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ adlaoa6p , denominator = ~ iadlaoa7 , nps_design , na.rm = TRUE ) Subsetting Restrict the survey design to beneficiaries who live alone: sub_nps_design &lt;- subset( nps_design , livealone == 1 ) Calculate the mean (average) of this subset: svymean( ~ adlaoa6p , sub_nps_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ adlaoa6p , nps_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ adlaoa6p , ~ age_category , nps_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nps_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ adlaoa6p , nps_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ adlaoa6p , nps_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ adlaoa6p , nps_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ trip_this_week , nps_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( adlaoa6p ~ trip_this_week , nps_design ) Perform a chi-squared test of association for survey data: svychisq( ~ trip_this_week + gender , nps_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( adlaoa6p ~ trip_this_week + gender , nps_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NPS users, this code replicates previously-presented examples: library(srvyr) nps_srvyr_design &lt;- as_survey( nps_design ) Calculate the mean (average) of a linear variable, overall and by groups: nps_srvyr_design %&gt;% summarize( mean = survey_mean( adlaoa6p , na.rm = TRUE ) ) nps_srvyr_design %&gt;% group_by( age_category ) %&gt;% summarize( mean = survey_mean( adlaoa6p , na.rm = TRUE ) ) Replication Example "],["national-survey-of-childrens-health-nsch.html", "National Survey of Children’s Health (NSCH) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " National Survey of Children’s Health (NSCH) Contributed by Emily Wiegand &lt;erowewiegand@gmail.com&gt; The National Survey of Children’s Health (NSCH) offers state-level estimates of children’s health care and the family environment. One row per sampled child under eighteen. A complex sample survey designed to generalize to non-institutionalized children in the United States at the state-level. Released every four or five years since 2003. Sponsored by the Maternal and Child Health Bureau of the Health Resources and Services Administration. Simplified Download and Importation The R lodown package easily downloads and imports all available NSCH microdata by simply specifying \"nsch\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nsch&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NSCH&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NSCH catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NSCH microdata files nsch_cat &lt;- get_catalog( &quot;nsch&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NSCH&quot; ) ) # 2012 only nsch_cat &lt;- subset( nsch_cat , year == 2012 ) # download the microdata to your local computer nsch_cat &lt;- lodown( &quot;nsch&quot; , nsch_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) nsch_imp &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NSCH&quot; , &quot;2012 main.rds&quot; ) ) nsch_design &lt;- svydesign( id = ~ 1 , strata = ~ state + sample , weights = ~ nschwt , data = imputationList( nsch_imp ) ) Variable Recoding Add new columns to the data set: nsch_design &lt;- update( nsch_design , indicator_1_3 = ifelse( k6q40 &gt; 1 , NA , k6q40 ) , indicator_5_2 = ifelse( k7q05r %in% 1:5 , 1 , ifelse( k7q05r %in% 0 , 0 , NA ) ) , indicator_5_3 = ifelse( k7q30 == 1 | k7q31 == 1 | k7q32 == 1 , 1 , ifelse( k7q30 == 0 | k7q31 == 0 | k7q32 == 0 , 0 , NA ) ) , povcat = factor( findInterval( povlevel_i , c( 1 , 2 , 6 , 8 ) ) , labels = c( &quot;below poverty&quot; , &quot;100-199% fpl&quot; , &quot;200-399% fpl&quot; , &quot;400%+ fpl&quot; ) ) , sex = factor( ifelse( sex %in% 1:2 , sex , NA ) , labels = c( &quot;male&quot; , &quot;female&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( nsch_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( nsch_design , svyby( ~ one , ~ state , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( nsch_design , svytotal( ~ one ) ) ) MIcombine( with( nsch_design , svyby( ~ one , ~ state , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( nsch_design , svymean( ~ ageyr_child ) ) ) MIcombine( with( nsch_design , svyby( ~ ageyr_child , ~ state , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( nsch_design , svymean( ~ povcat ) ) ) MIcombine( with( nsch_design , svyby( ~ povcat , ~ state , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( nsch_design , svytotal( ~ ageyr_child ) ) ) MIcombine( with( nsch_design , svyby( ~ ageyr_child , ~ state , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( nsch_design , svytotal( ~ povcat ) ) ) MIcombine( with( nsch_design , svyby( ~ povcat , ~ state , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( nsch_design , svyquantile( ~ ageyr_child , 0.5 , se = TRUE ) ) ) MIcombine( with( nsch_design , svyby( ~ ageyr_child , ~ state , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( nsch_design , svyratio( numerator = ~ k6q63 , denominator = ~ totkids4 ) ) ) Subsetting Restrict the survey design to only children: sub_nsch_design &lt;- subset( nsch_design , agepos4 == 1 ) Calculate the mean (average) of this subset: MIcombine( with( sub_nsch_design , svymean( ~ ageyr_child ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( nsch_design , svymean( ~ ageyr_child ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( nsch_design , svyby( ~ ageyr_child , ~ state , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsch_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( nsch_design , svyvar( ~ ageyr_child ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( nsch_design , svymean( ~ ageyr_child , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( nsch_design , svymean( ~ ageyr_child , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ indicator_5_2 , nsch_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( ageyr_child ~ indicator_5_2 , nsch_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ indicator_5_2 + povcat , nsch_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( nsch_design , svyglm( ageyr_child ~ indicator_5_2 + povcat ) ) ) summary( glm_result ) Replication Example "],["national-study-on-drug-use-and-health-nsduh.html", "National Study on Drug Use and Health (NSDUH) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " National Study on Drug Use and Health (NSDUH) The National Study on Drug Use and Health measures the prevalence and correlates of drug use in the United States. One table with one row per sampled respondent. A complex sample survey designed to generalize to the civilian, noninstitutionalized population of the United States aged 12 and older. Released periodically since 1979 and annually since 1990. Administered by the Substance Abuse and Mental Health Services Administration. Simplified Download and Importation The R lodown package easily downloads and imports all available NSDUH microdata by simply specifying \"nsduh\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nsduh&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NSDUH&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NSDUH catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NSDUH microdata files nsduh_cat &lt;- get_catalog( &quot;nsduh&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NSDUH&quot; ) ) # 2016 only nsduh_cat &lt;- subset( nsduh_cat , year == 2016 ) # download the microdata to your local computer nsduh_cat &lt;- lodown( &quot;nsduh&quot; , nsduh_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) nsduh_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NSDUH&quot; , &quot;2016 main.rds&quot; ) ) variables_to_keep &lt;- c( &#39;verep&#39; , &#39;vestr&#39; , &#39;analwt_c&#39; , &#39;health&#39; , &#39;cigtry&#39; , &#39;cocage&#39; , &#39;mjever&#39; , &#39;coutyp4&#39; , &#39;preg&#39; ) nsduh_df &lt;- nsduh_df[ variables_to_keep ] ; gc() nsduh_design &lt;- svydesign( id = ~ verep , strata = ~ vestr , data = nsduh_df , weights = ~ analwt_c , nest = TRUE ) Variable Recoding Add new columns to the data set: nsduh_design &lt;- update( nsduh_design , one = 1 , health = factor( health , levels = 1:5 , labels = c( &quot;excellent&quot; , &quot;very good&quot; , &quot;good&quot; , &quot;fair&quot; , &quot;poor&quot; ) ) , age_tried_first_cigarette = ifelse( cigtry &gt; 99 , NA , cigtry ) , age_tried_cocaine = ifelse( cocage &gt; 99 , NA , cocage ) , ever_used_marijuana = as.numeric( mjever == 1 ) , county_type = factor( coutyp4 , levels = 1:3 , labels = c( &quot;large metro&quot; , &quot;small metro&quot; , &quot;nonmetro&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nsduh_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ county_type , nsduh_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nsduh_design ) svyby( ~ one , ~ county_type , nsduh_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE ) svyby( ~ age_tried_first_cigarette , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ health , nsduh_design , na.rm = TRUE ) svyby( ~ health , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE ) svyby( ~ age_tried_first_cigarette , ~ county_type , nsduh_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ health , nsduh_design , na.rm = TRUE ) svyby( ~ health , ~ county_type , nsduh_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ age_tried_first_cigarette , nsduh_design , 0.5 , na.rm = TRUE ) svyby( ~ age_tried_first_cigarette , ~ county_type , nsduh_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ age_tried_first_cigarette , denominator = ~ age_tried_cocaine , nsduh_design , na.rm = TRUE ) Subsetting Restrict the survey design to individuals who are pregnant: sub_nsduh_design &lt;- subset( nsduh_design , preg == 1 ) Calculate the mean (average) of this subset: svymean( ~ age_tried_first_cigarette , sub_nsduh_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ age_tried_first_cigarette , ~ county_type , nsduh_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsduh_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ age_tried_first_cigarette , nsduh_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ ever_used_marijuana , nsduh_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( age_tried_first_cigarette ~ ever_used_marijuana , nsduh_design ) Perform a chi-squared test of association for survey data: svychisq( ~ ever_used_marijuana + health , nsduh_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( age_tried_first_cigarette ~ ever_used_marijuana + health , nsduh_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NSDUH users, this code replicates previously-presented examples: library(srvyr) nsduh_srvyr_design &lt;- as_survey( nsduh_design ) Calculate the mean (average) of a linear variable, overall and by groups: nsduh_srvyr_design %&gt;% summarize( mean = survey_mean( age_tried_first_cigarette , na.rm = TRUE ) ) nsduh_srvyr_design %&gt;% group_by( county_type ) %&gt;% summarize( mean = survey_mean( age_tried_first_cigarette , na.rm = TRUE ) ) Replication Example "],["national-survey-of-family-growth-nsfg.html", "National Survey of Family Growth (NSFG) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " National Survey of Family Growth (NSFG) The National Survey of Family Growth (NSFG) is the principal survey to measure reproductive behavior in the United States population. Multiple tables with one row per respondent for the female and male tables, then a separate table with one row per pregnancy. A complex sample survey designed to generalize to the 15-44 year old population of the United States, by gender. Released every couple of years since 1973. Administered by the Centers for Disease Control and Prevention. Simplified Download and Importation The R lodown package easily downloads and imports all available NSFG microdata by simply specifying \"nsfg\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nsfg&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NSFG&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NSFG catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NSFG microdata files nsfg_cat &lt;- get_catalog( &quot;nsfg&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NSFG&quot; ) ) # 2013-2015 only nsfg_cat &lt;- subset( nsfg_cat , grepl( &quot;2013_2015&quot; , full_url ) ) # download the microdata to your local computer nsfg_cat &lt;- lodown( &quot;nsfg&quot; , nsfg_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) nsfg_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NSFG&quot; , &quot;2013_2015_FemRespData.rds&quot; ) ) nsfg_design &lt;- svydesign( id = ~ secu , strata = ~ sest , data = nsfg_df , weights = ~ wgt2013_2015 , nest = TRUE ) Variable Recoding Add new columns to the data set: nsfg_design &lt;- update( nsfg_design , one = 1 , birth_control_pill = as.numeric( constat1 == 6 ) , age_categories = factor( findInterval( ager , c( 15 , 20 , 25 , 30 , 35 , 40 ) ) , labels = c( &#39;15-19&#39; , &#39;20-24&#39; , &#39;25-29&#39; , &#39;30-34&#39; , &#39;35-39&#39; , &#39;40-44&#39; ) ) , marstat = factor( marstat , levels = c( 1:6 , 8:9 ) , labels = c( &quot;Married to a person of the opposite sex&quot; , &quot;Not married but living together with a partner of the opposite sex&quot; , &quot;Widowed&quot; , &quot;Divorced or annulled&quot; , &quot;Separated, because you and your spouse are not getting along&quot; , &quot;Never been married&quot; , &quot;Refused&quot; , &quot;Don&#39;t know&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nsfg_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ age_categories , nsfg_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nsfg_design ) svyby( ~ one , ~ age_categories , nsfg_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ npregs_s , nsfg_design , na.rm = TRUE ) svyby( ~ npregs_s , ~ age_categories , nsfg_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ marstat , nsfg_design ) svyby( ~ marstat , ~ age_categories , nsfg_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ npregs_s , nsfg_design , na.rm = TRUE ) svyby( ~ npregs_s , ~ age_categories , nsfg_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ marstat , nsfg_design ) svyby( ~ marstat , ~ age_categories , nsfg_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ npregs_s , nsfg_design , 0.5 , na.rm = TRUE ) svyby( ~ npregs_s , ~ age_categories , nsfg_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ npregs_s , denominator = ~ nbabes_s , nsfg_design , na.rm = TRUE ) Subsetting Restrict the survey design to ever cohabited: sub_nsfg_design &lt;- subset( nsfg_design , timescoh &gt; 0 ) Calculate the mean (average) of this subset: svymean( ~ npregs_s , sub_nsfg_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ npregs_s , nsfg_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ npregs_s , ~ age_categories , nsfg_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nsfg_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ npregs_s , nsfg_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ npregs_s , nsfg_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ npregs_s , nsfg_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ birth_control_pill , nsfg_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( npregs_s ~ birth_control_pill , nsfg_design ) Perform a chi-squared test of association for survey data: svychisq( ~ birth_control_pill + marstat , nsfg_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( npregs_s ~ birth_control_pill + marstat , nsfg_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NSFG users, this code replicates previously-presented examples: library(srvyr) nsfg_srvyr_design &lt;- as_survey( nsfg_design ) Calculate the mean (average) of a linear variable, overall and by groups: nsfg_srvyr_design %&gt;% summarize( mean = survey_mean( npregs_s , na.rm = TRUE ) ) nsfg_srvyr_design %&gt;% group_by( age_categories ) %&gt;% summarize( mean = survey_mean( npregs_s , na.rm = TRUE ) ) Replication Example The example below matches statistics and standard errors from this table pulled from the CDC variance estimation examples: Match the sum of the weights: stopifnot( round( coef( svytotal( ~ one , nsfg_design ) ) , 0 ) == 61491766 ) Compute the statistics and standard errors for percentage of women currently using the pill by age: row_percents &lt;- c( 16.1723 , 27.8173 , 18.7197 , 13.4367 , 9.0851 , 7.7224 ) std_err_row_percents &lt;- c( 1.5414 , 2.4162 , 1.9890 , 1.5074 , 1.5222 , 1.6053 ) results &lt;- svyby( ~ birth_control_pill , ~ age_categories , nsfg_design , svymean ) stopifnot( all( round( coef( results ) * 100 , 4 ) == row_percents ) ) stopifnot( all( round( SE( results ) * 100 , 4 ) == std_err_row_percents ) ) "],["national-plan-and-provider-enumeration-system-nvss.html", "National Plan and Provider Enumeration System (NVSS) Simplified Download and Importation Analysis Examples with SQL and RSQLite   Analysis Examples with dplyr   Replication Example", " National Plan and Provider Enumeration System (NVSS) The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry. A single large table with one row per enumerated health care provider. A census of individuals and organizations who bill for medical services in the United States. Updated monthly with new providers. Maintained by the United States Centers for Medicare &amp; Medicaid Services (CMS) Simplified Download and Importation The R lodown package easily downloads and imports all available NVSS microdata by simply specifying \"nvss\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nvss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NVSS&quot; ) ) Analysis Examples with SQL and RSQLite   Connect to a database: library(DBI) dbdir &lt;- file.path( path.expand( &quot;~&quot; ) , &quot;NVSS&quot; , &quot;SQLite.db&quot; ) db &lt;- dbConnect( RSQLite::SQLite() , dbdir ) Variable Recoding Add new columns to the data set: dbSendQuery( db , &quot;ALTER TABLE npi ADD COLUMN individual INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE npi SET individual = CASE WHEN entity_type_code = 1 THEN 1 ELSE 0 END&quot; ) dbSendQuery( db , &quot;ALTER TABLE npi ADD COLUMN provider_enumeration_year INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE npi SET provider_enumeration_year = CAST( SUBSTRING( provider_enumeration_date , 7 , 10 ) AS INTEGER )&quot; ) Unweighted Counts Count the unweighted number of records in the SQL table, overall and by groups: dbGetQuery( db , &quot;SELECT COUNT(*) FROM npi&quot; ) dbGetQuery( db , &quot;SELECT provider_gender_code , COUNT(*) FROM npi GROUP BY provider_gender_code&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT AVG( provider_enumeration_year ) FROM npi&quot; ) dbGetQuery( db , &quot;SELECT provider_gender_code , AVG( provider_enumeration_year ) AS mean_provider_enumeration_year FROM npi GROUP BY provider_gender_code&quot; ) Calculate the distribution of a categorical variable: dbGetQuery( db , &quot;SELECT is_sole_proprietor , COUNT(*) / ( SELECT COUNT(*) FROM npi ) AS share_is_sole_proprietor FROM npi GROUP BY is_sole_proprietor&quot; ) Calculate the sum of a linear variable, overall and by groups: dbGetQuery( db , &quot;SELECT SUM( provider_enumeration_year ) FROM npi&quot; ) dbGetQuery( db , &quot;SELECT provider_gender_code , SUM( provider_enumeration_year ) AS sum_provider_enumeration_year FROM npi GROUP BY provider_gender_code&quot; ) Calculate the 25th, median, and 75th percentiles of a linear variable, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT LOWER_QUARTILE( provider_enumeration_year ) , MEDIAN( provider_enumeration_year ) , UPPER_QUARTILE( provider_enumeration_year ) FROM npi&quot; ) dbGetQuery( db , &quot;SELECT provider_gender_code , LOWER_QUARTILE( provider_enumeration_year ) AS lower_quartile_provider_enumeration_year , MEDIAN( provider_enumeration_year ) AS median_provider_enumeration_year , UPPER_QUARTILE( provider_enumeration_year ) AS upper_quartile_provider_enumeration_year FROM npi GROUP BY provider_gender_code&quot; ) Subsetting Limit your SQL analysis to California with WHERE: dbGetQuery( db , &quot;SELECT AVG( provider_enumeration_year ) FROM npi WHERE provider_business_practice_location_address_state_name = &#39;CA&#39;&quot; ) Measures of Uncertainty Calculate the variance and standard deviation, overall and by groups: RSQLite::initExtension( db ) dbGetQuery( db , &quot;SELECT VARIANCE( provider_enumeration_year ) , STDEV( provider_enumeration_year ) FROM npi&quot; ) dbGetQuery( db , &quot;SELECT provider_gender_code , VARIANCE( provider_enumeration_year ) AS var_provider_enumeration_year , STDEV( provider_enumeration_year ) AS stddev_provider_enumeration_year FROM npi GROUP BY provider_gender_code&quot; ) Regression Models and Tests of Association Perform a t-test: nvss_slim_df &lt;- dbGetQuery( db , &quot;SELECT provider_enumeration_year , individual , is_sole_proprietor FROM npi&quot; ) t.test( provider_enumeration_year ~ individual , nvss_slim_df ) Perform a chi-squared test of association: this_table &lt;- table( nvss_slim_df[ , c( &quot;individual&quot; , &quot;is_sole_proprietor&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( provider_enumeration_year ~ individual + is_sole_proprietor , data = nvss_slim_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for NVSS users, this code replicates previously-presented examples: library(dplyr) library(dbplyr) dplyr_db &lt;- dplyr::src_sqlite( dbdir ) nvss_tbl &lt;- tbl( dplyr_db , &#39;npi&#39; ) Calculate the mean (average) of a linear variable, overall and by groups: nvss_tbl %&gt;% summarize( mean = mean( provider_enumeration_year ) ) nvss_tbl %&gt;% group_by( provider_gender_code ) %&gt;% summarize( mean = mean( provider_enumeration_year ) ) Replication Example dbGetQuery( db , &quot;SELECT COUNT(*) FROM npi&quot; ) "],["new-york-city-housing-and-vacancy-survey-nychvs.html", "New York City Housing and Vacancy Survey (NYCHVS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " New York City Housing and Vacancy Survey (NYCHVS) The New York City Housing and Vacancy Survey (NYCHVS) covers the city-wide rental vacancy rate and other characteristics like neighborhood housing stock. One table with one record per occupied housing unit, a second table with one record per person inside each occupied housing unit, and a third table with one record per unoccupied housing unit. A complex sample survey designed to generalize to all occupied and unoccupied housing units in the five boroughs of New York City. Released triennially since 1998. Funded by the New York City Department of Housing Preservation and Development and conducted by the United States Census Bureau. Simplified Download and Importation The R lodown package easily downloads and imports all available NYCHVS microdata by simply specifying \"nychvs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;nychvs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NYCHVS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the NYCHVS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available NYCHVS microdata files nychvs_cat &lt;- get_catalog( &quot;nychvs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;NYCHVS&quot; ) ) # 2014 only nychvs_cat &lt;- subset( nychvs_cat , year == 2014 ) # download the microdata to your local computer nychvs_cat &lt;- lodown( &quot;nychvs&quot; , nychvs_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) # load the occupied units table nychvs_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;NYCHVS&quot; , &quot;2014/occ.rds&quot; ) ) nychvs_design &lt;- svydesign( ~ 1 , data = nychvs_df , weights = ~ fw ) Variable Recoding Add new columns to the data set: nychvs_design &lt;- update( nychvs_design , one = 1 , home_owners = as.numeric( sc115 == 1 ) , yearly_household_income = ifelse( uf42 == 9999999 , 0 , as.numeric( uf42 ) ) , gross_monthly_rent = ifelse( uf17 == 99999 , NA , as.numeric( uf17 ) ) , borough = factor( boro , levels = 1:5 , labels = c( &#39;Bronx&#39; , &#39;Brooklyn&#39; , &#39;Manhattan&#39; , &#39;Queens&#39; , &#39;Staten Island&#39; ) ) , householder_sex = factor( hhr2 , labels = c( &#39;male&#39; , &#39;female&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( nychvs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ borough , nychvs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , nychvs_design ) svyby( ~ one , ~ borough , nychvs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) svyby( ~ yearly_household_income , ~ borough , nychvs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ householder_sex , nychvs_design ) svyby( ~ householder_sex , ~ borough , nychvs_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) svyby( ~ yearly_household_income , ~ borough , nychvs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ householder_sex , nychvs_design ) svyby( ~ householder_sex , ~ borough , nychvs_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ yearly_household_income , nychvs_design , 0.5 , na.rm = TRUE ) svyby( ~ yearly_household_income , ~ borough , nychvs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ gross_monthly_rent , denominator = ~ yearly_household_income , nychvs_design , na.rm = TRUE ) Subsetting Restrict the survey design to Manhattan: sub_nychvs_design &lt;- subset( nychvs_design , boro == 3 ) Calculate the mean (average) of this subset: svymean( ~ yearly_household_income , sub_nychvs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ yearly_household_income , ~ borough , nychvs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( nychvs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ yearly_household_income , nychvs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ yearly_household_income , nychvs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ home_owners , nychvs_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( yearly_household_income ~ home_owners , nychvs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ home_owners + householder_sex , nychvs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( yearly_household_income ~ home_owners + householder_sex , nychvs_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for NYCHVS users, this code replicates previously-presented examples: library(srvyr) nychvs_srvyr_design &lt;- as_survey( nychvs_design ) Calculate the mean (average) of a linear variable, overall and by groups: nychvs_srvyr_design %&gt;% summarize( mean = survey_mean( yearly_household_income , na.rm = TRUE ) ) nychvs_srvyr_design %&gt;% group_by( borough ) %&gt;% summarize( mean = survey_mean( yearly_household_income , na.rm = TRUE ) ) Replication Example "],["pew-research-center-pew.html", "Pew Research Center (PEW) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Pew Research Center (PEW) The Pew Research Center releases its survey microdata on U.S. Politics &amp; Policy, Journalism &amp; Media, Internet, Science &amp; Tech, Religion &amp; Public Life, Hispanic Trends, Global Attitudes &amp; Trends, and Social &amp; Demographic Trends. Generally one table per survey, with one row per sampled respondent. Complex sample surveys, often designed to generalize to the U.S. adult population or the adult populations of the nations surveyed. Administered by the Pew Research Center. Simplified Download and Importation The R lodown package easily downloads and imports all available PEW microdata by simply specifying \"pew\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pew&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PEW&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PEW catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PEW microdata files pew_cat &lt;- get_catalog( &quot;pew&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PEW&quot; ) ) # spring 2015 only pew_cat &lt;- subset( pew_cat , name == &quot;Spring 2015 Survey Data&quot; ) # download the microdata to your local computer pew_cat &lt;- lodown( &quot;pew&quot; , pew_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) pew_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PEW&quot; , &quot;Global Attitudes &amp; Trends/2015/Spring 2015 Survey Data&quot; , &quot;Pew Research Global Attitudes Spring 2015 Dataset for Web FINAL.rds&quot; ) ) # limit the global attitudes data set to just israel israel_df &lt;- subset( pew_df , country == 14 ) pew_design &lt;- svydesign( id = ~ psu , strata = ~ stratum , weight = ~ weight , data = israel_df ) Variable Recoding Add new columns to the data set: pew_design &lt;- update( pew_design , one = 1 , your_day_today = factor( q1 , levels = 1:3 , labels = c( &#39;a typical day&#39; , &#39;a particularly good day&#39; , &#39;a particularly bad day&#39; ) ) , school_years = ifelse( q163b %in% 98:99 , NA , q163b ) , age_in_years = ifelse( q146 %in% 98:99 , NA , q146 ) , climate_change_concern = ifelse( q13a %in% 1:5 , as.numeric( q13a &lt; 3 ) , NA ) , country_economic_situation = factor( q3 , levels = 1:4 , labels = c( &#39;very good&#39; , &#39;somewhat good&#39; , &#39;somewhat bad&#39; , &#39;very bad&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pew_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ your_day_today , pew_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pew_design ) svyby( ~ one , ~ your_day_today , pew_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ school_years , pew_design , na.rm = TRUE ) svyby( ~ school_years , ~ your_day_today , pew_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ country_economic_situation , pew_design , na.rm = TRUE ) svyby( ~ country_economic_situation , ~ your_day_today , pew_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ school_years , pew_design , na.rm = TRUE ) svyby( ~ school_years , ~ your_day_today , pew_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ country_economic_situation , pew_design , na.rm = TRUE ) svyby( ~ country_economic_situation , ~ your_day_today , pew_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ school_years , pew_design , 0.5 , na.rm = TRUE ) svyby( ~ school_years , ~ your_day_today , pew_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ school_years , denominator = ~ age_in_years , pew_design , na.rm = TRUE ) Subsetting Restrict the survey design to seniors: sub_pew_design &lt;- subset( pew_design , q146 &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ school_years , sub_pew_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ school_years , pew_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ school_years , ~ your_day_today , pew_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pew_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ school_years , pew_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ school_years , pew_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ school_years , pew_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ climate_change_concern , pew_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( school_years ~ climate_change_concern , pew_design ) Perform a chi-squared test of association for survey data: svychisq( ~ climate_change_concern + country_economic_situation , pew_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( school_years ~ climate_change_concern + country_economic_situation , pew_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for PEW users, this code replicates previously-presented examples: library(srvyr) pew_srvyr_design &lt;- as_survey( pew_design ) Calculate the mean (average) of a linear variable, overall and by groups: pew_srvyr_design %&gt;% summarize( mean = survey_mean( school_years , na.rm = TRUE ) ) pew_srvyr_design %&gt;% group_by( your_day_today ) %&gt;% summarize( mean = survey_mean( school_years , na.rm = TRUE ) ) Replication Example "],["programme-for-the-international-assessment-of-adult-competencies-piaac.html", "Programme for the International Assessment of Adult Competencies (PIAAC) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " Programme for the International Assessment of Adult Competencies (PIAAC) The Programme for the International Assessment of Adult Competencies (PIAAC) offers cross-national comparisons for the serious study of advanced-nation labor markets. One row per sampled adult. A multiply-imputed, complex sample survey designed to generalize to the population aged 16 to 65 across thirty three OECD nations. No expected release timeline. Administered by the Organisation for Economic Co-operation and Development. Simplified Download and Importation The R lodown package easily downloads and imports all available PIAAC microdata by simply specifying \"piaac\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;piaac&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PIAAC&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PIAAC catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PIAAC microdata files piaac_cat &lt;- get_catalog( &quot;piaac&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PIAAC&quot; ) ) # download the microdata to your local computer piaac_cat &lt;- lodown( &quot;piaac&quot; , piaac_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) piaac_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PIAAC&quot; , &quot;prgusap1 design.rds&quot; ) ) Variable Recoding Add new columns to the data set: piaac_design &lt;- update( piaac_design , one = 1 , sex = factor( gender_r , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , age_categories = factor( ageg10lfs , levels = 1:5 , labels = c( &quot;24 or less&quot; , &quot;25-34&quot; , &quot;35-44&quot; , &quot;45-54&quot; , &quot;55 plus&quot; ) ) , working_at_paid_job_last_week = as.numeric( c_q01a == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( piaac_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( piaac_design , svyby( ~ one , ~ age_categories , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ one ) ) ) MIcombine( with( piaac_design , svyby( ~ one , ~ age_categories , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( piaac_design , svymean( ~ sex ) ) ) MIcombine( with( piaac_design , svyby( ~ sex , ~ age_categories , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ pvnum , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svytotal , na.rm = TRUE ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( piaac_design , svytotal( ~ sex ) ) ) MIcombine( with( piaac_design , svyby( ~ sex , ~ age_categories , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( piaac_design , svyquantile( ~ pvnum , 0.5 , se = TRUE , na.rm = TRUE ) ) ) MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE , na.rm = TRUE ) ) ) Estimate a ratio: MIcombine( with( piaac_design , svyratio( numerator = ~ pvnum , denominator = ~ pvlit , na.rm = TRUE ) ) ) Subsetting Restrict the survey design to self-reported fair or poor health: sub_piaac_design &lt;- subset( piaac_design , i_q08 %in% 4:5 ) Calculate the mean (average) of this subset: MIcombine( with( sub_piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( piaac_design , svyby( ~ pvnum , ~ age_categories , svymean , na.rm = TRUE ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( piaac_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( piaac_design , svyvar( ~ pvnum , na.rm = TRUE ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( piaac_design , svymean( ~ pvnum , na.rm = TRUE , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ working_at_paid_job_last_week , piaac_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( pvnum ~ working_at_paid_job_last_week , piaac_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ working_at_paid_job_last_week + sex , piaac_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( piaac_design , svyglm( pvnum ~ working_at_paid_job_last_week + sex ) ) ) summary( glm_result ) Replication Example The OECD’s Technical Report Table 18.9 on PDF page 455 includes statistics and standard errors for the three PIAAC domains. This code precisely replicates the Austria row shown in that official table. austria_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PIAAC&quot; , &quot;prgautp1 design.rds&quot; ) ) austria_pvlit &lt;- MIcombine( with( austria_design , svymean( ~ pvlit , na.rm = TRUE ) ) ) austria_pvnum &lt;- MIcombine( with( austria_design , svymean( ~ pvnum , na.rm = TRUE ) ) ) austria_pvpsl &lt;- MIcombine( with( austria_design , svymean( ~ pvpsl , na.rm = TRUE ) ) ) # confirm each estimate and standard error matches the published statistics stopifnot( round( coef( austria_pvlit ) ) == 269 ) stopifnot( round( SE( austria_pvlit ) , 1 ) == 0.7 ) stopifnot( round( coef( austria_pvnum ) ) == 275 ) stopifnot( round( SE( austria_pvnum ) , 1 ) == 0.9 ) stopifnot( round( coef( austria_pvpsl ) ) == 284 ) stopifnot( round( SE( austria_pvpsl ) , 1 ) == 0.7 ) "],["progress-in-international-reading-literacy-study-pirls.html", "Progress in International Reading Literacy Study (PIRLS) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " Progress in International Reading Literacy Study (PIRLS) The Progress in International Reading Literacy Study (PIRLS) tracks the reading competency of fourth graders across about fifty nations. A series of tables with one record per school (ACG), per student (ASG), per teacher (ATG), as well as files containing student achievement (ASA), home background (ASH), student-teacher linkage (AST), and within-country scoring reliability (ASR). A complex sample survey designed to generalize to the fourth-grade student population of participating countries. Released quinquennially since 2001. Funded by the International Association for the Evaluation of Educational Achievement and compiled by the Lynch School of Education at Boston College. Simplified Download and Importation The R lodown package easily downloads and imports all available PIRLS microdata by simply specifying \"pirls\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pirls&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PIRLS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PIRLS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PIRLS microdata files pirls_cat &lt;- get_catalog( &quot;pirls&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PIRLS&quot; ) ) # 2016 only pirls_cat &lt;- subset( pirls_cat , year == 2016 ) # download the microdata to your local computer pirls_cat &lt;- lodown( &quot;pirls&quot; , pirls_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) # load the ASG (student background) + ASH (home background) merged design pirls_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PIRLS&quot; , &quot;2016/asg_design.rds&quot; ) ) # optional step to limit memory usage variables_to_keep &lt;- c( &#39;idcntry&#39; , &#39;itsex&#39; , &#39;asdage&#39; , &#39;asrrea&#39; , &#39;asrlit&#39; ) pirls_design$designs &lt;- lapply( pirls_design$designs , function( w ) { w$variables &lt;- w$variables[ variables_to_keep ] w } ) gc() Variable Recoding Add new columns to the data set: pirls_design &lt;- update( pirls_design , one = 1 , idcntry = factor( idcntry ) , sex = factor( itsex , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , age_ten_or_older = as.numeric( asdage &gt;= 10 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( pirls_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( pirls_design , svyby( ~ one , ~ idcntry , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( pirls_design , svytotal( ~ one ) ) ) MIcombine( with( pirls_design , svyby( ~ one , ~ idcntry , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( pirls_design , svymean( ~ asrrea ) ) ) MIcombine( with( pirls_design , svyby( ~ asrrea , ~ idcntry , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( pirls_design , svymean( ~ sex , na.rm = TRUE ) ) ) MIcombine( with( pirls_design , svyby( ~ sex , ~ idcntry , svymean , na.rm = TRUE ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( pirls_design , svytotal( ~ asrrea ) ) ) MIcombine( with( pirls_design , svyby( ~ asrrea , ~ idcntry , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( pirls_design , svytotal( ~ sex , na.rm = TRUE ) ) ) MIcombine( with( pirls_design , svyby( ~ sex , ~ idcntry , svytotal , na.rm = TRUE ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( pirls_design , svyquantile( ~ asrrea , 0.5 , se = TRUE ) ) ) MIcombine( with( pirls_design , svyby( ~ asrrea , ~ idcntry , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( pirls_design , svyratio( numerator = ~ asrlit , denominator = ~ asrrea ) ) ) Subsetting Restrict the survey design to Australia, Austria, Azerbaijan, Belgium (French): sub_pirls_design &lt;- subset( pirls_design , idcntry %in% c( 36 , 40 , 31 , 957 ) ) Calculate the mean (average) of this subset: MIcombine( with( sub_pirls_design , svymean( ~ asrrea ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( pirls_design , svymean( ~ asrrea ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( pirls_design , svyby( ~ asrrea , ~ idcntry , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pirls_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( pirls_design , svyvar( ~ asrrea ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( pirls_design , svymean( ~ asrrea , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( pirls_design , svymean( ~ asrrea , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ age_ten_or_older , pirls_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( asrrea ~ age_ten_or_older , pirls_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ age_ten_or_older + sex , pirls_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( pirls_design , svyglm( asrrea ~ age_ten_or_older + sex ) ) ) summary( glm_result ) Replication Example These calculations reproduce the reading proficiency statistics and standard errors displayed in Appendix 4A PDF pages 10 and 11 for both Australia and the United States. australia_usa_design &lt;- subset( pirls_design , idcntry %in% c( 36 , 840 ) ) rm( pirls_design ) ; gc() results &lt;- MIcombine( with( australia_usa_design , svyby( ~ asrrea , ~ idcntry , svymean ) ) ) stopifnot( round( coef( results )[1] , 2 ) == 544.36 ) stopifnot( round( SE( results )[1] , 2 ) == 2.53 ) stopifnot( round( coef( results )[2] , 2 ) == 549.44 ) stopifnot( round( SE( results )[2] , 2 ) == 3.09 ) "],["program-for-international-student-assessment-pisa.html", "Program for International Student Assessment (PISA) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " Program for International Student Assessment (PISA) The authoritative source for evaluating educational achievement across nations, the Program(me) for International Student Assessment ranks the math, science, and reading skills of high school students across the developed world. A large table with one row per student, a smaller table with one row per school, then multiple (optional) tables such as one row per parent or per teacher. A complex sample survey designed to generalize to 15-year-old schoolchildren in more than sixty countries. Released triennially since 2000. Administered by the OECD. Simplified Download and Importation The R lodown package easily downloads and imports all available PISA microdata by simply specifying \"pisa\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pisa&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PISA&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PISA catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PISA microdata files pisa_cat &lt;- get_catalog( &quot;pisa&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PISA&quot; ) ) # 2015 only pisa_cat &lt;- subset( pisa_cat , year == 2015 ) # download the microdata to your local computer pisa_cat &lt;- lodown( &quot;pisa&quot; , pisa_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, database-backed complex sample survey design: library(DBI) library(RSQLite) library(survey) library(mitools) pisa_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PISA&quot; , &quot;2015 cmb_stu_qqq design.rds&quot; ) ) pisa_design &lt;- lodown:::svyMDBdesign( pisa_design ) Variable Recoding Add new columns to the data set: pisa_design &lt;- update( pisa_design , gender = factor( st004d01t , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , how_many_computers_at_home = factor( st012q06na , labels = c( &quot;none&quot; , &quot;one&quot; , &quot;two&quot; , &quot;three or more&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( pisa_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( pisa_design , svyby( ~ one , ~ gender , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( pisa_design , svytotal( ~ one ) ) ) MIcombine( with( pisa_design , svyby( ~ one , ~ gender , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( pisa_design , svymean( ~ scie ) ) ) MIcombine( with( pisa_design , svyby( ~ scie , ~ gender , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( pisa_design , svymean( ~ how_many_computers_at_home ) ) ) MIcombine( with( pisa_design , svyby( ~ how_many_computers_at_home , ~ gender , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( pisa_design , svytotal( ~ scie ) ) ) MIcombine( with( pisa_design , svyby( ~ scie , ~ gender , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( pisa_design , svytotal( ~ how_many_computers_at_home ) ) ) MIcombine( with( pisa_design , svyby( ~ how_many_computers_at_home , ~ gender , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( pisa_design , svyquantile( ~ scie , 0.5 , se = TRUE ) ) ) MIcombine( with( pisa_design , svyby( ~ scie , ~ gender , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( pisa_design , svyratio( numerator = ~ math , denominator = ~ reading ) ) ) Subsetting Restrict the survey design to Albania: sub_pisa_design &lt;- subset( pisa_design , cnt == &quot;ALB&quot; ) Calculate the mean (average) of this subset: MIcombine( with( sub_pisa_design , svymean( ~ scie ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( pisa_design , svymean( ~ scie ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( pisa_design , svyby( ~ scie , ~ gender , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pisa_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( pisa_design , svyvar( ~ scie ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( pisa_design , svymean( ~ scie , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( pisa_design , svymean( ~ scie , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ oecd , pisa_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( scie ~ oecd , pisa_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ oecd + how_many_computers_at_home , pisa_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( pisa_design , svyglm( scie ~ oecd + how_many_computers_at_home ) ) ) summary( glm_result ) Replication Example "],["public-libraries-survey-pls.html", "Public Libraries Survey (PLS) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " Public Libraries Survey (PLS) An annual census of public libraries in the United States. One table with one row per state, a second table with one row per library system, and a third table with one row per library building or bookmobile. Released annually since 1992. Conducted by the Institute of Museum and Library Services (IMLS) and collected by the US Census Bureau. Simplified Download and Importation The R lodown package easily downloads and imports all available PLS microdata by simply specifying \"pls\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pls&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PLS&quot; ) ) Analysis Examples with base R   Load a data frame: pls_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PLS&quot; , &quot;2014/pls_fy_ae_puplda.rds&quot; ) ) Variable Recoding Add new columns to the data set: pls_df &lt;- transform( pls_df , c_relatn = factor( c_relatn , levels = c( &quot;HQ&quot; , &quot;ME&quot; , &quot;NO&quot; ) , c( &quot;HQ-Headquarters of a federation or cooperative&quot; , &quot;ME-Member of a federation or cooperative&quot; , &quot;NO-Not a member of a federation or cooperative&quot; ) ) , more_than_one_librarian = as.numeric( libraria &gt; 1 ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( pls_df ) table( pls_df[ , &quot;stabr&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( pls_df[ , &quot;popu_lsa&quot; ] ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( pls_df[ , &quot;c_relatn&quot; ] ) ) prop.table( table( pls_df[ , c( &quot;c_relatn&quot; , &quot;stabr&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( pls_df[ , &quot;popu_lsa&quot; ] ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( pls_df[ , &quot;popu_lsa&quot; ] , 0.5 ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to more than one million annual visits: sub_pls_df &lt;- subset( pls_df , visits &gt; 1000000 ) Calculate the mean (average) of this subset: mean( sub_pls_df[ , &quot;popu_lsa&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( pls_df[ , &quot;popu_lsa&quot; ] ) tapply( pls_df[ , &quot;popu_lsa&quot; ] , pls_df[ , &quot;stabr&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( popu_lsa ~ more_than_one_librarian , pls_df ) Perform a chi-squared test of association: this_table &lt;- table( pls_df[ , c( &quot;more_than_one_librarian&quot; , &quot;c_relatn&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( popu_lsa ~ more_than_one_librarian + c_relatn , data = pls_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for PLS users, this code replicates previously-presented examples: library(dplyr) pls_tbl &lt;- tbl_df( pls_df ) Calculate the mean (average) of a linear variable, overall and by groups: pls_tbl %&gt;% summarize( mean = mean( popu_lsa ) ) pls_tbl %&gt;% group_by( stabr ) %&gt;% summarize( mean = mean( popu_lsa ) ) "],["pesquisa-mensal-de-emprego-pme.html", "Pesquisa Mensal de Emprego (PME) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Pesquisa Mensal de Emprego (PME) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; The Pesquisa Mensal de Emprego (PME) is the monthly labor force survey covering the six largest Brazilian cities. One table with one row per individual within each sampled household. A complex sample survey designed to generalize to the civilian population of Brazil’s six largest cities. Released monthly since March 2002. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available PME microdata by simply specifying \"pme\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pme&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PME&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PME catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PME microdata files pme_cat &lt;- get_catalog( &quot;pme&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PME&quot; ) ) # 2016 only pme_cat &lt;- subset( pme_cat , year == 2016 ) # download the microdata to your local computer pme_cat &lt;- lodown( &quot;pme&quot; , pme_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) pme_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PME&quot; , &quot;pme 2016 01.rds&quot; ) ) # throw out records missing their cluster variable pme_df &lt;- subset( pme_df , !is.na( v113 ) ) pop_totals &lt;- unique( pme_df[ , c( &#39;v035&#39; , &#39;v114&#39; ) ] ) prestratified_design &lt;- svydesign( ~ v113 , strata = ~ v112 , data = pme_df , weights = ~ v211 , nest = TRUE ) pme_design &lt;- postStratify( prestratified_design , ~ v035 , pop_totals ) Variable Recoding Add new columns to the data set: pme_design &lt;- update( pme_design , one = 1 , # calculate whether each person is at least ten years of age pia = as.numeric( v234 &gt;= 10 ) , # determine individuals who are employed ocup_c = as.numeric( v401 == 1 | v402 == 1 | v403 == 1 ) , sexo = factor( v203 , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , region = factor( v035 , levels = c( 26 , 29 , 31 , 33 , 35 , 43 ) , labels = c( &quot;Recife&quot; , &quot;Salvador&quot; , &quot;Belo Horizonte&quot; , &quot;Rio de Janeiro&quot; , &quot;Sao Paulo&quot; , &quot;Porto Alegre&quot; ) ) ) pme_design &lt;- update( pme_design , # determine individuals who are unemployed desocup30 = as.numeric( ocup_c == 0 &amp; !is.na( v461 ) &amp; v465 == 1 ) ) pme_design &lt;- update( pme_design , # determine individuals who are either working or not working pea_c = as.numeric( ocup_c == 1 | desocup30 == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pme_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region , pme_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pme_design ) svyby( ~ one , ~ region , pme_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ vd25 , pme_design , na.rm = TRUE ) svyby( ~ vd25 , ~ region , pme_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ sexo , pme_design ) svyby( ~ sexo , ~ region , pme_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ vd25 , pme_design , na.rm = TRUE ) svyby( ~ vd25 , ~ region , pme_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ sexo , pme_design ) svyby( ~ sexo , ~ region , pme_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ vd25 , pme_design , 0.5 , na.rm = TRUE ) svyby( ~ vd25 , ~ region , pme_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ desocup30 , denominator = ~ pea_c , pme_design , na.rm = TRUE ) Subsetting Restrict the survey design to teenagers: sub_pme_design &lt;- subset( pme_design , v234 %in% 13:19 ) Calculate the mean (average) of this subset: svymean( ~ vd25 , sub_pme_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ vd25 , pme_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ vd25 , ~ region , pme_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pme_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ vd25 , pme_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ vd25 , pme_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ vd25 , pme_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ ocup_c , pme_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( vd25 ~ ocup_c , pme_design ) Perform a chi-squared test of association for survey data: svychisq( ~ ocup_c + sexo , pme_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( vd25 ~ ocup_c + sexo , pme_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for PME users, this code replicates previously-presented examples: library(srvyr) pme_srvyr_design &lt;- as_survey( pme_design ) Calculate the mean (average) of a linear variable, overall and by groups: pme_srvyr_design %&gt;% summarize( mean = survey_mean( vd25 , na.rm = TRUE ) ) pme_srvyr_design %&gt;% group_by( region ) %&gt;% summarize( mean = survey_mean( vd25 , na.rm = TRUE ) ) Replication Example "],["pesquisa-nacional-por-amostra-de-domicilios-pnad.html", "Pesquisa Nacional por Amostra de Domicilios (PNAD) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " Pesquisa Nacional por Amostra de Domicilios (PNAD) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; Brazil’s previous principal household survey, the Pesquisa Nacional por Amostra de Domicilios (PNAD) measures general education, labor, income, and housing characteristics of the population. One table with one row per sampled household and a second table with one row per individual within each sampled household. A complex sample survey designed to generalize to the civilian non-institutional population of Brazil, although the rural north was not included prior to 2004. Released annually since 2001 except for years ending in zero, when the decennial census takes its place. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available PNAD microdata by simply specifying \"pnad\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pnad&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PNAD&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PNAD catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PNAD microdata files pnad_cat &lt;- get_catalog( &quot;pnad&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PNAD&quot; ) ) # 2011 only pnad_cat &lt;- subset( pnad_cat , year == 2011 ) # download the microdata to your local computer pnad_cat &lt;- lodown( &quot;pnad&quot; , pnad_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) pnad_df &lt;- readRDS( pnad_cat[ 1 , &#39;output_filename&#39; ] ) pop_types &lt;- data.frame( v4609 = unique( pnad_df$v4609 ) , Freq = unique( pnad_df$v4609 ) ) prestratified_design &lt;- svydesign( id = ~ v4618 , strata = ~ v4617 , data = pnad_df , weights = ~ pre_wgt , nest = TRUE ) rm( pnad_df ) ; gc() pnad_design &lt;- postStratify( design = prestratified_design , strata = ~ v4609 , population = pop_types ) rm( prestratified_design ) ; gc() Variable Recoding Add new columns to the data set: pnad_design &lt;- update( pnad_design , age_categories = factor( 1 + findInterval( v8005 , seq( 5 , 60 , 5 ) ) ) , male = as.numeric( v0302 == 2 ) , teenagers = as.numeric( v8005 &gt; 12 &amp; v8005 &lt; 20 ) , started_working_before_thirteen = as.numeric( v9892 &lt; 13 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pnad_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region , pnad_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pnad_design ) svyby( ~ one , ~ region , pnad_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ v4720 , pnad_design , na.rm = TRUE ) svyby( ~ v4720 , ~ region , pnad_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ age_categories , pnad_design ) svyby( ~ age_categories , ~ region , pnad_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ v4720 , pnad_design , na.rm = TRUE ) svyby( ~ v4720 , ~ region , pnad_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ age_categories , pnad_design ) svyby( ~ age_categories , ~ region , pnad_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ v4720 , pnad_design , 0.5 , na.rm = TRUE ) svyby( ~ v4720 , ~ region , pnad_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ started_working_before_thirteen , denominator = ~ teenagers , pnad_design , na.rm = TRUE ) Subsetting Restrict the survey design to married persons: sub_pnad_design &lt;- subset( pnad_design , v4011 == 1 ) Calculate the mean (average) of this subset: svymean( ~ v4720 , sub_pnad_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ v4720 , pnad_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ v4720 , ~ region , pnad_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pnad_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ v4720 , pnad_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ v4720 , pnad_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ v4720 , pnad_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , pnad_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( v4720 ~ male , pnad_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + age_categories , pnad_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( v4720 ~ male + age_categories , pnad_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for PNAD users, this code calculates the gini coefficient on complex sample survey data: library(convey) pnad_design &lt;- convey_prep( pnad_design ) sub_pnad_design &lt;- subset( pnad_design , !is.na( v4720 ) &amp; v4720 != 0 &amp; v8005 &gt;= 15 ) svygini( ~ v4720 , sub_pnad_design , na.rm = TRUE ) Replication Example svytotal( ~one , pnad_design ) svytotal( ~factor( v0302 ) , pnad_design ) cv( svytotal( ~factor( v0302 ) , pnad_design ) ) "],["pesquisa-nacional-por-amostra-de-domicilios---continua-pnadc.html", "Pesquisa Nacional por Amostra de Domicilios - Continua (PNADC) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " Pesquisa Nacional por Amostra de Domicilios - Continua (PNADC) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; Brazil’s principal household survey, the Pesquisa Nacional por Amostra de Domicilios Continua (PNADC) measures general education, labor, income, and housing characteristics of the population. One table with one row per sampled household and a second table with one row per individual within each sampled household. A complex sample survey designed to generalize to the civilian non-institutional population of Brazil. Released quarterly since 2012. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available PNADC microdata by simply specifying \"pnadc\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pnadc&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PNADC&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PNADC catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PNADC microdata files pnadc_cat &lt;- get_catalog( &quot;pnadc&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PNADC&quot; ) ) # 2015 3rd quarter only pnadc_cat &lt;- subset( pnadc_cat , year == 2015 &amp; quarter == &#39;03&#39; ) # download the microdata to your local computer pnadc_cat &lt;- lodown( &quot;pnadc&quot; , pnadc_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) options( survey.lonely.psu = &quot;adjust&quot; ) pnadc_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PNADC&quot; , &quot;pnadc 2015 03.rds&quot; ) ) # add a column of all ones pnadc_df$one &lt;- 1 # construct a data.frame object with all state names. uf &lt;- structure(list(V1 = c(11L, 12L, 13L, 14L, 15L, 16L, 17L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 31L, 32L, 33L, 35L, 41L, 42L, 43L, 50L, 51L, 52L, 53L), V2 = structure(c(22L, 1L, 4L, 23L, 14L, 3L, 27L, 10L, 18L, 6L, 20L, 15L, 17L, 2L, 26L, 5L, 13L, 8L, 19L, 25L, 16L, 24L, 21L, 12L, 11L, 9L, 7L), .Label = c(&quot;Acre&quot;, &quot;Alagoas&quot;, &quot;Amapa&quot;, &quot;Amazonas&quot;, &quot;Bahia&quot;, &quot;Ceara&quot;, &quot;Distrito Federal&quot;, &quot;Espirito Santo&quot;, &quot;Goias&quot;, &quot;Maranhao&quot;, &quot;Mato Grosso&quot;, &quot;Mato Grosso do Sul&quot;, &quot;Minas Gerais&quot;, &quot;Para&quot;, &quot;Paraiba&quot;, &quot;Parana&quot;, &quot;Pernambuco&quot;, &quot;Piaui&quot;, &quot;Rio de Janeiro&quot;, &quot;Rio Grande do Norte&quot;, &quot;Rio Grande do Sul&quot;, &quot;Rondonia&quot;, &quot;Roraima&quot;, &quot;Santa Catarina&quot;, &quot;Sao Paulo&quot;, &quot;Sergipe&quot;, &quot;Tocantins&quot;), class = &quot;factor&quot;)), .Names = c(&quot;uf&quot;, &quot;uf_name&quot;), class = &quot;data.frame&quot;, row.names = c(NA, -27L)) # merge this data.frame onto the main `x` data.frame # using `uf` as the merge field, keeping all non-matches. pnadc_df &lt;- merge( pnadc_df , uf , all.x = TRUE ) # confirm complete matches stopifnot( all( !is.na( pnadc_df$uf_name ) ) ) # preliminary survey design pre_stratified &lt;- svydesign( ids = ~ upa , strata = ~ estrato , weights = ~ v1027 , data = pnadc_df , nest = TRUE ) # warning: do not use `pre_stratified` in your analyses! # you must use the `pnadc_design` object created below. # post-stratification targets df_pos &lt;- data.frame( posest = unique( pnadc_df$posest ) , Freq = unique( pnadc_df$v1029 ) ) # final survey design object pnadc_design &lt;- postStratify( pre_stratified , ~ posest , df_pos ) # remove the `pnadc_df` data.frame object # and the `pre_stratified` design before stratification rm( pnadc_df , pre_stratified ) Variable Recoding Add new columns to the data set: pnadc_design &lt;- update( pnadc_design , age_categories = factor( 1 + findInterval( v2009 , seq( 5 , 60 , 5 ) ) ) , male = as.numeric( v2007 == 1 ) , pia = as.numeric( v2009 &gt;= 14 ) , region = substr( uf , 1 , 1 ) ) pnadc_design &lt;- update( pnadc_design , ocup_c = ifelse( pia == 1 , as.numeric( vd4002 %in% 1 ) , NA ) , desocup30 = ifelse( pia == 1 , as.numeric( vd4002 %in% 2 ) , NA ) , # calculate usual income from main job # (rendimento habitual do trabalho principal) vd4016n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4016 , NA ) , # calculate effective income from main job # (rendimento efetivo do trabalho principal) vd4017n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4017 , NA ) , # calculate usual income from all jobs # (variavel rendimento habitual de todos os trabalhos) vd4019n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4019 , NA ) , # calculate effective income from all jobs # (rendimento efetivo do todos os trabalhos) vd4020n = ifelse( pia %in% 1 &amp; vd4015 %in% 1 , vd4020 , NA ) , # determine individuals who are either working or not working # (that is, the potential labor force) pea_c = as.numeric( ocup_c == 1 | desocup30 == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pnadc_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ uf_name , pnadc_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pnadc_design ) svyby( ~ one , ~ uf_name , pnadc_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ vd4020n , pnadc_design , na.rm = TRUE ) svyby( ~ vd4020n , ~ uf_name , pnadc_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ age_categories , pnadc_design ) svyby( ~ age_categories , ~ uf_name , pnadc_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ vd4020n , pnadc_design , na.rm = TRUE ) svyby( ~ vd4020n , ~ uf_name , pnadc_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ age_categories , pnadc_design ) svyby( ~ age_categories , ~ uf_name , pnadc_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ vd4020n , pnadc_design , 0.5 , na.rm = TRUE ) svyby( ~ vd4020n , ~ uf_name , pnadc_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ocup_c , denominator = ~ pea_c , pnadc_design , na.rm = TRUE ) Subsetting Restrict the survey design to employed persons: sub_pnadc_design &lt;- subset( pnadc_design , ocup_c == 1 ) Calculate the mean (average) of this subset: svymean( ~ vd4020n , sub_pnadc_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ vd4020n , pnadc_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ vd4020n , ~ uf_name , pnadc_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pnadc_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ vd4020n , pnadc_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ vd4020n , pnadc_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ vd4020n , pnadc_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , pnadc_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( vd4020n ~ male , pnadc_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + age_categories , pnadc_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( vd4020n ~ male + age_categories , pnadc_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for PNADC users, this code calculates the gini coefficient on complex sample survey data: library(convey) # read pnadc microdata for the year 2017 visita 1 repeated_cat &lt;- get_catalog( &quot;pnadc&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PNADC&quot; ) ) repeated_cat &lt;- subset( repeated_cat , year == 2017 &amp; interview == 1 ) repeated_cat &lt;- lodown( &quot;pnadc&quot; , repeated_cat ) visita_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PNADC&quot; , &quot;pnadc 2017 visita_1.rds&quot; ) ) # read deflators file library(readxl) annual.ftp &lt;- &quot;ftp://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_continua/Anual/Microdados/Documentacao/&quot; tf &lt;- tempfile() download.file( paste0( annual.ftp , &quot;deflator_2017_pnadc.xls&quot; ) , tf , mode = &quot;wb&quot; ) deflator_2017_pnadc &lt;- read_excel( path = tf ) names(deflator_2017_pnadc)[2]&lt;- &quot;trimestre&quot; names(deflator_2017_pnadc)&lt;- tolower(names(deflator_2017_pnadc)) # merge the two files using the variables ano, trimestre, uf visita_df &lt;- merge(visita_df, deflator_2017_pnadc, by= c(&quot;ano&quot;,&quot;trimestre&quot;, &quot;uf&quot;) ) # define region variable visita_df$region &lt;- factor(substring(visita_df$uf, 1, 1), levels = 1:5, labels = c(&quot;North&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;, &quot;South&quot;, &quot;Center-West&quot;) ) visita_df$state_name &lt;- factor( visita_df$uf , levels = c( 11:17 , 21:29 , 31:33 , 35 , 41:43 , 50:53 ) , labels = c( &quot;Rondonia&quot; , &quot;Acre&quot; , &quot;Amazonas&quot; , &quot;Roraima&quot; , &quot;Para&quot; , &quot;Amapa&quot; , &quot;Tocantins&quot; , &quot;Maranhao&quot; , &quot;Piaui&quot; , &quot;Ceara&quot; , &quot;Rio Grande do Norte&quot; , &quot;Paraiba&quot; , &quot;Pernambuco&quot; , &quot;Alagoas&quot; , &quot;Sergipe&quot; , &quot;Bahia&quot; , &quot;Minas Gerais&quot; , &quot;Espirito Santo&quot; , &quot;Rio de Janeiro&quot; , &quot;Sao Paulo&quot; , &quot;Parana&quot; , &quot;Santa Catarina&quot; , &quot;Rio Grande do Sul&quot; , &quot;Mato Grosso do Sul&quot; , &quot;Mato Grosso&quot; , &quot;Goias&quot; , &quot;Distrito Federal&quot; ) ) # define deflated effective working income variable: visita_df$vd4020_def &lt;- visita_df$vd4020 * visita_df$co2 # deflated effective per capita household income variable: visita_df$vd5002_def&lt;- visita_df$vd5002 * visita_df$co2 # lonely psu options(survey.lonely.psu = &quot;adjust&quot;) # preliminary survey design pre_stratified &lt;- svydesign(id=~upa, strata =~estrato , weights =~v1031, nest=T, data = visita_df) # post-stratification targets df.pos &lt;- data.frame(posest = as.character(unique(visita_df[,&quot;posest&quot;])), Freq = as.numeric(unique(visita_df[,&quot;v1030&quot;]))) # final survey design object visita_design &lt;- postStratify( pre_stratified, ~ posest, df.pos) visita_design &lt;- convey_prep(visita_design) # estimating Gini index based on the efective income from all jobs # defining the filter - ocupied people sub_visita_design &lt;- subset(visita_design, vd4018==1 &amp; vd4002==1 &amp; v2009&gt;=14) # for the whole country svygini(~ vd4020_def, sub_visita_design, na.rm=TRUE) # by regions svyby(~vd4020_def, ~region, sub_visita_design, svygini, na.rm=TRUE ) # by states svyby(~vd4020_def, ~state_name, sub_visita_design, svygini, na.rm=TRUE ) # estimating the Gini index based on the per capita household income # defining the filter - people in the households sub_visita_design&lt;-subset(visita_design, vd2002&lt;=14) # for the whole country svygini (~vd5002_def, sub_visita_design, na.rm=TRUE) # by regions svyby(~vd5002_def, ~region, sub_visita_design, svygini, na.rm=TRUE ) # by states svyby(~vd5002_def, ~state_name, sub_visita_design, svygini, na.rm=TRUE ) # to check the results see: # https://www.ibge.gov.br/estatisticas-novoportal/sociais/trabalho/17270-pnad-continua.html?edicao=20635&amp;t=resultados Replication Example nationwide_pop &lt;- svytotal( ~ pia , pnadc_design , na.rm = TRUE ) nationwide_forca &lt;- svytotal( ~ factor( vd4001 ) , pnadc_design , na.rm = TRUE ) nationwide_ocupacao &lt;- svytotal( ~ factor( vd4002 ) , pnadc_design , na.rm = TRUE ) regional_pop &lt;- svyby( ~ pia , ~ region , pnadc_design , svytotal , na.rm = TRUE ) regional_forca &lt;- svyby( ~ factor( vd4001 ) , ~ region , pnadc_design , svytotal , na.rm = TRUE ) regional_ocupacao &lt;- svyby( ~ factor( vd4002 ) , ~ region , pnadc_design , svytotal , na.rm = TRUE ) "],["pesquisa-nacional-de-saude-pns.html", "Pesquisa Nacional de Saude (PNS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Pesquisa Nacional de Saude (PNS) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; The Pesquisa Nacional de Saude (PNS) is Brazil’s healthcare survey. One table with one row per long-questionnaire respondent and a second table with one row for all respondents. A complex sample survey designed to generalize to Brazil’s civilian population. First released 2013. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available PNS microdata by simply specifying \"pns\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pns&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PNS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PNS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PNS microdata files pns_cat &lt;- get_catalog( &quot;pns&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PNS&quot; ) ) # download the microdata to your local computer pns_cat &lt;- lodown( &quot;pns&quot; , pns_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) pns_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PNS&quot; , &quot;2013 long questionnaire survey design.rds&quot; ) ) Variable Recoding Add new columns to the data set: pns_design &lt;- update( pns_design , one = 1 , health_insurance = as.numeric( i001 == 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pns_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ uf , pns_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pns_design ) svyby( ~ one , ~ uf , pns_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ w00101 , pns_design , na.rm = TRUE ) svyby( ~ w00101 , ~ uf , pns_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ c006 , pns_design ) svyby( ~ c006 , ~ uf , pns_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ w00101 , pns_design , na.rm = TRUE ) svyby( ~ w00101 , ~ uf , pns_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ c006 , pns_design ) svyby( ~ c006 , ~ uf , pns_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ w00101 , pns_design , 0.5 , na.rm = TRUE ) svyby( ~ w00101 , ~ uf , pns_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ w00203 , denominator = ~ w00101 , pns_design , na.rm = TRUE ) Subsetting Restrict the survey design to at least 30 minutes of physical activity: sub_pns_design &lt;- subset( pns_design , atfi04 == 1 ) Calculate the mean (average) of this subset: svymean( ~ w00101 , sub_pns_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ w00101 , pns_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ w00101 , ~ uf , pns_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pns_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ w00101 , pns_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ w00101 , pns_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ w00101 , pns_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ health_insurance , pns_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( w00101 ~ health_insurance , pns_design ) Perform a chi-squared test of association for survey data: svychisq( ~ health_insurance + c006 , pns_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( w00101 ~ health_insurance + c006 , pns_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for PNS users, this code replicates previously-presented examples: library(srvyr) pns_srvyr_design &lt;- as_survey( pns_design ) Calculate the mean (average) of a linear variable, overall and by groups: pns_srvyr_design %&gt;% summarize( mean = survey_mean( w00101 , na.rm = TRUE ) ) pns_srvyr_design %&gt;% group_by( uf ) %&gt;% summarize( mean = survey_mean( w00101 , na.rm = TRUE ) ) Replication Example "],["pesquisa-de-orcamentos-familiares-pof.html", "Pesquisa de Orcamentos Familiares (POF) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Pesquisa de Orcamentos Familiares (POF) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; The Pesquisa de Orcamentos Familiares is Brazil’s national survey of household budgets. One table of survey responses per sampled household. Additional tables, many containing one record per expenditure. A complex sample survey designed to generalize to the civilian population of Brazil. Released at irregular intervals, with only 2002-2003 and 2008-2009 microdata available. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available POF microdata by simply specifying \"pof\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;pof&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;POF&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the POF catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available POF microdata files pof_cat &lt;- get_catalog( &quot;pof&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;POF&quot; ) ) # 2008-2009 only pof_cat &lt;- subset( pof_cat , period == &quot;2008_2009&quot; ) # download the microdata to your local computer pof_cat &lt;- lodown( &quot;pof&quot; , pof_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) poststr &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;POF&quot; , &quot;2008_2009/poststr.rds&quot; ) ) t_morador_s &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;POF&quot; , &quot;2008_2009/t_morador_s.rds&quot; ) ) t_morador_s &lt;- transform( t_morador_s , control = paste0( cod_uf , num_seq , num_dv ) ) pof_df &lt;- merge( t_morador_s , poststr ) stopifnot( nrow( pof_df ) == nrow( t_morador_s ) ) pre_stratified_design &lt;- svydesign( id = ~control , strata = ~estrato_unico , weights = ~fator_expansao1 , data = pof_df , nest = TRUE ) population_totals &lt;- data.frame( pos_estrato = unique( pof_df$pos_estrato ) , Freq = unique( pof_df$tot_pop ) ) pof_design &lt;- postStratify( pre_stratified_design , ~ pos_estrato , population_totals ) Variable Recoding Add new columns to the data set: pof_design &lt;- update( pof_design , one = 1 , # centimeters instead of meters altura_imputado = altura_imputado / 100 , age_categories = factor( 1 + findInterval( idade_anos , c( 20 , 25 , 30 , 35 , 45 , 55 , 65 , 75 ) ) , levels = 1:9 , labels = c( &quot;under 20&quot; , &quot;20-24&quot; , &quot;25-29&quot; , &quot;30-34&quot; , &quot;35-44&quot; , &quot;45-54&quot; , &quot;55-64&quot; , &quot;65-74&quot; , &quot;75+&quot; ) ) , # create a body mass index (bmi) variable, excluding babies (who have altura_imputado==0) body_mass_index = ifelse( altura_imputado == 0 , 0 , peso_imputado / ( altura_imputado ^ 2 ) ) , sexo = ifelse( cod_sexo == &#39;01&#39; , &quot;masculino&quot; , ifelse( cod_sexo == &#39;02&#39; , &quot;feminino&quot; , NA ) ) ) pof_design &lt;- transform( pof_design , # individuals with a low bmi - underweight underweight = ifelse( body_mass_index &lt; 18.5 , 1 , 0 ) , # individuals with a high bmi - overweight overweight = ifelse( body_mass_index &gt;= 25 , 1 , 0 ) , # individuals with a very high bmi - obese obese = ifelse( body_mass_index &gt;= 30 , 1 , 0 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( pof_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ sexo , pof_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , pof_design ) svyby( ~ one , ~ sexo , pof_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ body_mass_index , pof_design , na.rm = TRUE ) svyby( ~ body_mass_index , ~ sexo , pof_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ age_categories , pof_design ) svyby( ~ age_categories , ~ sexo , pof_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ body_mass_index , pof_design , na.rm = TRUE ) svyby( ~ body_mass_index , ~ sexo , pof_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ age_categories , pof_design ) svyby( ~ age_categories , ~ sexo , pof_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ body_mass_index , pof_design , 0.5 , na.rm = TRUE ) svyby( ~ body_mass_index , ~ sexo , pof_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ peso_imputado , denominator = ~ altura_imputado , pof_design , na.rm = TRUE ) Subsetting Restrict the survey design to : sub_pof_design &lt;- subset( pof_design , underweight == 1 ) Calculate the mean (average) of this subset: svymean( ~ body_mass_index , sub_pof_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ body_mass_index , pof_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ body_mass_index , ~ sexo , pof_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( pof_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ body_mass_index , pof_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ body_mass_index , pof_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ body_mass_index , pof_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ obese , pof_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( body_mass_index ~ obese , pof_design ) Perform a chi-squared test of association for survey data: svychisq( ~ obese + age_categories , pof_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( body_mass_index ~ obese + age_categories , pof_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for POF users, this code replicates previously-presented examples: library(srvyr) pof_srvyr_design &lt;- as_survey( pof_design ) Calculate the mean (average) of a linear variable, overall and by groups: pof_srvyr_design %&gt;% summarize( mean = survey_mean( body_mass_index , na.rm = TRUE ) ) pof_srvyr_design %&gt;% group_by( sexo ) %&gt;% summarize( mean = survey_mean( body_mass_index , na.rm = TRUE ) ) Replication Example "],["panel-study-of-income-dynamics-psid.html", "Panel Study of Income Dynamics (PSID) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Panel Study of Income Dynamics (PSID) The Panel Study of Income Dynamics is the longest running longitudinal household survey in the world. One cross-year individual with one record per respondent in participating household, many family data tables with one record per family per timepoint. A complex sample survey designed to generalize to residents of the United States. Released either annually or biennially since 1968. Administered by the University of Michigan’s Institute for Social Research and funded by consortium. Simplified Download and Importation The R lodown package easily downloads and imports all available PSID microdata by simply specifying \"psid\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;psid&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PSID&quot; ) , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the PSID catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available PSID microdata files psid_cat &lt;- get_catalog( &quot;psid&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;PSID&quot; ) , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; ) # download the microdata to your local computer psid_cat &lt;- lodown( &quot;psid&quot; , psid_cat , your_email = &quot;email@address.com&quot; , your_password = &quot;password&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) # identify the cross-year individual filename cross_year_individual_rds &lt;- grep( &quot;cross-year individual&quot; , list.files( file.path( path.expand( &quot;~&quot; ) , &quot;PSID&quot; ) , recursive = TRUE , full.names = TRUE ) , value = TRUE ) individual_df &lt;- readRDS( cross_year_individual_rds ) ind_variables_to_keep &lt;- c( &#39;one&#39; , # column with all ones &#39;er30001&#39; , # 1968 interview number &#39;er30002&#39; , # 1968 person number &#39;er31997&#39; , # primary sampling unit variable &#39;er31996&#39; , # stratification variable &#39;er33801&#39; , # interview number, 2005 &#39;er34301&#39; , # interview number, 2015 &#39;er32000&#39; , # sex &#39;er34305&#39; , # age in 2015 &#39;er33813&#39; , # employment status in 2005 &#39;er34317&#39; , # employment status in 2015 &#39;er33848&#39; , # 2005 longitudinal weight &#39;er34413&#39; # 2015 longitudinal weight ) individual_df &lt;- individual_df[ ind_variables_to_keep ] ; gc() family_2005_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PSID&quot; , &quot;family files/2005.rds&quot; ) ) fam_2005_variables_to_keep &lt;- c( &#39;er25002&#39; , # 2005 interview number &#39;er28037&#39; # 2005 total family income ) family_2005_df &lt;- family_2005_df[ fam_2005_variables_to_keep ] ; gc() family_2015_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;PSID&quot; , &quot;family files/2015.rds&quot; ) ) fam_2015_variables_to_keep &lt;- c( &#39;er60002&#39; , # 2015 interview number &#39;er65349&#39; # 2015 total family income ) family_2015_df &lt;- family_2015_df[ fam_2015_variables_to_keep ] ; gc() ind_fam_2005 &lt;- merge( individual_df , family_2005_df , by.x = &#39;er33801&#39; , by.y = &#39;er25002&#39; ) ind_fam_2015 &lt;- merge( individual_df , family_2015_df , by.x = &#39;er34301&#39; , by.y = &#39;er60002&#39; ) psid_df &lt;- merge( ind_fam_2005 , ind_fam_2015 , all = TRUE ) psid_design &lt;- svydesign( ~ er31997 , strata = ~ er31996 , data = psid_df , weights = ~ er33848 , nest = TRUE ) Variable Recoding Add new columns to the data set: psid_design &lt;- update( psid_design , employment_2005 = factor( er33813 , levels = 1:8 , labels = c( &#39;working now&#39; , &#39;only temporarily laid off&#39; , &#39;looking for work, unemployed&#39; , &#39;retired&#39; , &#39;permanently disabled&#39; , &#39;housewife; keeping house&#39; , &#39;student&#39; , &#39;other&#39; ) ) , employed_in_2015 = factor( er34317 , levels = 1:8 , labels = c( &#39;working now&#39; , &#39;only temporarily laid off&#39; , &#39;looking for work, unemployed&#39; , &#39;retired&#39; , &#39;permanently disabled&#39; , &#39;housewife; keeping house&#39; , &#39;student&#39; , &#39;other&#39; ) ) , female = as.numeric( er32000 == 2 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( psid_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ employment_2005 , psid_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , psid_design ) svyby( ~ one , ~ employment_2005 , psid_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ er28037 , psid_design , na.rm = TRUE ) svyby( ~ er28037 , ~ employment_2005 , psid_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ employed_in_2015 , psid_design , na.rm = TRUE ) svyby( ~ employed_in_2015 , ~ employment_2005 , psid_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ er28037 , psid_design , na.rm = TRUE ) svyby( ~ er28037 , ~ employment_2005 , psid_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ employed_in_2015 , psid_design , na.rm = TRUE ) svyby( ~ employed_in_2015 , ~ employment_2005 , psid_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ er28037 , psid_design , 0.5 , na.rm = TRUE ) svyby( ~ er28037 , ~ employment_2005 , psid_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ er28037 , denominator = ~ er65349 , psid_design , na.rm = TRUE ) Subsetting Restrict the survey design to senior in 2015: sub_psid_design &lt;- subset( psid_design , er34305 &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ er28037 , sub_psid_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ er28037 , psid_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ er28037 , ~ employment_2005 , psid_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( psid_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ er28037 , psid_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ er28037 , psid_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ er28037 , psid_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ female , psid_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( er28037 ~ female , psid_design ) Perform a chi-squared test of association for survey data: svychisq( ~ female + employed_in_2015 , psid_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( er28037 ~ female + employed_in_2015 , psid_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for PSID users, this code replicates previously-presented examples: library(srvyr) psid_srvyr_design &lt;- as_survey( psid_design ) Calculate the mean (average) of a linear variable, overall and by groups: psid_srvyr_design %&gt;% summarize( mean = survey_mean( er28037 , na.rm = TRUE ) ) psid_srvyr_design %&gt;% group_by( employment_2005 ) %&gt;% summarize( mean = survey_mean( er28037 , na.rm = TRUE ) ) Replication Example "],["national-plan-and-provider-enumeration-system-saeb.html", "National Plan and Provider Enumeration System (SAEB) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr   Replication Example", " National Plan and Provider Enumeration System (SAEB) The National Plan and Provider Enumeration System (NPPES) contains information about every medical provider, insurance plan, and clearinghouse actively operating in the United States healthcare industry. A single large table with one row per enumerated health care provider. A census of individuals and organizations who bill for medical services in the United States. Updated monthly with new providers. Maintained by the United States Centers for Medicare &amp; Medicaid Services (CMS) Simplified Download and Importation The R lodown package easily downloads and imports all available SAEB microdata by simply specifying \"saeb\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;saeb&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SAEB&quot; ) ) Analysis Examples with base R   Load a data frame: column_names &lt;- names( read.csv( file.path( path.expand( &quot;~&quot; ) , &quot;SAEB&quot; , &quot;2015&quot; , &quot;escolas.csv&quot; ) , nrow = 1 )[ FALSE , , ] ) column_names &lt;- gsub( &quot;\\\\.&quot; , &quot;_&quot; , tolower( column_names ) ) column_types &lt;- ifelse( SAScii::parse.SAScii( file.path( path.expand( &quot;~&quot; ) , &quot;SAEB&quot; , &quot;2015&quot; , &quot;import.sas&quot; ) ) , &#39;n&#39; , &#39;c&#39; ) columns_to_import &lt;- c( &quot;entity_type_code&quot; , &quot;provider_gender_code&quot; , &quot;provider_enumeration_date&quot; , &quot;is_sole_proprietor&quot; , &quot;provider_business_practice_location_address_state_name&quot; ) stopifnot( all( columns_to_import %in% column_names ) ) saeb_df &lt;- data.frame( readr::read_csv( file.path( path.expand( &quot;~&quot; ) , &quot;SAEB&quot; , &quot;escolas.csv&quot; ) , col_names = columns_to_import , col_types = paste0( ifelse( column_names %in% columns_to_import , column_types , &#39;_&#39; ) , collapse = &quot;&quot; ) , skip = 1 ) ) Variable Recoding Add new columns to the data set: dbSendQuery( db , &quot;ALTER TABLE ADD COLUMN individual INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE SET individual = CASE WHEN entity_type_code = 1 THEN 1 ELSE 0 END&quot; ) dbSendQuery( db , &quot;ALTER TABLE ADD COLUMN provider_enumeration_year INTEGER&quot; ) dbSendQuery( db , &quot;UPDATE SET provider_enumeration_year = CAST( SUBSTRING( provider_enumeration_date , 7 , 10 ) AS INTEGER )&quot; ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( saeb_df ) table( saeb_df[ , &quot;provider_gender_code&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( saeb_df[ , &quot;provider_enumeration_year&quot; ] ) tapply( saeb_df[ , &quot;provider_enumeration_year&quot; ] , saeb_df[ , &quot;provider_gender_code&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( saeb_df[ , &quot;is_sole_proprietor&quot; ] ) ) prop.table( table( saeb_df[ , c( &quot;is_sole_proprietor&quot; , &quot;provider_gender_code&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( saeb_df[ , &quot;provider_enumeration_year&quot; ] ) tapply( saeb_df[ , &quot;provider_enumeration_year&quot; ] , saeb_df[ , &quot;provider_gender_code&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( saeb_df[ , &quot;provider_enumeration_year&quot; ] , 0.5 ) tapply( saeb_df[ , &quot;provider_enumeration_year&quot; ] , saeb_df[ , &quot;provider_gender_code&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to California: sub_saeb_df &lt;- subset( saeb_df , provider_business_practice_location_address_state_name = &#39;CA&#39; ) Calculate the mean (average) of this subset: mean( sub_saeb_df[ , &quot;provider_enumeration_year&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( saeb_df[ , &quot;provider_enumeration_year&quot; ] ) tapply( saeb_df[ , &quot;provider_enumeration_year&quot; ] , saeb_df[ , &quot;provider_gender_code&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( provider_enumeration_year ~ individual , saeb_df ) Perform a chi-squared test of association: this_table &lt;- table( saeb_df[ , c( &quot;individual&quot; , &quot;is_sole_proprietor&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( provider_enumeration_year ~ individual + is_sole_proprietor , data = saeb_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for SAEB users, this code replicates previously-presented examples: library(dplyr) saeb_tbl &lt;- tbl_df( saeb_df ) Calculate the mean (average) of a linear variable, overall and by groups: saeb_tbl %&gt;% summarize( mean = mean( provider_enumeration_year ) ) saeb_tbl %&gt;% group_by( provider_gender_code ) %&gt;% summarize( mean = mean( provider_enumeration_year ) ) Replication Example dbGetQuery( db , &quot;SELECT COUNT(*) FROM &quot; ) "],["survey-of-business-owners-sbo.html", "Survey of Business Owners (SBO) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " Survey of Business Owners (SBO) The Survey of Business Owners tracks nearly every tax-filing sole proprietorship, partnership, and corporation in the nation. One table with one row per firm per state per industry. A complex sample survey designed to generalize to all firms in the United States, however the public use microdata only includes classifiable (non-identifiable) firms which comprise nearly all businesses but only about half of workers. Released as part of the U.S. Census Bureau’s Economic Census, every year ending in 2 or 7. Administered by the U.S. Census Bureau. Simplified Download and Importation The R lodown package easily downloads and imports all available SBO microdata by simply specifying \"sbo\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;sbo&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SBO&quot; ) ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: gc() options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) library(mitools) sbo_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;SBO&quot; , &quot;2007 main.rds&quot; ) ) # keep only the variables you need variables_to_keep &lt;- c( &quot;one&quot; , &quot;newwgt&quot; , &quot;tabwgt&quot; , &quot;receipts_noisy&quot; , &quot;employment_noisy&quot; , &quot;n07_employer&quot; , &quot;established&quot; , &quot;healthins&quot; , &quot;husbwife&quot; ) # keep only columns used in this analysis sbo_design$coef$variables &lt;- sbo_design$coef$variables[ variables_to_keep ] sbo_design$var &lt;- lapply( sbo_design$var , function( w ){ w$variables &lt;- w$variables[ variables_to_keep ] w } ) gc() # this step conserves RAM Variable Recoding Add new columns to the data set: sbo_design &lt;- sbo_update( sbo_design , established_before_2000 = ifelse( established %in% c( &#39;0&#39; , &#39;A&#39; ) , NA , as.numeric( established &lt; 4 ) ) , healthins = factor( healthins , levels = 1:2 , labels = c( &quot;offered health insurance&quot; , &quot;did not offer health insurance&quot; ) ) ) gc() Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svyby( ~ one , ~ one , unwtd.count ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ one , ~ healthins , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svytotal( ~ one ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ one , ~ healthins , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ healthins , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svymean( ~ n07_employer , na.rm = TRUE ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ n07_employer , ~ healthins , svymean , na.rm = TRUE ) ) ) Calculate the sum of a linear variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svytotal( ~ receipts_noisy ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ healthins , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svytotal( ~ n07_employer , na.rm = TRUE ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ n07_employer , ~ healthins , svytotal , na.rm = TRUE ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: sbo_MIcombine( sbo_with( sbo_design , svyquantile( ~ receipts_noisy , 0.5 , se = TRUE ) ) ) sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ healthins , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: sbo_MIcombine( sbo_with( sbo_design , svyratio( numerator = ~ receipts_noisy , denominator = ~ employment_noisy ) ) ) Subsetting Restrict the survey design to jointly owned by husband and wife: sub_sbo_design &lt;- sbo_subset( sbo_design , husbwife %in% 1:3 ) Calculate the mean (average) of this subset: sbo_MIcombine( sbo_with( sub_sbo_design , svymean( ~ receipts_noisy ) ) ) ; rm( sub_sbo_design ) ; gc() Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svyby( ~ receipts_noisy , ~ healthins , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: sbo_degf( sbo_design ) Calculate the complex sample survey-adjusted variance of any statistic: sbo_MIcombine( sbo_with( sbo_design , svyvar( ~ receipts_noisy ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy , deff = TRUE ) ) ) # SRS with replacement sbo_MIcombine( sbo_with( sbo_design , svymean( ~ receipts_noisy , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: sbo_MIsvyciprop( ~ established_before_2000 , sbo_design , method = &quot;likelihood&quot; , na.rm = TRUE ) ; gc() Regression Models and Tests of Association Perform a design-based t-test: # not implemented sbo_MIsvyttest( receipts_noisy ~ established_before_2000 , sbo_design ) Perform a chi-squared test of association for survey data: # not implemented sbo_MIsvychisq( ~ established_before_2000 + n07_employer , sbo_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- sbo_MIcombine( sbo_with( sbo_design , svyglm( receipts_noisy ~ established_before_2000 + n07_employer ) ) ) glm_result Replication Example "],["survey-of-consumer-finances-scf.html", "Survey of Consumer Finances (SCF) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " Survey of Consumer Finances (SCF) The Survey of Consumer Finances (SCF) tracks the wealth of American families. Six thousand households answer a battery of questions about income, net worth, credit card debt, pensions, mortgages, even the lease on their cars. Plenty of surveys collect annual income, only the Survey of Consumer Finances captures such detailed asset data. One set of five tables of survey responses and a separate table with replicate weights, each table containing with one row per sampled household. The set of five tables of survey responses contain successive replicates of each sampled households, used to properly account for statistical uncertainty. A complex sample survey designed to generalize to the civilian non-institutional population of the United States. Released triennially since 1989. Administered by the Board of Governors of the Federal Reserve System. Simplified Download and Importation The R lodown package easily downloads and imports all available SCF microdata by simply specifying \"scf\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;scf&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SCF&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the SCF catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available SCF microdata files scf_cat &lt;- get_catalog( &quot;scf&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SCF&quot; ) ) # 2016 only scf_cat &lt;- subset( scf_cat , year == 2016 ) # download the microdata to your local computer scf_cat &lt;- lodown( &quot;scf&quot; , scf_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) scf_imp &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;SCF&quot; , &quot;scf 2016.rds&quot; ) ) scf_rw &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;SCF&quot; , &quot;scf 2016 rw.rds&quot; ) ) scf_design &lt;- svrepdesign( weights = ~wgt , repweights = scf_rw[ , -1 ] , data = imputationList( scf_imp ) , scale = 1 , rscales = rep( 1 / 998 , 999 ) , mse = FALSE , type = &quot;other&quot; , combined.weights = TRUE ) Variable Recoding Add new columns to the data set: scf_design &lt;- update( scf_design , hhsex = factor( hhsex , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , married = as.numeric( married == 1 ) , edcl = factor( edcl , labels = c( &quot;less than high school&quot; , &quot;high school or GED&quot; , &quot;some college&quot; , &quot;college degree&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: scf_MIcombine( with( scf_design , svyby( ~ one , ~ one , unwtd.count ) ) ) scf_MIcombine( with( scf_design , svyby( ~ one , ~ hhsex , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: scf_MIcombine( with( scf_design , svytotal( ~ one ) ) ) scf_MIcombine( with( scf_design , svyby( ~ one , ~ hhsex , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: scf_MIcombine( with( scf_design , svymean( ~ networth ) ) ) scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: scf_MIcombine( with( scf_design , svymean( ~ edcl ) ) ) scf_MIcombine( with( scf_design , svyby( ~ edcl , ~ hhsex , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: scf_MIcombine( with( scf_design , svytotal( ~ networth ) ) ) scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: scf_MIcombine( with( scf_design , svytotal( ~ edcl ) ) ) scf_MIcombine( with( scf_design , svyby( ~ edcl , ~ hhsex , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: scf_MIcombine( with( scf_design , svyquantile( ~ networth , 0.5 , se = TRUE , method = &#39;constant&#39; , interval.type = &#39;quantile&#39; ) ) ) scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svyquantile , 0.5 , se = TRUE , method = &#39;constant&#39; , interval.type = &#39;quantile&#39; , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: scf_MIcombine( with( scf_design , svyratio( numerator = ~ income , denominator = ~ networth ) ) ) Subsetting Restrict the survey design to labor force participants: sub_scf_design &lt;- subset( scf_design , lf == 1 ) Calculate the mean (average) of this subset: scf_MIcombine( with( sub_scf_design , svymean( ~ networth ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- scf_MIcombine( with( scf_design , svymean( ~ networth ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- scf_MIcombine( with( scf_design , svyby( ~ networth , ~ hhsex , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( scf_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: scf_MIcombine( with( scf_design , svyvar( ~ networth ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement scf_MIcombine( with( scf_design , svymean( ~ networth , deff = TRUE ) ) ) # SRS with replacement scf_MIcombine( with( scf_design , svymean( ~ networth , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ married , scf_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( networth ~ married , scf_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ married + edcl , scf_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- scf_MIcombine( with( scf_design , svyglm( networth ~ married + edcl ) ) ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for SCF users, this code calculates the gini coefficient on complex sample survey data: library(convey) scf_design$designs &lt;- lapply( scf_design$designs , convey_prep ) scf_MIcombine( with( scf_design , svygini( ~ networth ) ) ) Replication Example The statistics computed off of the public use file come very close to the net worth estimates in Table 2 of the bulletin but do not match exactly. A member of the SCF staff at the Federal Reserve re-ran the net worth calculations using the 2016 public use file to confirm that the calculations presented on this page follow the correct methodology. # compute mean net worth using the 2016 PUF mean_net_worth &lt;- scf_MIcombine( with( scf_design , svymean( ~ networth ) ) ) # confirm the estimate and standard error match FRB calculations within one dollar stopifnot( round( coef( mean_net_worth ) ) == 689509 ) stopifnot( round( SE( mean_net_worth ) ) == 12670 ) # compute median net worth using the 2016 PUF median_net_worth &lt;- scf_MIcombine( with( scf_design , svyquantile( ~ networth , 0.5 , se = TRUE , method = &#39;constant&#39; , interval.type = &#39;quantile&#39; , keep.var = TRUE , ci = TRUE ) ) ) # confirm the estimate and standard error match FRB calculations within one dollar stopifnot( round( coef( median_net_worth ) ) == 97306 ) stopifnot( round( SE( median_net_worth ) ) == 2699 ) "],["surveillance-epidemiology-and-end-results-seer.html", "Surveillance Epidemiology and End Results (SEER) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " Surveillance Epidemiology and End Results (SEER) The Surveillance Epidemiology and End Results (SEER) aggregates person-level information for more than a quarter of cancer incidence in the United States. A series of both individual- and population-level tables, grouped by site of cancer diagnosis. A registry covering various geographies across the US population, standardized by SEER*Stat to produce nationally-representative estimates. Updated every spring based on the previous November’s submission of data. Maintained by the United States National Cancer Institute (NCI) Simplified Download and Importation The R lodown package easily downloads and imports all available SEER microdata by simply specifying \"seer\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;seer&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SEER&quot; ) , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) Analysis Examples with base R   Load a data frame: available_files &lt;- list.files( file.path( path.expand( &quot;~&quot; ) , &quot;SEER&quot; ) , recursive = TRUE , full.names = TRUE ) seer_df &lt;- readRDS( grep( &quot;incidence(.*)yr1973(.*)LYMYLEUK&quot; , available_files , value = TRUE ) ) Variable Recoding Add new columns to the data set: seer_df &lt;- transform( seer_df , survival_months = ifelse( srv_time_mon == 9999 , NA , as.numeric( srv_time_mon ) ) , female = as.numeric( sex == 2 ) , race_ethnicity = ifelse( race1v == 99 , &quot;unknown&quot; , ifelse( nhiade &gt; 0 , &quot;hispanic&quot; , ifelse( race1v == 1 , &quot;white non-hispanic&quot; , ifelse( race1v == 2 , &quot;black non-hispanic&quot; , &quot;other non-hispanic&quot; ) ) ) ) , marital_status_at_dx = factor( as.numeric( mar_stat ) , levels = c( 1:6 , 9 ) , labels = c( &quot;single (never married)&quot; , &quot;married&quot; , &quot;separated&quot; , &quot;divorced&quot; , &quot;widowed&quot; , &quot;unmarried or domestic partner or unregistered&quot; , &quot;unknown&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( seer_df ) table( seer_df[ , &quot;race_ethnicity&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( seer_df[ , &quot;survival_months&quot; ] , na.rm = TRUE ) tapply( seer_df[ , &quot;survival_months&quot; ] , seer_df[ , &quot;race_ethnicity&quot; ] , mean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( seer_df[ , &quot;marital_status_at_dx&quot; ] ) ) prop.table( table( seer_df[ , c( &quot;marital_status_at_dx&quot; , &quot;race_ethnicity&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( seer_df[ , &quot;survival_months&quot; ] , na.rm = TRUE ) tapply( seer_df[ , &quot;survival_months&quot; ] , seer_df[ , &quot;race_ethnicity&quot; ] , sum , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( seer_df[ , &quot;survival_months&quot; ] , 0.5 , na.rm = TRUE ) tapply( seer_df[ , &quot;survival_months&quot; ] , seer_df[ , &quot;race_ethnicity&quot; ] , quantile , 0.5 , na.rm = TRUE ) Subsetting Limit your data.frame to inpatient hospital reporting source: sub_seer_df &lt;- subset( seer_df , rept_src == 1 ) Calculate the mean (average) of this subset: mean( sub_seer_df[ , &quot;survival_months&quot; ] , na.rm = TRUE ) Measures of Uncertainty Calculate the variance, overall and by groups: var( seer_df[ , &quot;survival_months&quot; ] , na.rm = TRUE ) tapply( seer_df[ , &quot;survival_months&quot; ] , seer_df[ , &quot;race_ethnicity&quot; ] , var , na.rm = TRUE ) Regression Models and Tests of Association Perform a t-test: t.test( survival_months ~ female , seer_df ) Perform a chi-squared test of association: this_table &lt;- table( seer_df[ , c( &quot;female&quot; , &quot;marital_status_at_dx&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( survival_months ~ female + marital_status_at_dx , data = seer_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for SEER users, this code replicates previously-presented examples: library(dplyr) seer_tbl &lt;- tbl_df( seer_df ) Calculate the mean (average) of a linear variable, overall and by groups: seer_tbl %&gt;% summarize( mean = mean( survival_months , na.rm = TRUE ) ) seer_tbl %&gt;% group_by( race_ethnicity ) %&gt;% summarize( mean = mean( survival_months , na.rm = TRUE ) ) "],["survey-of-health-ageing-and-retirement-in-europe-share.html", "Survey of Health, Ageing and Retirement in Europe (SHARE) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Survey of Health, Ageing and Retirement in Europe (SHARE) The Survey of Health, Ageing and Retirement in Europe interviews senior citizens across the continent for their entire life. Allows for findings like, “Among Belgians who were 50-74 years old in 2004, X% lived in nursing homes by 2010.” Many tables, most with one row per sampled respondent for the period. A complex sample longitudinal survey designed to generalize to the civilian, non-institutionalized population of participating European countries aged 50 or older. Released every two or three years since 2004. Coordinated at the Max Planck Institute and funded by consortium. Simplified Download and Importation The R lodown package easily downloads and imports all available SHARE microdata by simply specifying \"share\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;share&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SHARE&quot; ) , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the SHARE catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available SHARE microdata files share_cat &lt;- get_catalog( &quot;share&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SHARE&quot; ) , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) # wave 1, wave 6, and longitudinal weights only share_cat &lt;- subset( share_cat , grepl( &quot;ave 1|ave 6|ongitudinal&quot; , output_folder ) ) # download the microdata to your local computer share_cat &lt;- lodown( &quot;share&quot; , share_cat , your_username = &quot;username&quot; , your_password = &quot;password&quot; ) Analysis Examples with the survey library   Construct a complex sample survey design: options( survey.lonely.psu = &quot;adjust&quot; ) library(survey) available_files &lt;- list.files( file.path( path.expand( &quot;~&quot; ) , &quot;SHARE&quot; ) , recursive = TRUE , full.names = TRUE ) # wave six demographics file share_dn6_df &lt;- readRDS( grep( &quot;6\\\\.[0-9]\\\\.[0-9](.*)sharew6(.*)dn\\\\.rds&quot; , available_files , value = TRUE ) ) share_dn6_df &lt;- share_dn6_df[ c( &quot;mergeid&quot; , &quot;country&quot; , &quot;dn042_&quot; , &quot;dn004_&quot; ) ] # wave six physical health file share_ph1_df &lt;- readRDS( grep( &quot;sharew1(.*)ph\\\\.rds&quot; , available_files , value = TRUE ) ) share_ph1_df$weight_in_2004 &lt;- ifelse( share_ph1_df$ph012_ &lt; 0 , NA , share_ph1_df$ph012_ ) share_ph1_df &lt;- share_ph1_df[ c( &quot;mergeid&quot; , &quot;weight_in_2004&quot; , &quot;ph005_&quot; ) ] # wave six physical health file share_ph6_df &lt;- readRDS( grep( &quot;6\\\\.1\\\\.0(.*)sharew6(.*)ph\\\\.rds&quot; , available_files , value = TRUE ) ) share_ph6_df$weight_in_2015 &lt;- ifelse( share_ph6_df$ph012_ &lt; 0 , NA , share_ph6_df$ph012_ ) share_ph6_df &lt;- share_ph6_df[ c( &quot;mergeid&quot; , &quot;weight_in_2015&quot; , &quot;ph003_&quot; ) ] # longitudinal weights file share_longwt_df &lt;- readRDS( grep( &quot;longitudinal_weights_w1\\\\-(.*)\\\\.rds&quot; , available_files , value = TRUE ) ) # france only longitudinal weights france_df &lt;- subset( share_longwt_df , country == 17 &amp; ( cliw_a &gt; 0 ) ) nrow_check &lt;- nrow( france_df ) # merge on each of the tables france_df &lt;- merge( france_df , share_dn6_df ) france_df &lt;- merge( france_df , share_ph1_df ) france_df &lt;- merge( france_df , share_ph6_df ) # confirm no change in records stopifnot( nrow( france_df ) == nrow_check ) share_design &lt;- svydesign( ~ psu + ssu , strata = ~ stratum1 + stratum2 , data = france_df , weights = ~ cliw_a , nest = TRUE ) Variable Recoding Add new columns to the data set: share_design &lt;- update( share_design , one = 1 , sexe = factor( dn042_ , levels = 1:2 , labels = c( &#39;masculin&#39; , &#39;feminin&#39; ) ) , health_in_general_2015 = factor( ph003_ , levels = 1:5 , labels = c( &quot;excellente&quot; , &quot;tres bonne&quot; , &quot;bonne&quot; , &quot;acceptable&quot; , &quot;mediocre&quot; ) ) , fortemente_limite_2004 = ifelse( ph005_ %in% 1:3 , as.numeric( ph005_ == 1 ) , NA ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( share_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ sexe , share_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , share_design ) svyby( ~ one , ~ sexe , share_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ weight_in_2015 , share_design , na.rm = TRUE ) svyby( ~ weight_in_2015 , ~ sexe , share_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ health_in_general_2015 , share_design , na.rm = TRUE ) svyby( ~ health_in_general_2015 , ~ sexe , share_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ weight_in_2015 , share_design , na.rm = TRUE ) svyby( ~ weight_in_2015 , ~ sexe , share_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ health_in_general_2015 , share_design , na.rm = TRUE ) svyby( ~ health_in_general_2015 , ~ sexe , share_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ weight_in_2015 , share_design , 0.5 , na.rm = TRUE ) svyby( ~ weight_in_2015 , ~ sexe , share_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ weight_in_2015 , denominator = ~ weight_in_2004 , share_design , na.rm = TRUE ) Subsetting Restrict the survey design to persons born in france: sub_share_design &lt;- subset( share_design , dn004_ == 1 ) Calculate the mean (average) of this subset: svymean( ~ weight_in_2015 , sub_share_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ weight_in_2015 , share_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ weight_in_2015 , ~ sexe , share_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( share_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ weight_in_2015 , share_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ weight_in_2015 , share_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ weight_in_2015 , share_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ fortemente_limite_2004 , share_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( weight_in_2015 ~ fortemente_limite_2004 , share_design ) Perform a chi-squared test of association for survey data: svychisq( ~ fortemente_limite_2004 + health_in_general_2015 , share_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( weight_in_2015 ~ fortemente_limite_2004 + health_in_general_2015 , share_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for SHARE users, this code replicates previously-presented examples: library(srvyr) share_srvyr_design &lt;- as_survey( share_design ) Calculate the mean (average) of a linear variable, overall and by groups: share_srvyr_design %&gt;% summarize( mean = survey_mean( weight_in_2015 , na.rm = TRUE ) ) share_srvyr_design %&gt;% group_by( sexe ) %&gt;% summarize( mean = survey_mean( weight_in_2015 , na.rm = TRUE ) ) Replication Example "],["pesquisa-nacional-por-amostra-de-domicilios-sipp.html", "Pesquisa Nacional por Amostra de Domicilios (SIPP) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " Pesquisa Nacional por Amostra de Domicilios (SIPP) Contributed by Dr. Djalma Pessoa &lt;pessoad@gmail.com&gt; Brazil’s previous principal household survey, the Pesquisa Nacional por Amostra de Domicilios (PNAD) measures general education, labor, income, and housing characteristics of the population. One table with one row per sampled household and a second table with one row per individual within each sampled household. A complex sample survey designed to generalize to the civilian non-institutional population of Brazil, although the rural north was not included prior to 2004. Released annually since 2001 except for years ending in zero, when the decennial census takes its place. Administered by the Instituto Brasileiro de Geografia e Estatistica. Simplified Download and Importation The R lodown package easily downloads and imports all available SIPP microdata by simply specifying \"sipp\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;sipp&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SIPP&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the SIPP catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available SIPP microdata files sipp_cat &lt;- get_catalog( &quot;sipp&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SIPP&quot; ) ) # 2011 only sipp_cat &lt;- subset( sipp_cat , year == 2011 ) # download the microdata to your local computer sipp_cat &lt;- lodown( &quot;sipp&quot; , sipp_cat ) Analysis Examples with the survey library   Construct a database-backed complex sample survey design: library(DBI) library(RSQLite) library(survey) options( survey.lonely.psu = &quot;adjust&quot; ) prestratified_design &lt;- svydesign( id = ~v4618 , strata = ~v4617 , data = sipp_cat[ 1 , &quot;db_tablename&quot; ] , weights = ~pre_wgt , nest = TRUE , dbtype = &quot;SQLite&quot; , dbname = sipp_cat[ 1 , &quot;dbfile&quot; ] ) sipp_design &lt;- lodown:::pnad_postStratify( design = prestratified_design , strata.col = &#39;v4609&#39; , oldwgt = &#39;pre_wgt&#39; ) Variable Recoding Add new columns to the data set: sipp_design &lt;- update( sipp_design , age_categories = factor( 1 + findInterval( v8005 , seq( 5 , 60 , 5 ) ) ) , male = as.numeric( v0302 == 2 ) , teenagers = as.numeric( v8005 &gt; 12 &amp; v8005 &lt; 20 ) , started_working_before_thirteen = as.numeric( v9892 &lt; 13 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( sipp_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ region , sipp_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , sipp_design ) svyby( ~ one , ~ region , sipp_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ v4720 , sipp_design , na.rm = TRUE ) svyby( ~ v4720 , ~ region , sipp_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ age_categories , sipp_design ) svyby( ~ age_categories , ~ region , sipp_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ v4720 , sipp_design , na.rm = TRUE ) svyby( ~ v4720 , ~ region , sipp_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ age_categories , sipp_design ) svyby( ~ age_categories , ~ region , sipp_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ v4720 , sipp_design , 0.5 , na.rm = TRUE ) svyby( ~ v4720 , ~ region , sipp_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ started_working_before_thirteen , denominator = ~ teenagers , sipp_design , na.rm = TRUE ) Subsetting Restrict the survey design to married persons: sub_sipp_design &lt;- subset( sipp_design , v4011 == 1 ) Calculate the mean (average) of this subset: svymean( ~ v4720 , sub_sipp_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ v4720 , sipp_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ v4720 , ~ region , sipp_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( sipp_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ v4720 , sipp_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ v4720 , sipp_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ v4720 , sipp_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ male , sipp_design , method = &quot;likelihood&quot; ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( v4720 ~ male , sipp_design ) Perform a chi-squared test of association for survey data: svychisq( ~ male + age_categories , sipp_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( v4720 ~ male + age_categories , sipp_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for SIPP users, this code calculates the gini coefficient on complex sample survey data: library(convey) sipp_design &lt;- convey_prep( sipp_design ) sub_sipp_design &lt;- subset( sipp_design , !is.na( v4720 ) &amp; v4720 != 0 &amp; v8005 &gt;= 15 ) svygini( ~ v4720 , sub_sipp_design , na.rm = TRUE ) Replication Example svytotal( ~one , sipp_design ) svytotal( ~factor( v0302 ) , sipp_design ) cv( svytotal( ~factor( v0302 ) , sipp_design ) ) "],["social-security-administration-public-use-microdata-ssa.html", "Social Security Administration Public Use Microdata (SSA) Simplified Download and Importation Analysis Examples with base R   Analysis Examples with dplyr  ", " Social Security Administration Public Use Microdata (SSA) Research extracts provided by the Social Security Administration. Tables contain either one record per person or one record per person per year. The entire population of either social security number holders (most of the country) or social security recipients (just beneficiaries). One-percent samples should be multiplied by 100 to get accurate nationwide count statistics, five-percent samples by 20. No expected release timeline. Released by the United States Social Security Administration (SSA). Simplified Download and Importation The R lodown package easily downloads and imports all available SSA microdata by simply specifying \"ssa\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;ssa&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;SSA&quot; ) ) Analysis Examples with base R   Load a data frame: ssa_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;SSA&quot; , &quot;ssr_data/SSIPUF.rds&quot; ) ) Variable Recoding Add new columns to the data set: ssa_df &lt;- transform( ssa_df , mental_disorder = as.numeric( diag %in% 1:2 ) , program_eligibility = factor( prel , levels = 0:5 , labels = c( &quot;Unspecified&quot; , &quot;Aged individual&quot; , &quot;Aged spouse&quot; , &quot;Disabled or blind individual&quot; , &quot;Disabled or blind spouse&quot; , &quot;Disabled or blind child&quot; ) ) ) Unweighted Counts Count the unweighted number of records in the table, overall and by groups: nrow( ssa_df ) table( ssa_df[ , &quot;stat&quot; ] , useNA = &quot;always&quot; ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: mean( ssa_df[ , &quot;fpmt&quot; ] ) tapply( ssa_df[ , &quot;fpmt&quot; ] , ssa_df[ , &quot;stat&quot; ] , mean ) Calculate the distribution of a categorical variable, overall and by groups: prop.table( table( ssa_df[ , &quot;program_eligibility&quot; ] ) ) prop.table( table( ssa_df[ , c( &quot;program_eligibility&quot; , &quot;stat&quot; ) ] ) , margin = 2 ) Calculate the sum of a linear variable, overall and by groups: sum( ssa_df[ , &quot;fpmt&quot; ] ) tapply( ssa_df[ , &quot;fpmt&quot; ] , ssa_df[ , &quot;stat&quot; ] , sum ) Calculate the median (50th percentile) of a linear variable, overall and by groups: quantile( ssa_df[ , &quot;fpmt&quot; ] , 0.5 ) tapply( ssa_df[ , &quot;fpmt&quot; ] , ssa_df[ , &quot;stat&quot; ] , quantile , 0.5 ) Subsetting Limit your data.frame to females: sub_ssa_df &lt;- subset( ssa_df , sex == &quot;F&quot; ) Calculate the mean (average) of this subset: mean( sub_ssa_df[ , &quot;fpmt&quot; ] ) Measures of Uncertainty Calculate the variance, overall and by groups: var( ssa_df[ , &quot;fpmt&quot; ] ) tapply( ssa_df[ , &quot;fpmt&quot; ] , ssa_df[ , &quot;stat&quot; ] , var ) Regression Models and Tests of Association Perform a t-test: t.test( fpmt ~ mental_disorder , ssa_df ) Perform a chi-squared test of association: this_table &lt;- table( ssa_df[ , c( &quot;mental_disorder&quot; , &quot;program_eligibility&quot; ) ] ) chisq.test( this_table ) Perform a generalized linear model: glm_result &lt;- glm( fpmt ~ mental_disorder + program_eligibility , data = ssa_df ) summary( glm_result ) Analysis Examples with dplyr   The R dplyr library offers an alternative grammar of data manipulation to base R and SQL syntax. dplyr offers many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, and the tidyverse style of non-standard evaluation. This vignette details the available features. As a starting point for SSA users, this code replicates previously-presented examples: library(dplyr) ssa_tbl &lt;- tbl_df( ssa_df ) Calculate the mean (average) of a linear variable, overall and by groups: ssa_tbl %&gt;% summarize( mean = mean( fpmt ) ) ssa_tbl %&gt;% group_by( stat ) %&gt;% summarize( mean = mean( fpmt ) ) "],["trends-in-international-mathematics-and-science-study-timss.html", "Trends in International Mathematics and Science Study (TIMSS) Simplified Download and Importation Analysis Examples with the survey library   Replication Example", " Trends in International Mathematics and Science Study (TIMSS) The Trends in International Mathematics and Science Study (TIMSS) tracks the math and science competency of fourth graders across about fifty nations. A series of tables with one record per school (ACG), per student (ASG), per teacher (ATG), as well as files containing student achievement (ASA), home background (ASH), student-teacher linkage (AST), and within-country scoring reliability (ASR). A complex sample survey designed to generalize to the fourth-grade student population of participating countries. Released quadrennially since 1995. Funded by the International Association for the Evaluation of Educational Achievement and compiled by the Lynch School of Education at Boston College. Simplified Download and Importation The R lodown package easily downloads and imports all available TIMSS microdata by simply specifying \"timss\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;timss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;TIMSS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the TIMSS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available TIMSS microdata files timss_cat &lt;- get_catalog( &quot;timss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;TIMSS&quot; ) ) # 2015 only timss_cat &lt;- subset( timss_cat , year == 2015 ) # download the microdata to your local computer timss_cat &lt;- lodown( &quot;timss&quot; , timss_cat ) Analysis Examples with the survey library   Construct a multiply-imputed, complex sample survey design: library(survey) library(mitools) library(RSQLite) # load the ASG (student background) + ASH (home background) merged design timss_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;TIMSS&quot; , &quot;2015/asg_design.rds&quot; ) ) design_weights &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;TIMSS&quot; , &quot;2015/asg_weights.rds&quot; ) ) five_tablenames &lt;- paste0( &quot;asg_2015_&quot; , 1:5 ) timss_design &lt;- lodown:::svyMDBdesign( timss_design ) Variable Recoding Add new columns to the data set: timss_design &lt;- update( timss_design , one = 1 , idcntry = factor( idcntry ) , sex = factor( itsex , labels = c( &quot;male&quot; , &quot;female&quot; ) ) , born_2005_or_later = as.numeric( itbirthy &gt;= 2005 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: MIcombine( with( timss_design , svyby( ~ one , ~ one , unwtd.count ) ) ) MIcombine( with( timss_design , svyby( ~ one , ~ sex , unwtd.count ) ) ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: MIcombine( with( timss_design , svytotal( ~ one ) ) ) MIcombine( with( timss_design , svyby( ~ one , ~ sex , svytotal ) ) ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: MIcombine( with( timss_design , svymean( ~ asmmat ) ) ) MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svymean ) ) ) Calculate the distribution of a categorical variable, overall and by groups: MIcombine( with( timss_design , svymean( ~ idcntry ) ) ) MIcombine( with( timss_design , svyby( ~ idcntry , ~ sex , svymean ) ) ) Calculate the sum of a linear variable, overall and by groups: MIcombine( with( timss_design , svytotal( ~ asmmat ) ) ) MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svytotal ) ) ) Calculate the weighted sum of a categorical variable, overall and by groups: MIcombine( with( timss_design , svytotal( ~ idcntry ) ) ) MIcombine( with( timss_design , svyby( ~ idcntry , ~ sex , svytotal ) ) ) Calculate the median (50th percentile) of a linear variable, overall and by groups: MIcombine( with( timss_design , svyquantile( ~ asmmat , 0.5 , se = TRUE ) ) ) MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svyquantile , 0.5 , se = TRUE , keep.var = TRUE , ci = TRUE ) ) ) Estimate a ratio: MIcombine( with( timss_design , svyratio( numerator = ~ asssci , denominator = ~ asmmat ) ) ) Subsetting Restrict the survey design to Australia, Austria, Azerbaijan, Belgium (French): sub_timss_design &lt;- subset( timss_design , idcntry %in% c( 36 , 40 , 31 , 957 ) ) Calculate the mean (average) of this subset: MIcombine( with( sub_timss_design , svymean( ~ asmmat ) ) ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- MIcombine( with( timss_design , svymean( ~ asmmat ) ) ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- MIcombine( with( timss_design , svyby( ~ asmmat , ~ sex , svymean ) ) ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( timss_design$designs[[1]] ) Calculate the complex sample survey-adjusted variance of any statistic: MIcombine( with( timss_design , svyvar( ~ asmmat ) ) ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement MIcombine( with( timss_design , svymean( ~ asmmat , deff = TRUE ) ) ) # SRS with replacement MIcombine( with( timss_design , svymean( ~ asmmat , deff = &quot;replace&quot; ) ) ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: MIsvyciprop( ~ born_2001_or_later , timss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: MIsvyttest( asmmat ~ born_2001_or_later , timss_design ) Perform a chi-squared test of association for survey data: MIsvychisq( ~ born_2001_or_later + idcntry , timss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- MIcombine( with( timss_design , svyglm( asmmat ~ born_2001_or_later + idcntry ) ) ) summary( glm_result ) Replication Example "],["united-states-decennial-census-public-use-microdata-sample-uspums.html", "United States Decennial Census Public Use Microdata Sample (USPUMS) Simplified Download and Importation Analysis Examples with the survey library   Poverty and Inequality Estimation with convey   Replication Example", " United States Decennial Census Public Use Microdata Sample (USPUMS) The Long-Form Decennial Census of the United States. One table with one row per household and a second table with one row per individual within each household. 1990 and 2000 include both 1% and 5% samples. 2010 contains only a 10% sample. An enumeration of the civilian population of the United States. Released decennially by the United States Census Bureau since 1990, however earlier extracts are available from IPUMS International. Administered by the US Census Bureau. Simplified Download and Importation The R lodown package easily downloads and imports all available USPUMS microdata by simply specifying \"uspums\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;uspums&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;USPUMS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the USPUMS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available USPUMS microdata files uspums_cat &lt;- get_catalog( &quot;uspums&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;USPUMS&quot; ) ) # 2000 1% sample only uspums_cat &lt;- subset( uspums_cat , year == 2000 &amp; percent == 1 ) # download the microdata to your local computer uspums_cat &lt;- lodown( &quot;uspums&quot; , uspums_cat ) Analysis Examples with the survey library   Construct a database-backed complex sample survey design: library(DBI) library(RSQLite) library(survey) options( survey.lonely.psu = &quot;adjust&quot; ) uspums_design &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;USPUMS&quot; , &quot;pums_2000_1_m.rds&quot; ) ) uspums_design &lt;- open( uspums_design , driver = SQLite() ) Variable Recoding Add new columns to the data set: uspums_design &lt;- update( uspums_design , age_categories = factor( 1 + findInterval( age , c( 18 , 35 , 65 ) ) , labels = c( &quot;under 18&quot; , &quot;18-34&quot; , &quot;35-64&quot; , &quot;65+&quot; ) ) , married = as.numeric( marstat == 1 ) , poverty_status = ifelse( poverty == 0 , NA , poverty ) , unemployed = as.numeric( esr %in% 3 ) , labor_force = as.numeric( esr %in% 1:5 ) , employment_status = factor( esr , levels = 0:6 , labels = c( &quot;NIU&quot; , &quot;Employed, at work&quot; , &quot;Employed, with a job but not at work&quot; , &quot;Unemployed&quot; , &quot;Armed Forces, at work&quot; , &quot;Armed Forces, with a job but not at work&quot; , &quot;Not in labor force&quot; ) ) , state_name = factor( state , levels = c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 66, 72, 78) , labels = c(&quot;ALABAMA&quot;, &quot;ALASKA&quot;, &quot;ARIZONA&quot;, &quot;ARKANSAS&quot;, &quot;CALIFORNIA&quot;, &quot;COLORADO&quot;, &quot;CONNECTICUT&quot;, &quot;DELAWARE&quot;, &quot;DISTRICT OF COLUMBIA&quot;, &quot;FLORIDA&quot;, &quot;GEORGIA&quot;, &quot;HAWAII&quot;, &quot;IDAHO&quot;, &quot;ILLINOIS&quot;, &quot;INDIANA&quot;, &quot;IOWA&quot;, &quot;KANSAS&quot;, &quot;KENTUCKY&quot;, &quot;LOUISIANA&quot;, &quot;MAINE&quot;, &quot;MARYLAND&quot;, &quot;MASSACHUSETTS&quot;, &quot;MICHIGAN&quot;, &quot;MINNESOTA&quot;, &quot;MISSISSIPPI&quot;, &quot;MISSOURI&quot;, &quot;MONTANA&quot;, &quot;NEBRASKA&quot;, &quot;NEVADA&quot;, &quot;NEW HAMPSHIRE&quot;, &quot;NEW JERSEY&quot;, &quot;NEW MEXICO&quot;, &quot;NEW YORK&quot;, &quot;NORTH CAROLINA&quot;, &quot;NORTH DAKOTA&quot;, &quot;OHIO&quot;, &quot;OKLAHOMA&quot;, &quot;OREGON&quot;, &quot;PENNSYLVANIA&quot;, &quot;RHODE ISLAND&quot;, &quot;SOUTH CAROLINA&quot;, &quot;SOUTH DAKOTA&quot;, &quot;TENNESSEE&quot;, &quot;TEXAS&quot;, &quot;UTAH&quot;, &quot;VERMONT&quot;, &quot;VIRGINIA&quot;, &quot;WASHINGTON&quot;, &quot;WEST VIRGINIA&quot;, &quot;WISCONSIN&quot;, &quot;WYOMING&quot;, &quot;GUAM&quot;, &quot;PUERTO RICO&quot;, &quot;U.S. VIRGIN ISLANDS&quot;) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( uspums_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ state_name , uspums_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , uspums_design ) svyby( ~ one , ~ state_name , uspums_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ poverty_status , uspums_design , na.rm = TRUE ) svyby( ~ poverty_status , ~ state_name , uspums_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ employment_status , uspums_design ) svyby( ~ employment_status , ~ state_name , uspums_design , svymean ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ poverty_status , uspums_design , na.rm = TRUE ) svyby( ~ poverty_status , ~ state_name , uspums_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ employment_status , uspums_design ) svyby( ~ employment_status , ~ state_name , uspums_design , svytotal ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ poverty_status , uspums_design , 0.5 , na.rm = TRUE ) svyby( ~ poverty_status , ~ state_name , uspums_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ unemployed , denominator = ~ labor_force , uspums_design , na.rm = TRUE ) Subsetting Restrict the survey design to females: sub_uspums_design &lt;- subset( uspums_design , sex == 2 ) Calculate the mean (average) of this subset: svymean( ~ poverty_status , sub_uspums_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ poverty_status , uspums_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ poverty_status , ~ state_name , uspums_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( uspums_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ poverty_status , uspums_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ poverty_status , uspums_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ poverty_status , uspums_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ married , uspums_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( poverty_status ~ married , uspums_design ) Perform a chi-squared test of association for survey data: svychisq( ~ married + employment_status , uspums_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( poverty_status ~ married + employment_status , uspums_design ) summary( glm_result ) Poverty and Inequality Estimation with convey   The R convey library estimates measures of income concentration, poverty, inequality, and wellbeing. This textbook details the available features. As a starting point for USPUMS users, this code calculates the gini coefficient on complex sample survey data: library(convey) uspums_design &lt;- convey_prep( uspums_design ) svygini( ~ hinc , uspums_design , na.rm = TRUE ) Replication Example "],["world-values-survey-wvs.html", "World Values Survey (WVS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " World Values Survey (WVS) The World Values Survey studies changing values and their impact on social and political life in almost one hundred nations. One table per country per wave, with one row per sampled respondent. A complex sample survey designed to generalize the population aged eighteen and older in participating countries. Released about twice per decade since 1981. Administered as a confederacy, guided by a scientific advisory committee and funded by consortium. Simplified Download and Importation The R lodown package easily downloads and imports all available WVS microdata by simply specifying \"wvs\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;wvs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;WVS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the WVS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available WVS microdata files wvs_cat &lt;- get_catalog( &quot;wvs&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;WVS&quot; ) ) # wave six only wvs_cat &lt;- subset( wvs_cat , grepl( &quot;United(.*)States&quot; , full_url ) &amp; wave == 6 ) # download the microdata to your local computer wvs_cat &lt;- lodown( &quot;wvs&quot; , wvs_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) wvs_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;WVS&quot; , &quot;wave 6/F00003106-WV6_Data_United_States_2011_spss_v_2016-01-01.rds&quot; ) ) # construct a fake survey design warning( &quot;this survey design produces correct point estimates but incorrect standard errors.&quot; ) wvs_design &lt;- svydesign( ~ 1 , data = wvs_df , weights = ~ v258 ) Variable Recoding Add new columns to the data set: wvs_design &lt;- update( wvs_design , one = 1 , language_spoken_at_home = factor( v247 , levels = c( 101 , 128 , 144 , 208 , 426 , 800 ) , labels = c( &#39;chinese&#39; , &#39;english&#39; , &#39;french&#39; , &#39;japanese&#39; , &#39;spanish; castilian&#39; , &#39;other&#39; ) ) , citizen = as.numeric( v246 == 1 ) , task_creativity_1_10 = as.numeric( v232 ) , work_independence_1_10 = as.numeric( v233 ) , family_importance = factor( v4 , labels = c( &#39;very&#39; , &#39;rather&#39; , &#39;not very&#39; , &#39;not at all&#39; ) ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( wvs_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ language_spoken_at_home , wvs_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , wvs_design ) svyby( ~ one , ~ language_spoken_at_home , wvs_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE ) svyby( ~ task_creativity_1_10 , ~ language_spoken_at_home , wvs_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ family_importance , wvs_design , na.rm = TRUE ) svyby( ~ family_importance , ~ language_spoken_at_home , wvs_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE ) svyby( ~ task_creativity_1_10 , ~ language_spoken_at_home , wvs_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ family_importance , wvs_design , na.rm = TRUE ) svyby( ~ family_importance , ~ language_spoken_at_home , wvs_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ task_creativity_1_10 , wvs_design , 0.5 , na.rm = TRUE ) svyby( ~ task_creativity_1_10 , ~ language_spoken_at_home , wvs_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ task_creativity_1_10 , denominator = ~ work_independence_1_10 , wvs_design , na.rm = TRUE ) Subsetting Restrict the survey design to seniors: sub_wvs_design &lt;- subset( wvs_design , v242 &gt;= 65 ) Calculate the mean (average) of this subset: svymean( ~ task_creativity_1_10 , sub_wvs_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ task_creativity_1_10 , ~ language_spoken_at_home , wvs_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( wvs_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ task_creativity_1_10 , wvs_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ citizen , wvs_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( task_creativity_1_10 ~ citizen , wvs_design ) Perform a chi-squared test of association for survey data: svychisq( ~ citizen + family_importance , wvs_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( task_creativity_1_10 ~ citizen + family_importance , wvs_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for WVS users, this code replicates previously-presented examples: library(srvyr) wvs_srvyr_design &lt;- as_survey( wvs_design ) Calculate the mean (average) of a linear variable, overall and by groups: wvs_srvyr_design %&gt;% summarize( mean = survey_mean( task_creativity_1_10 , na.rm = TRUE ) ) wvs_srvyr_design %&gt;% group_by( language_spoken_at_home ) %&gt;% summarize( mean = survey_mean( task_creativity_1_10 , na.rm = TRUE ) ) Replication Example "],["youth-risk-behavior-surveillance-system-yrbss.html", "Youth Risk Behavior Surveillance System (YRBSS) Simplified Download and Importation Analysis Examples with the survey library   Analysis Examples with srvyr   Replication Example", " Youth Risk Behavior Surveillance System (YRBSS) The Youth Risk Behavior Surveillance System is the high school edition of the Behavioral Risk Factor Surveillance System (BRFSS), a scientific study of good kids who do bad things. One table with one row per sampled youth respondent. A complex sample survey designed to generalize to all public and private school students in grades 9-12 in the United States. Released biennially since 1993. Administered by the Centers for Disease Control and Prevention. Simplified Download and Importation The R lodown package easily downloads and imports all available YRBSS microdata by simply specifying \"yrbss\" with an output_dir = parameter in the lodown() function. Depending on your internet connection and computer processing speed, you might prefer to run this step overnight. library(lodown) lodown( &quot;yrbss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; ) ) lodown also provides a catalog of available microdata extracts with the get_catalog() function. After requesting the YRBSS catalog, you could pass a subsetted catalog through the lodown() function in order to download and import specific extracts (rather than all available extracts). library(lodown) # examine all available YRBSS microdata files yrbss_cat &lt;- get_catalog( &quot;yrbss&quot; , output_dir = file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; ) ) # 2015 only yrbss_cat &lt;- subset( yrbss_cat , year == 2015 ) # download the microdata to your local computer yrbss_cat &lt;- lodown( &quot;yrbss&quot; , yrbss_cat ) Analysis Examples with the survey library   Construct a complex sample survey design: library(survey) yrbss_df &lt;- readRDS( file.path( path.expand( &quot;~&quot; ) , &quot;YRBSS&quot; , &quot;2015 main.rds&quot; ) ) yrbss_design &lt;- svydesign( ~ psu , strata = ~ stratum , data = yrbss_df , weights = ~ weight , nest = TRUE ) Variable Recoding Add new columns to the data set: yrbss_design &lt;- update( yrbss_design , q2 = q2 , never_rarely_wore_bike_helmet = as.numeric( qn8 == 1 ) , ever_smoked_marijuana = as.numeric( qn47 == 1 ) , ever_tried_to_quit_cigarettes = as.numeric( q36 &gt; 2 ) , smoked_cigarettes_past_year = as.numeric( q36 &gt; 1 ) ) Unweighted Counts Count the unweighted number of records in the survey sample, overall and by groups: sum( weights( yrbss_design , &quot;sampling&quot; ) != 0 ) svyby( ~ one , ~ ever_smoked_marijuana , yrbss_design , unwtd.count ) Weighted Counts Count the weighted size of the generalizable population, overall and by groups: svytotal( ~ one , yrbss_design ) svyby( ~ one , ~ ever_smoked_marijuana , yrbss_design , svytotal ) Descriptive Statistics Calculate the mean (average) of a linear variable, overall and by groups: svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_smoked_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the distribution of a categorical variable, overall and by groups: svymean( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_smoked_marijuana , yrbss_design , svymean , na.rm = TRUE ) Calculate the sum of a linear variable, overall and by groups: svytotal( ~ bmipct , yrbss_design , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_smoked_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the weighted sum of a categorical variable, overall and by groups: svytotal( ~ q2 , yrbss_design , na.rm = TRUE ) svyby( ~ q2 , ~ ever_smoked_marijuana , yrbss_design , svytotal , na.rm = TRUE ) Calculate the median (50th percentile) of a linear variable, overall and by groups: svyquantile( ~ bmipct , yrbss_design , 0.5 , na.rm = TRUE ) svyby( ~ bmipct , ~ ever_smoked_marijuana , yrbss_design , svyquantile , 0.5 , ci = TRUE , keep.var = TRUE , na.rm = TRUE ) Estimate a ratio: svyratio( numerator = ~ ever_tried_to_quit_cigarettes , denominator = ~ smoked_cigarettes_past_year , yrbss_design , na.rm = TRUE ) Subsetting Restrict the survey design to youths who ever drank alcohol: sub_yrbss_design &lt;- subset( yrbss_design , qn41 == 1 ) Calculate the mean (average) of this subset: svymean( ~ bmipct , sub_yrbss_design , na.rm = TRUE ) Measures of Uncertainty Extract the coefficient, standard error, confidence interval, and coefficient of variation from any descriptive statistics function result, overall and by groups: this_result &lt;- svymean( ~ bmipct , yrbss_design , na.rm = TRUE ) coef( this_result ) SE( this_result ) confint( this_result ) cv( this_result ) grouped_result &lt;- svyby( ~ bmipct , ~ ever_smoked_marijuana , yrbss_design , svymean , na.rm = TRUE ) coef( grouped_result ) SE( grouped_result ) confint( grouped_result ) cv( grouped_result ) Calculate the degrees of freedom of any survey design object: degf( yrbss_design ) Calculate the complex sample survey-adjusted variance of any statistic: svyvar( ~ bmipct , yrbss_design , na.rm = TRUE ) Include the complex sample design effect in the result for a specific statistic: # SRS without replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = TRUE ) # SRS with replacement svymean( ~ bmipct , yrbss_design , na.rm = TRUE , deff = &quot;replace&quot; ) Compute confidence intervals for proportions using methods that may be more accurate near 0 and 1. See ?svyciprop for alternatives: svyciprop( ~ never_rarely_wore_bike_helmet , yrbss_design , method = &quot;likelihood&quot; , na.rm = TRUE ) Regression Models and Tests of Association Perform a design-based t-test: svyttest( bmipct ~ never_rarely_wore_bike_helmet , yrbss_design ) Perform a chi-squared test of association for survey data: svychisq( ~ never_rarely_wore_bike_helmet + q2 , yrbss_design ) Perform a survey-weighted generalized linear model: glm_result &lt;- svyglm( bmipct ~ never_rarely_wore_bike_helmet + q2 , yrbss_design ) summary( glm_result ) Analysis Examples with srvyr   The R srvyr library calculates summary statistics from survey data, such as the mean, total or quantile using dplyr-like syntax. srvyr allows for the use of many verbs, such as summarize, group_by, and mutate, the convenience of pipe-able functions, the tidyverse style of non-standard evaluation and more consistent return types than the survey package. This vignette details the available features. As a starting point for YRBSS users, this code replicates previously-presented examples: library(srvyr) yrbss_srvyr_design &lt;- as_survey( yrbss_design ) Calculate the mean (average) of a linear variable, overall and by groups: yrbss_srvyr_design %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) yrbss_srvyr_design %&gt;% group_by( ever_smoked_marijuana ) %&gt;% summarize( mean = survey_mean( bmipct , na.rm = TRUE ) ) Replication Example This snippet replicates the “never/rarely wore bicycle helmet” row of PDF page 29 of this CDC analysis software document. unwtd.count( ~ never_rarely_wore_bike_helmet , yrbss_design ) svytotal( ~ one , subset( yrbss_design , !is.na( never_rarely_wore_bike_helmet ) ) ) svymean( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE ) svyciprop( ~ never_rarely_wore_bike_helmet , yrbss_design , na.rm = TRUE , method = &quot;beta&quot; ) "]]
